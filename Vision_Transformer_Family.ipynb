{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0Anbanblvb1",
        "outputId": "33d00c20-8f8b-4d61-b299-a82ff07d8b16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting einops\n",
            "  Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.8.0\n",
            "Collecting odach\n",
            "  Downloading odach-0.1.5.post2206210229-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: odach\n",
            "Successfully installed odach-0.1.5.post2206210229\n"
          ]
        }
      ],
      "source": [
        "! pip install einops\n",
        "! pip install odach\n",
        "# ! pip install wandb\n",
        "# ! pip install vit-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "f_9Tpzgpsv0d"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "\n",
        "# Try to remove the tree; if it fails, throw an error using try...except.\n",
        "try:\n",
        "    shutil.rmtree(\"/content/sample_data\")\n",
        "except OSError as e:\n",
        "    print(\"Error: %s - %s.\" % (e.filename, e.strerror))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "TpZufgx1veLO"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZaWwPFttIVXg",
        "outputId": "f605ae08-8c4a-4048-8086-4de8751c3781"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mhint: Using 'master' as the name for the initial branch. This default branch name\u001b[m\n",
            "\u001b[33mhint: is subject to change. To configure the initial branch name to use in all\u001b[m\n",
            "\u001b[33mhint: of your new repositories, which will suppress this warning, call:\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: \tgit config --global init.defaultBranch <name>\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: Names commonly chosen instead of 'master' are 'main', 'trunk' and\u001b[m\n",
            "\u001b[33mhint: 'development'. The just-created branch can be renamed via this command:\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: \tgit branch -m <name>\u001b[m\n",
            "Initialized empty Git repository in /content/.git/\n",
            "remote: Enumerating objects: 170, done.\u001b[K\n",
            "remote: Counting objects: 100% (170/170), done.\u001b[K\n",
            "remote: Compressing objects: 100% (166/166), done.\u001b[K\n",
            "remote: Total 170 (delta 92), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (170/170), 240.51 KiB | 14.15 MiB/s, done.\n",
            "Resolving deltas: 100% (92/92), done.\n",
            "From https://github.com/RichardMinsooGo/Bible_2_U_ViT_Family\n",
            " * branch            main       -> FETCH_HEAD\n",
            " * [new branch]      main       -> origin/main\n"
          ]
        }
      ],
      "source": [
        "# Clone from Github Repository\n",
        "! git init .\n",
        "! git remote add origin https://github.com/RichardMinsooGo/Bible_2_U_ViT_Family.git\n",
        "# ! git pull origin master\n",
        "! git pull origin main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XSVuU6GeO2g7",
        "outputId": "42b1cee4-91f4-4678-8e6f-01de917fa262",
        "collapsed": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==> Preparing data..\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n",
            "100% 170498071/170498071 [00:03<00:00, 43612071.37it/s]\n",
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "==> Building model..\n",
            "cuda\n",
            "\n",
            "Epoch :  1\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            " [================================================================>]  Step: 55ms | Tot: 5s629ms | Loss: 2.277 | Acc: 13.682% (6841/50000) 98/98 \n",
            " [================================================================>]  Step: 13ms | Tot: 1s403ms | Loss: 1.959 | Acc: 28.360% (2836/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 06:32:44 2024 Epoch 1, lr: 0.0001000, val loss: 195.85245, acc: 28.36000\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "Epoch : 1 train_loss :2.277 , val_loss :195.852 val_acc : 28.36 lr :0.0000994 epoch_time : 9 sec\n",
            "\n",
            "Epoch :  2\n",
            " [================================================================>]  Step: 38ms | Tot: 5s748ms | Loss: 2.170 | Acc: 19.080% (9540/50000) 98/98 \n",
            " [================================================================>]  Step: 13ms | Tot: 1s417ms | Loss: 1.756 | Acc: 37.620% (3762/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 06:32:52 2024 Epoch 2, lr: 0.0000994, val loss: 175.55773, acc: 37.62000\n",
            "Epoch : 2 train_loss :2.170 , val_loss :175.558 val_acc : 37.62 lr :0.0001000 epoch_time : 8 sec\n",
            "\n",
            "Epoch :  3\n",
            " [================================================================>]  Step: 38ms | Tot: 5s757ms | Loss: 2.085 | Acc: 22.744% (11372/50000) 98/98 \n",
            " [================================================================>]  Step: 13ms | Tot: 1s400ms | Loss: 1.641 | Acc: 41.170% (4117/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 06:33:01 2024 Epoch 3, lr: 0.0001000, val loss: 164.11944, acc: 41.17000\n",
            "Epoch : 3 train_loss :2.085 , val_loss :164.119 val_acc : 41.17 lr :0.0000994 epoch_time : 8 sec\n",
            "\n",
            "Epoch :  4\n",
            " [================================================================>]  Step: 38ms | Tot: 5s875ms | Loss: 2.022 | Acc: 25.106% (12553/50000) 98/98 \n",
            " [================================================================>]  Step: 13ms | Tot: 1s364ms | Loss: 1.523 | Acc: 45.940% (4594/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 06:33:09 2024 Epoch 4, lr: 0.0000994, val loss: 152.25760, acc: 45.94000\n",
            "Epoch : 4 train_loss :2.022 , val_loss :152.258 val_acc : 45.94 lr :0.0000976 epoch_time : 8 sec\n",
            "\n",
            "Epoch :  5\n",
            " [================================================================>]  Step: 38ms | Tot: 5s687ms | Loss: 1.974 | Acc: 27.060% (13530/50000) 98/98 \n",
            " [================================================================>]  Step: 13ms | Tot: 1s393ms | Loss: 1.491 | Acc: 46.140% (4614/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 06:33:18 2024 Epoch 5, lr: 0.0000976, val loss: 149.09939, acc: 46.14000\n",
            "Epoch : 5 train_loss :1.974 , val_loss :149.099 val_acc : 46.14 lr :0.0000946 epoch_time : 8 sec\n",
            "\n",
            "Epoch :  6\n",
            " [================================================================>]  Step: 38ms | Tot: 5s655ms | Loss: 1.937 | Acc: 28.596% (14298/50000) 98/98 \n",
            " [================================================================>]  Step: 13ms | Tot: 1s384ms | Loss: 1.413 | Acc: 49.210% (4921/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 06:33:26 2024 Epoch 6, lr: 0.0000946, val loss: 141.28552, acc: 49.21000\n",
            "Epoch : 6 train_loss :1.937 , val_loss :141.286 val_acc : 49.21 lr :0.0000905 epoch_time : 8 sec\n",
            "\n",
            "Epoch :  7\n",
            " [================================================================>]  Step: 38ms | Tot: 5s940ms | Loss: 1.912 | Acc: 29.704% (14852/50000) 98/98 \n",
            " [================================================================>]  Step: 13ms | Tot: 1s374ms | Loss: 1.399 | Acc: 50.550% (5055/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 06:33:35 2024 Epoch 7, lr: 0.0000905, val loss: 139.89441, acc: 50.55000\n",
            "Epoch : 7 train_loss :1.912 , val_loss :139.894 val_acc : 50.55 lr :0.0000854 epoch_time : 8 sec\n",
            "\n",
            "Epoch :  8\n",
            " [================================================================>]  Step: 38ms | Tot: 5s870ms | Loss: 1.886 | Acc: 30.810% (15405/50000) 98/98 \n",
            " [================================================================>]  Step: 13ms | Tot: 1s389ms | Loss: 1.344 | Acc: 51.650% (5165/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 06:33:43 2024 Epoch 8, lr: 0.0000854, val loss: 134.42757, acc: 51.65000\n",
            "Epoch : 8 train_loss :1.886 , val_loss :134.428 val_acc : 51.65 lr :0.0000794 epoch_time : 8 sec\n",
            "\n",
            "Epoch :  9\n",
            " [================================================================>]  Step: 38ms | Tot: 5s633ms | Loss: 1.864 | Acc: 31.446% (15723/50000) 98/98 \n",
            " [================================================================>]  Step: 13ms | Tot: 1s400ms | Loss: 1.334 | Acc: 53.120% (5312/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 06:33:51 2024 Epoch 9, lr: 0.0000794, val loss: 133.44617, acc: 53.12000\n",
            "Epoch : 9 train_loss :1.864 , val_loss :133.446 val_acc : 53.12 lr :0.0000727 epoch_time : 8 sec\n",
            "\n",
            "Epoch :  10\n",
            " [================================================================>]  Step: 38ms | Tot: 5s931ms | Loss: 1.842 | Acc: 31.938% (15969/50000) 98/98 \n",
            " [================================================================>]  Step: 13ms | Tot: 1s376ms | Loss: 1.296 | Acc: 54.370% (5437/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 06:34:00 2024 Epoch 10, lr: 0.0000727, val loss: 129.58190, acc: 54.37000\n",
            "Epoch : 10 train_loss :1.842 , val_loss :129.582 val_acc : 54.37 lr :0.0000655 epoch_time : 8 sec\n",
            "\n",
            "Epoch :  11\n",
            " [================================================================>]  Step: 38ms | Tot: 5s944ms | Loss: 1.828 | Acc: 32.650% (16325/50000) 98/98 \n",
            " [================================================================>]  Step: 14ms | Tot: 1s388ms | Loss: 1.298 | Acc: 54.020% (5402/10000) 100/100 \n",
            "Current time : Sat Apr 27 06:34:08 2024 Epoch 11, lr: 0.0000655, val loss: 129.81429, acc: 54.02000\n",
            "Epoch : 11 train_loss :1.828 , val_loss :129.814 val_acc : 54.02 lr :0.0000578 epoch_time : 8 sec\n",
            "\n",
            "Epoch :  12\n",
            " [================================================================>]  Step: 38ms | Tot: 5s814ms | Loss: 1.817 | Acc: 33.298% (16649/50000) 98/98 \n",
            " [================================================================>]  Step: 13ms | Tot: 1s379ms | Loss: 1.286 | Acc: 54.850% (5485/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 06:34:17 2024 Epoch 12, lr: 0.0000578, val loss: 128.61060, acc: 54.85000\n",
            "Epoch : 12 train_loss :1.817 , val_loss :128.611 val_acc : 54.85 lr :0.0000500 epoch_time : 8 sec\n",
            "\n",
            "Epoch :  13\n",
            " [================================================================>]  Step: 38ms | Tot: 5s881ms | Loss: 1.798 | Acc: 33.784% (16892/50000) 98/98 \n",
            " [================================================================>]  Step: 13ms | Tot: 1s370ms | Loss: 1.289 | Acc: 54.550% (5455/10000) 100/100 \n",
            "Current time : Sat Apr 27 06:34:25 2024 Epoch 13, lr: 0.0000500, val loss: 128.87849, acc: 54.55000\n",
            "Epoch : 13 train_loss :1.798 , val_loss :128.878 val_acc : 54.55 lr :0.0000422 epoch_time : 8 sec\n",
            "\n",
            "Epoch :  14\n",
            " [================================================================>]  Step: 38ms | Tot: 5s972ms | Loss: 1.786 | Acc: 34.086% (17043/50000) 98/98 \n",
            " [================================================================>]  Step: 13ms | Tot: 1s370ms | Loss: 1.258 | Acc: 55.790% (5579/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 06:34:33 2024 Epoch 14, lr: 0.0000422, val loss: 125.80318, acc: 55.79000\n",
            "Epoch : 14 train_loss :1.786 , val_loss :125.803 val_acc : 55.79 lr :0.0000345 epoch_time : 8 sec\n",
            "\n",
            "Epoch :  15\n",
            " [================================================================>]  Step: 39ms | Tot: 5s925ms | Loss: 1.784 | Acc: 34.382% (17191/50000) 98/98 \n",
            " [================================================================>]  Step: 13ms | Tot: 1s364ms | Loss: 1.241 | Acc: 56.520% (5652/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 06:34:42 2024 Epoch 15, lr: 0.0000345, val loss: 124.05572, acc: 56.52000\n",
            "Epoch : 15 train_loss :1.784 , val_loss :124.056 val_acc : 56.52 lr :0.0000273 epoch_time : 8 sec\n",
            "\n",
            "Epoch :  16\n",
            " [================================================================>]  Step: 40ms | Tot: 5s871ms | Loss: 1.770 | Acc: 35.034% (17517/50000) 98/98 \n",
            " [================================================================>]  Step: 13ms | Tot: 1s398ms | Loss: 1.240 | Acc: 56.770% (5677/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 06:34:50 2024 Epoch 16, lr: 0.0000273, val loss: 124.00458, acc: 56.77000\n",
            "Epoch : 16 train_loss :1.770 , val_loss :124.005 val_acc : 56.77 lr :0.0000206 epoch_time : 8 sec\n",
            "\n",
            "Epoch :  17\n",
            " [================================================================>]  Step: 38ms | Tot: 5s695ms | Loss: 1.764 | Acc: 34.882% (17441/50000) 98/98 \n",
            " [================================================================>]  Step: 13ms | Tot: 1s376ms | Loss: 1.233 | Acc: 56.790% (5679/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 06:34:59 2024 Epoch 17, lr: 0.0000206, val loss: 123.30422, acc: 56.79000\n",
            "Epoch : 17 train_loss :1.764 , val_loss :123.304 val_acc : 56.79 lr :0.0000146 epoch_time : 8 sec\n",
            "\n",
            "Epoch :  18\n",
            " [================================================================>]  Step: 40ms | Tot: 6s144ms | Loss: 1.761 | Acc: 35.444% (17722/50000) 98/98 \n",
            " [================================================================>]  Step: 13ms | Tot: 1s369ms | Loss: 1.212 | Acc: 57.860% (5786/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 06:35:07 2024 Epoch 18, lr: 0.0000146, val loss: 121.19223, acc: 57.86000\n",
            "Epoch : 18 train_loss :1.761 , val_loss :121.192 val_acc : 57.86 lr :0.0000095 epoch_time : 8 sec\n",
            "\n",
            "Epoch :  19\n",
            " [================================================================>]  Step: 38ms | Tot: 5s682ms | Loss: 1.753 | Acc: 35.396% (17698/50000) 98/98 \n",
            " [================================================================>]  Step: 13ms | Tot: 1s385ms | Loss: 1.212 | Acc: 57.800% (5780/10000) 100/100 \n",
            "Current time : Sat Apr 27 06:35:16 2024 Epoch 19, lr: 0.0000095, val loss: 121.21596, acc: 57.80000\n",
            "Epoch : 19 train_loss :1.753 , val_loss :121.216 val_acc : 57.8 lr :0.0000054 epoch_time : 8 sec\n",
            "\n",
            "Epoch :  20\n",
            " [================================================================>]  Step: 39ms | Tot: 5s755ms | Loss: 1.748 | Acc: 35.642% (17821/50000) 98/98 \n",
            " [================================================================>]  Step: 13ms | Tot: 1s393ms | Loss: 1.212 | Acc: 57.810% (5781/10000) 100/100 \n",
            "Current time : Sat Apr 27 06:35:24 2024 Epoch 20, lr: 0.0000054, val loss: 121.20482, acc: 57.81000\n",
            "Epoch : 20 train_loss :1.748 , val_loss :121.205 val_acc : 57.81 lr :0.0000024 epoch_time : 8 sec\n"
          ]
        }
      ],
      "source": [
        "! python train.py --n_epochs 20 # vit-patchsize-4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSvJbGvUO1QU",
        "outputId": "a11088ef-9db2-4d62-bc84-32447786bc78",
        "collapsed": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==> Preparing data..\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "==> Building model..\n",
            "cuda\n",
            "\n",
            "Epoch :  1\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            " [================================================================>]  Step: 86ms | Tot: 12s84ms | Loss: 2.278 | Acc: 14.076% (7038/50000) 98/98 \n",
            " [================================================================>]  Step: 29ms | Tot: 2s915ms | Loss: 1.987 | Acc: 29.340% (2934/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 06:35:47 2024 Epoch 1, lr: 0.0001000, val loss: 198.70237, acc: 29.34000\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "Epoch : 1 train_loss :2.278 , val_loss :198.702 val_acc : 29.34 lr :0.0000994 epoch_time : 16 sec\n",
            "\n",
            "Epoch :  2\n",
            " [================================================================>]  Step: 87ms | Tot: 12s123ms | Loss: 2.197 | Acc: 18.312% (9156/50000) 98/98 \n",
            " [================================================================>]  Step: 29ms | Tot: 2s926ms | Loss: 1.820 | Acc: 36.280% (3628/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 06:36:04 2024 Epoch 2, lr: 0.0000994, val loss: 181.96808, acc: 36.28000\n",
            "Epoch : 2 train_loss :2.197 , val_loss :181.968 val_acc : 36.28 lr :0.0001000 epoch_time : 16 sec\n",
            "\n",
            "Epoch :  3\n",
            " [================================================================>]  Step: 87ms | Tot: 12s109ms | Loss: 2.103 | Acc: 22.126% (11063/50000) 98/98 \n",
            " [================================================================>]  Step: 29ms | Tot: 2s921ms | Loss: 1.638 | Acc: 42.720% (4272/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 06:36:20 2024 Epoch 3, lr: 0.0001000, val loss: 163.82400, acc: 42.72000\n",
            "Epoch : 3 train_loss :2.103 , val_loss :163.824 val_acc : 42.72 lr :0.0000994 epoch_time : 16 sec\n",
            "\n",
            "Epoch :  4\n",
            " [================================================================>]  Step: 86ms | Tot: 12s160ms | Loss: 2.023 | Acc: 25.062% (12531/50000) 98/98 \n",
            " [================================================================>]  Step: 29ms | Tot: 2s948ms | Loss: 1.543 | Acc: 45.660% (4566/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 06:36:37 2024 Epoch 4, lr: 0.0000994, val loss: 154.30410, acc: 45.66000\n",
            "Epoch : 4 train_loss :2.023 , val_loss :154.304 val_acc : 45.66 lr :0.0000976 epoch_time : 16 sec\n",
            "\n",
            "Epoch :  5\n",
            " [================================================================>]  Step: 86ms | Tot: 12s115ms | Loss: 1.980 | Acc: 26.778% (13389/50000) 98/98 \n",
            " [================================================================>]  Step: 29ms | Tot: 2s917ms | Loss: 1.463 | Acc: 48.280% (4828/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 06:36:53 2024 Epoch 5, lr: 0.0000976, val loss: 146.25884, acc: 48.28000\n",
            "Epoch : 5 train_loss :1.980 , val_loss :146.259 val_acc : 48.28 lr :0.0000946 epoch_time : 16 sec\n",
            "\n",
            "Epoch :  6\n",
            " [================================================================>]  Step: 88ms | Tot: 12s210ms | Loss: 1.945 | Acc: 28.082% (14041/50000) 98/98 \n",
            " [================================================================>]  Step: 29ms | Tot: 2s934ms | Loss: 1.409 | Acc: 50.320% (5032/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 06:37:10 2024 Epoch 6, lr: 0.0000946, val loss: 140.85768, acc: 50.32000\n",
            "Epoch : 6 train_loss :1.945 , val_loss :140.858 val_acc : 50.32 lr :0.0000905 epoch_time : 16 sec\n",
            "\n",
            "Epoch :  7\n",
            " [================================================================>]  Step: 87ms | Tot: 12s116ms | Loss: 1.908 | Acc: 29.516% (14758/50000) 98/98 \n",
            " [================================================================>]  Step: 29ms | Tot: 2s904ms | Loss: 1.388 | Acc: 51.280% (5128/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 06:37:26 2024 Epoch 7, lr: 0.0000905, val loss: 138.78580, acc: 51.28000\n",
            "Epoch : 7 train_loss :1.908 , val_loss :138.786 val_acc : 51.28 lr :0.0000854 epoch_time : 16 sec\n",
            "\n",
            "Epoch :  8\n",
            " [================================================================>]  Step: 86ms | Tot: 12s123ms | Loss: 1.888 | Acc: 30.240% (15120/50000) 98/98 \n",
            " [================================================================>]  Step: 29ms | Tot: 2s897ms | Loss: 1.378 | Acc: 51.500% (5150/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 06:37:43 2024 Epoch 8, lr: 0.0000854, val loss: 137.81517, acc: 51.50000\n",
            "Epoch : 8 train_loss :1.888 , val_loss :137.815 val_acc : 51.5 lr :0.0000794 epoch_time : 16 sec\n",
            "\n",
            "Epoch :  9\n",
            " [================================================================>]  Step: 86ms | Tot: 12s89ms | Loss: 1.869 | Acc: 31.070% (15535/50000) 98/98 \n",
            " [================================================================>]  Step: 29ms | Tot: 2s928ms | Loss: 1.341 | Acc: 52.510% (5251/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 06:38:00 2024 Epoch 9, lr: 0.0000794, val loss: 134.12284, acc: 52.51000\n",
            "Epoch : 9 train_loss :1.869 , val_loss :134.123 val_acc : 52.51 lr :0.0000727 epoch_time : 16 sec\n",
            "\n",
            "Epoch :  10\n",
            " [================================================================>]  Step: 86ms | Tot: 12s150ms | Loss: 1.843 | Acc: 32.236% (16118/50000) 98/98 \n",
            " [================================================================>]  Step: 29ms | Tot: 2s914ms | Loss: 1.340 | Acc: 52.550% (5255/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 06:38:16 2024 Epoch 10, lr: 0.0000727, val loss: 133.99961, acc: 52.55000\n",
            "Epoch : 10 train_loss :1.843 , val_loss :134.000 val_acc : 52.55 lr :0.0000655 epoch_time : 16 sec\n",
            "\n",
            "Epoch :  11\n",
            " [================================================================>]  Step: 86ms | Tot: 12s181ms | Loss: 1.832 | Acc: 32.484% (16242/50000) 98/98 \n",
            " [================================================================>]  Step: 29ms | Tot: 2s916ms | Loss: 1.295 | Acc: 54.210% (5421/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 06:38:33 2024 Epoch 11, lr: 0.0000655, val loss: 129.54787, acc: 54.21000\n",
            "Epoch : 11 train_loss :1.832 , val_loss :129.548 val_acc : 54.21 lr :0.0000578 epoch_time : 16 sec\n",
            "\n",
            "Epoch :  12\n",
            " [================================================================>]  Step: 88ms | Tot: 12s106ms | Loss: 1.815 | Acc: 33.294% (16647/50000) 98/98 \n",
            " [================================================================>]  Step: 29ms | Tot: 2s938ms | Loss: 1.292 | Acc: 54.560% (5456/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 06:38:49 2024 Epoch 12, lr: 0.0000578, val loss: 129.15470, acc: 54.56000\n",
            "Epoch : 12 train_loss :1.815 , val_loss :129.155 val_acc : 54.56 lr :0.0000500 epoch_time : 16 sec\n",
            "\n",
            "Epoch :  13\n",
            " [================================================================>]  Step: 88ms | Tot: 12s103ms | Loss: 1.806 | Acc: 33.508% (16754/50000) 98/98 \n",
            " [================================================================>]  Step: 29ms | Tot: 2s920ms | Loss: 1.263 | Acc: 55.680% (5568/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 06:39:06 2024 Epoch 13, lr: 0.0000500, val loss: 126.31505, acc: 55.68000\n",
            "Epoch : 13 train_loss :1.806 , val_loss :126.315 val_acc : 55.68 lr :0.0000422 epoch_time : 16 sec\n",
            "\n",
            "Epoch :  14\n",
            " [================================================================>]  Step: 86ms | Tot: 12s126ms | Loss: 1.783 | Acc: 34.226% (17113/50000) 98/98 \n",
            " [================================================================>]  Step: 29ms | Tot: 2s922ms | Loss: 1.247 | Acc: 55.860% (5586/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 06:39:22 2024 Epoch 14, lr: 0.0000422, val loss: 124.73164, acc: 55.86000\n",
            "Epoch : 14 train_loss :1.783 , val_loss :124.732 val_acc : 55.86 lr :0.0000345 epoch_time : 16 sec\n",
            "\n",
            "Epoch :  15\n",
            " [================================================================>]  Step: 87ms | Tot: 12s87ms | Loss: 1.776 | Acc: 34.974% (17487/50000) 98/98 \n",
            " [================================================================>]  Step: 29ms | Tot: 2s912ms | Loss: 1.256 | Acc: 56.510% (5651/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 06:39:39 2024 Epoch 15, lr: 0.0000345, val loss: 125.60082, acc: 56.51000\n",
            "Epoch : 15 train_loss :1.776 , val_loss :125.601 val_acc : 56.51 lr :0.0000273 epoch_time : 16 sec\n",
            "\n",
            "Epoch :  16\n",
            " [================================================================>]  Step: 87ms | Tot: 12s115ms | Loss: 1.771 | Acc: 34.900% (17450/50000) 98/98 \n",
            " [================================================================>]  Step: 29ms | Tot: 2s920ms | Loss: 1.251 | Acc: 56.600% (5660/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 06:39:55 2024 Epoch 16, lr: 0.0000273, val loss: 125.05835, acc: 56.60000\n",
            "Epoch : 16 train_loss :1.771 , val_loss :125.058 val_acc : 56.6 lr :0.0000206 epoch_time : 16 sec\n",
            "\n",
            "Epoch :  17\n",
            " [================================================================>]  Step: 87ms | Tot: 12s200ms | Loss: 1.758 | Acc: 35.336% (17668/50000) 98/98 \n",
            " [================================================================>]  Step: 29ms | Tot: 2s914ms | Loss: 1.237 | Acc: 56.530% (5653/10000) 100/100 \n",
            "Current time : Sat Apr 27 06:40:11 2024 Epoch 17, lr: 0.0000206, val loss: 123.67942, acc: 56.53000\n",
            "Epoch : 17 train_loss :1.758 , val_loss :123.679 val_acc : 56.53 lr :0.0000146 epoch_time : 16 sec\n",
            "\n",
            "Epoch :  18\n",
            " [================================================================>]  Step: 86ms | Tot: 12s125ms | Loss: 1.751 | Acc: 35.418% (17709/50000) 98/98 \n",
            " [================================================================>]  Step: 29ms | Tot: 2s913ms | Loss: 1.222 | Acc: 57.440% (5744/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 06:40:28 2024 Epoch 18, lr: 0.0000146, val loss: 122.23828, acc: 57.44000\n",
            "Epoch : 18 train_loss :1.751 , val_loss :122.238 val_acc : 57.44 lr :0.0000095 epoch_time : 16 sec\n",
            "\n",
            "Epoch :  19\n",
            " [================================================================>]  Step: 86ms | Tot: 12s138ms | Loss: 1.750 | Acc: 35.604% (17802/50000) 98/98 \n",
            " [================================================================>]  Step: 30ms | Tot: 2s950ms | Loss: 1.231 | Acc: 56.860% (5686/10000) 100/100 \n",
            "Current time : Sat Apr 27 06:40:44 2024 Epoch 19, lr: 0.0000095, val loss: 123.11671, acc: 56.86000\n",
            "Epoch : 19 train_loss :1.750 , val_loss :123.117 val_acc : 56.86 lr :0.0000054 epoch_time : 16 sec\n",
            "\n",
            "Epoch :  20\n",
            " [================================================================>]  Step: 86ms | Tot: 12s88ms | Loss: 1.754 | Acc: 35.386% (17693/50000) 98/98 \n",
            " [================================================================>]  Step: 29ms | Tot: 2s910ms | Loss: 1.217 | Acc: 57.440% (5744/10000) 100/100 \n",
            "Current time : Sat Apr 27 06:41:00 2024 Epoch 20, lr: 0.0000054, val loss: 121.72264, acc: 57.44000\n",
            "Epoch : 20 train_loss :1.754 , val_loss :121.723 val_acc : 57.44 lr :0.0000024 epoch_time : 16 sec\n"
          ]
        }
      ],
      "source": [
        "! python train.py  --size 48 --n_epochs 20 # vit-patchsize-4-imsize-48"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mdiNnZPrO1TD",
        "outputId": "e7b7f954-234e-49f2-af7c-938abe8e0a80",
        "collapsed": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==> Preparing data..\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "==> Building model..\n",
            "cuda\n",
            "\n",
            "Epoch :  1\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            " [================================================================>]  Step: 173ms | Tot: 24s33ms | Loss: 2.287 | Acc: 13.508% (6754/50000) 98/98 \n",
            " [================================================================>]  Step: 51ms | Tot: 5s87ms | Loss: 2.047 | Acc: 25.160% (2516/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 06:41:38 2024 Epoch 1, lr: 0.0001000, val loss: 204.71897, acc: 25.16000\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "Epoch : 1 train_loss :2.287 , val_loss :204.719 val_acc : 25.16 lr :0.0000994 epoch_time : 31 sec\n",
            "\n",
            "Epoch :  2\n",
            " [================================================================>]  Step: 170ms | Tot: 24s44ms | Loss: 2.207 | Acc: 17.382% (8691/50000) 98/98 \n",
            " [================================================================>]  Step: 51ms | Tot: 5s86ms | Loss: 1.859 | Acc: 33.330% (3333/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 06:42:09 2024 Epoch 2, lr: 0.0000994, val loss: 185.90653, acc: 33.33000\n",
            "Epoch : 2 train_loss :2.207 , val_loss :185.907 val_acc : 33.33 lr :0.0001000 epoch_time : 30 sec\n",
            "\n",
            "Epoch :  3\n",
            " [================================================================>]  Step: 170ms | Tot: 24s46ms | Loss: 2.115 | Acc: 21.468% (10734/50000) 98/98 \n",
            " [================================================================>]  Step: 51ms | Tot: 5s100ms | Loss: 1.686 | Acc: 38.880% (3888/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 06:42:39 2024 Epoch 3, lr: 0.0001000, val loss: 168.59032, acc: 38.88000\n",
            "Epoch : 3 train_loss :2.115 , val_loss :168.590 val_acc : 38.88 lr :0.0000994 epoch_time : 30 sec\n",
            "\n",
            "Epoch :  4\n",
            " [================================================================>]  Step: 170ms | Tot: 24s47ms | Loss: 2.035 | Acc: 24.316% (12158/50000) 98/98 \n",
            " [================================================================>]  Step: 51ms | Tot: 5s94ms | Loss: 1.581 | Acc: 43.970% (4397/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 06:43:10 2024 Epoch 4, lr: 0.0000994, val loss: 158.07941, acc: 43.97000\n",
            "Epoch : 4 train_loss :2.035 , val_loss :158.079 val_acc : 43.97 lr :0.0000976 epoch_time : 30 sec\n",
            "\n",
            "Epoch :  5\n",
            " [================================================================>]  Step: 170ms | Tot: 24s53ms | Loss: 1.977 | Acc: 26.834% (13417/50000) 98/98 \n",
            " [================================================================>]  Step: 51ms | Tot: 5s85ms | Loss: 1.544 | Acc: 44.860% (4486/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 06:43:41 2024 Epoch 5, lr: 0.0000976, val loss: 154.43498, acc: 44.86000\n",
            "Epoch : 5 train_loss :1.977 , val_loss :154.435 val_acc : 44.86 lr :0.0000946 epoch_time : 30 sec\n",
            "\n",
            "Epoch :  6\n",
            " [================================================================>]  Step: 170ms | Tot: 24s80ms | Loss: 1.933 | Acc: 28.466% (14233/50000) 98/98 \n",
            " [================================================================>]  Step: 51ms | Tot: 5s76ms | Loss: 1.428 | Acc: 49.440% (4944/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 06:44:11 2024 Epoch 6, lr: 0.0000946, val loss: 142.82327, acc: 49.44000\n",
            "Epoch : 6 train_loss :1.933 , val_loss :142.823 val_acc : 49.44 lr :0.0000905 epoch_time : 30 sec\n",
            "\n",
            "Epoch :  7\n",
            " [================================================================>]  Step: 170ms | Tot: 24s53ms | Loss: 1.909 | Acc: 29.406% (14703/50000) 98/98 \n",
            " [================================================================>]  Step: 51ms | Tot: 5s86ms | Loss: 1.426 | Acc: 50.090% (5009/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 06:44:42 2024 Epoch 7, lr: 0.0000905, val loss: 142.58555, acc: 50.09000\n",
            "Epoch : 7 train_loss :1.909 , val_loss :142.586 val_acc : 50.09 lr :0.0000854 epoch_time : 30 sec\n",
            "\n",
            "Epoch :  8\n",
            " [================================================================>]  Step: 170ms | Tot: 24s44ms | Loss: 1.884 | Acc: 30.214% (15107/50000) 98/98 \n",
            " [================================================================>]  Step: 51ms | Tot: 5s81ms | Loss: 1.381 | Acc: 51.440% (5144/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 06:45:13 2024 Epoch 8, lr: 0.0000854, val loss: 138.13862, acc: 51.44000\n",
            "Epoch : 8 train_loss :1.884 , val_loss :138.139 val_acc : 51.44 lr :0.0000794 epoch_time : 30 sec\n",
            "\n",
            "Epoch :  9\n",
            " [================================================================>]  Step: 170ms | Tot: 24s63ms | Loss: 1.861 | Acc: 31.204% (15602/50000) 98/98 \n",
            " [================================================================>]  Step: 51ms | Tot: 5s84ms | Loss: 1.381 | Acc: 52.040% (5204/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 06:45:43 2024 Epoch 9, lr: 0.0000794, val loss: 138.07153, acc: 52.04000\n",
            "Epoch : 9 train_loss :1.861 , val_loss :138.072 val_acc : 52.04 lr :0.0000727 epoch_time : 30 sec\n",
            "\n",
            "Epoch :  10\n",
            " [================================================================>]  Step: 170ms | Tot: 24s49ms | Loss: 1.845 | Acc: 32.308% (16154/50000) 98/98 \n",
            " [================================================================>]  Step: 51ms | Tot: 5s77ms | Loss: 1.308 | Acc: 54.030% (5403/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 06:46:14 2024 Epoch 10, lr: 0.0000727, val loss: 130.79921, acc: 54.03000\n",
            "Epoch : 10 train_loss :1.845 , val_loss :130.799 val_acc : 54.03 lr :0.0000655 epoch_time : 30 sec\n",
            "\n",
            "Epoch :  11\n",
            " [================================================================>]  Step: 170ms | Tot: 24s37ms | Loss: 1.821 | Acc: 32.648% (16324/50000) 98/98 \n",
            " [================================================================>]  Step: 51ms | Tot: 5s86ms | Loss: 1.285 | Acc: 54.200% (5420/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 06:46:45 2024 Epoch 11, lr: 0.0000655, val loss: 128.45281, acc: 54.20000\n",
            "Epoch : 11 train_loss :1.821 , val_loss :128.453 val_acc : 54.2 lr :0.0000578 epoch_time : 30 sec\n",
            "\n",
            "Epoch :  12\n",
            " [================================================================>]  Step: 170ms | Tot: 24s50ms | Loss: 1.805 | Acc: 33.326% (16663/50000) 98/98 \n",
            " [================================================================>]  Step: 51ms | Tot: 5s81ms | Loss: 1.274 | Acc: 55.030% (5503/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 06:47:16 2024 Epoch 12, lr: 0.0000578, val loss: 127.39543, acc: 55.03000\n",
            "Epoch : 12 train_loss :1.805 , val_loss :127.395 val_acc : 55.03 lr :0.0000500 epoch_time : 30 sec\n",
            "\n",
            "Epoch :  13\n",
            " [================================================================>]  Step: 170ms | Tot: 24s43ms | Loss: 1.790 | Acc: 34.146% (17073/50000) 98/98 \n",
            " [================================================================>]  Step: 51ms | Tot: 5s85ms | Loss: 1.279 | Acc: 54.790% (5479/10000) 100/100 \n",
            "Current time : Sat Apr 27 06:47:46 2024 Epoch 13, lr: 0.0000500, val loss: 127.91070, acc: 54.79000\n",
            "Epoch : 13 train_loss :1.790 , val_loss :127.911 val_acc : 54.79 lr :0.0000422 epoch_time : 30 sec\n",
            "\n",
            "Epoch :  14\n",
            " [================================================================>]  Step: 170ms | Tot: 24s48ms | Loss: 1.779 | Acc: 34.342% (17171/50000) 98/98 \n",
            " [================================================================>]  Step: 51ms | Tot: 5s97ms | Loss: 1.235 | Acc: 56.390% (5639/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 06:48:17 2024 Epoch 14, lr: 0.0000422, val loss: 123.48341, acc: 56.39000\n",
            "Epoch : 14 train_loss :1.779 , val_loss :123.483 val_acc : 56.39 lr :0.0000345 epoch_time : 30 sec\n",
            "\n",
            "Epoch :  15\n",
            " [================================================================>]  Step: 170ms | Tot: 24s68ms | Loss: 1.765 | Acc: 35.048% (17524/50000) 98/98 \n",
            " [================================================================>]  Step: 51ms | Tot: 5s88ms | Loss: 1.250 | Acc: 56.160% (5616/10000) 100/100 \n",
            "Current time : Sat Apr 27 06:48:47 2024 Epoch 15, lr: 0.0000345, val loss: 125.04760, acc: 56.16000\n",
            "Epoch : 15 train_loss :1.765 , val_loss :125.048 val_acc : 56.16 lr :0.0000273 epoch_time : 30 sec\n",
            "\n",
            "Epoch :  16\n",
            " [================================================================>]  Step: 170ms | Tot: 24s52ms | Loss: 1.756 | Acc: 35.060% (17530/50000) 98/98 \n",
            " [================================================================>]  Step: 51ms | Tot: 5s99ms | Loss: 1.249 | Acc: 56.670% (5667/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 06:49:18 2024 Epoch 16, lr: 0.0000273, val loss: 124.86736, acc: 56.67000\n",
            "Epoch : 16 train_loss :1.756 , val_loss :124.867 val_acc : 56.67 lr :0.0000206 epoch_time : 30 sec\n",
            "\n",
            "Epoch :  17\n",
            " [================================================================>]  Step: 170ms | Tot: 24s69ms | Loss: 1.755 | Acc: 35.472% (17736/50000) 98/98 \n",
            " [================================================================>]  Step: 51ms | Tot: 5s90ms | Loss: 1.221 | Acc: 57.480% (5748/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 06:49:48 2024 Epoch 17, lr: 0.0000206, val loss: 122.05811, acc: 57.48000\n",
            "Epoch : 17 train_loss :1.755 , val_loss :122.058 val_acc : 57.48 lr :0.0000146 epoch_time : 30 sec\n",
            "\n",
            "Epoch :  18\n",
            " [================================================================>]  Step: 170ms | Tot: 24s53ms | Loss: 1.741 | Acc: 35.622% (17811/50000) 98/98 \n",
            " [================================================================>]  Step: 51ms | Tot: 5s87ms | Loss: 1.222 | Acc: 57.860% (5786/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 06:50:19 2024 Epoch 18, lr: 0.0000146, val loss: 122.15242, acc: 57.86000\n",
            "Epoch : 18 train_loss :1.741 , val_loss :122.152 val_acc : 57.86 lr :0.0000095 epoch_time : 30 sec\n",
            "\n",
            "Epoch :  19\n",
            " [================================================================>]  Step: 170ms | Tot: 24s58ms | Loss: 1.742 | Acc: 35.804% (17902/50000) 98/98 \n",
            " [================================================================>]  Step: 51ms | Tot: 5s78ms | Loss: 1.220 | Acc: 57.630% (5763/10000) 100/100 \n",
            "Current time : Sat Apr 27 06:50:49 2024 Epoch 19, lr: 0.0000095, val loss: 121.96566, acc: 57.63000\n",
            "Epoch : 19 train_loss :1.742 , val_loss :121.966 val_acc : 57.63 lr :0.0000054 epoch_time : 30 sec\n",
            "\n",
            "Epoch :  20\n",
            " [================================================================>]  Step: 170ms | Tot: 24s44ms | Loss: 1.733 | Acc: 35.894% (17947/50000) 98/98 \n",
            " [================================================================>]  Step: 51ms | Tot: 5s99ms | Loss: 1.212 | Acc: 57.720% (5772/10000) 100/100 \n",
            "Current time : Sat Apr 27 06:51:20 2024 Epoch 20, lr: 0.0000054, val loss: 121.24892, acc: 57.72000\n",
            "Epoch : 20 train_loss :1.733 , val_loss :121.249 val_acc : 57.72 lr :0.0000024 epoch_time : 30 sec\n"
          ]
        }
      ],
      "source": [
        "! python train.py --patch 2 --n_epochs 20 # vit-patchsize-2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2X-LFno9PIfk",
        "outputId": "09ef34ec-3457-44d6-80ae-8836a364564c",
        "collapsed": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==> Preparing data..\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "==> Building model..\n",
            "cuda\n",
            "\n",
            "Epoch :  1\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            " [================================================================>]  Step: 46ms | Tot: 5s961ms | Loss: 2.248 | Acc: 15.210% (7605/50000) 98/98 \n",
            " [================================================================>]  Step: 14ms | Tot: 1s423ms | Loss: 1.838 | Acc: 32.220% (3222/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 06:51:36 2024 Epoch 1, lr: 0.0001000, val loss: 183.81880, acc: 32.22000\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "Epoch : 1 train_loss :2.248 , val_loss :183.819 val_acc : 32.22 lr :0.0000994 epoch_time : 9 sec\n",
            "\n",
            "Epoch :  2\n",
            " [================================================================>]  Step: 42ms | Tot: 6s24ms | Loss: 2.071 | Acc: 22.756% (11378/50000) 98/98 \n",
            " [================================================================>]  Step: 14ms | Tot: 1s421ms | Loss: 1.612 | Acc: 42.000% (4200/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 06:51:45 2024 Epoch 2, lr: 0.0000994, val loss: 161.22356, acc: 42.00000\n",
            "Epoch : 2 train_loss :2.071 , val_loss :161.224 val_acc : 42.0 lr :0.0001000 epoch_time : 8 sec\n",
            "\n",
            "Epoch :  3\n",
            " [================================================================>]  Step: 41ms | Tot: 6s57ms | Loss: 1.990 | Acc: 25.948% (12974/50000) 98/98 \n",
            " [================================================================>]  Step: 14ms | Tot: 1s442ms | Loss: 1.479 | Acc: 46.870% (4687/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 06:51:54 2024 Epoch 3, lr: 0.0001000, val loss: 147.92328, acc: 46.87000\n",
            "Epoch : 3 train_loss :1.990 , val_loss :147.923 val_acc : 46.87 lr :0.0000994 epoch_time : 8 sec\n",
            "\n",
            "Epoch :  4\n",
            " [================================================================>]  Step: 41ms | Tot: 6s8ms | Loss: 1.938 | Acc: 28.516% (14258/50000) 98/98 \n",
            " [================================================================>]  Step: 14ms | Tot: 1s436ms | Loss: 1.420 | Acc: 48.930% (4893/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 06:52:03 2024 Epoch 4, lr: 0.0000994, val loss: 141.98271, acc: 48.93000\n",
            "Epoch : 4 train_loss :1.938 , val_loss :141.983 val_acc : 48.93 lr :0.0000976 epoch_time : 8 sec\n",
            "\n",
            "Epoch :  5\n",
            " [================================================================>]  Step: 41ms | Tot: 6s100ms | Loss: 1.890 | Acc: 30.422% (15211/50000) 98/98 \n",
            " [================================================================>]  Step: 14ms | Tot: 1s431ms | Loss: 1.392 | Acc: 50.080% (5008/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 06:52:11 2024 Epoch 5, lr: 0.0000976, val loss: 139.20886, acc: 50.08000\n",
            "Epoch : 5 train_loss :1.890 , val_loss :139.209 val_acc : 50.08 lr :0.0000946 epoch_time : 8 sec\n",
            "\n",
            "Epoch :  6\n",
            " [================================================================>]  Step: 41ms | Tot: 5s984ms | Loss: 1.874 | Acc: 31.020% (15510/50000) 98/98 \n",
            " [================================================================>]  Step: 14ms | Tot: 1s424ms | Loss: 1.332 | Acc: 52.070% (5207/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 06:52:20 2024 Epoch 6, lr: 0.0000946, val loss: 133.17188, acc: 52.07000\n",
            "Epoch : 6 train_loss :1.874 , val_loss :133.172 val_acc : 52.07 lr :0.0000905 epoch_time : 8 sec\n",
            "\n",
            "Epoch :  7\n",
            " [================================================================>]  Step: 42ms | Tot: 5s910ms | Loss: 1.847 | Acc: 31.876% (15938/50000) 98/98 \n",
            " [================================================================>]  Step: 14ms | Tot: 1s425ms | Loss: 1.320 | Acc: 52.600% (5260/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 06:52:29 2024 Epoch 7, lr: 0.0000905, val loss: 131.96956, acc: 52.60000\n",
            "Epoch : 7 train_loss :1.847 , val_loss :131.970 val_acc : 52.6 lr :0.0000854 epoch_time : 8 sec\n",
            "\n",
            "Epoch :  8\n",
            " [================================================================>]  Step: 42ms | Tot: 6s67ms | Loss: 1.820 | Acc: 32.850% (16425/50000) 98/98 \n",
            " [================================================================>]  Step: 14ms | Tot: 1s445ms | Loss: 1.291 | Acc: 53.790% (5379/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 06:52:38 2024 Epoch 8, lr: 0.0000854, val loss: 129.13455, acc: 53.79000\n",
            "Epoch : 8 train_loss :1.820 , val_loss :129.135 val_acc : 53.79 lr :0.0000794 epoch_time : 8 sec\n",
            "\n",
            "Epoch :  9\n",
            " [================================================================>]  Step: 43ms | Tot: 6s20ms | Loss: 1.801 | Acc: 33.614% (16807/50000) 98/98 \n",
            " [================================================================>]  Step: 14ms | Tot: 1s432ms | Loss: 1.244 | Acc: 55.630% (5563/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 06:52:47 2024 Epoch 9, lr: 0.0000794, val loss: 124.43747, acc: 55.63000\n",
            "Epoch : 9 train_loss :1.801 , val_loss :124.437 val_acc : 55.63 lr :0.0000727 epoch_time : 8 sec\n",
            "\n",
            "Epoch :  10\n",
            " [================================================================>]  Step: 41ms | Tot: 5s948ms | Loss: 1.782 | Acc: 34.174% (17087/50000) 98/98 \n",
            " [================================================================>]  Step: 14ms | Tot: 1s421ms | Loss: 1.235 | Acc: 56.380% (5638/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 06:52:55 2024 Epoch 10, lr: 0.0000727, val loss: 123.48498, acc: 56.38000\n",
            "Epoch : 10 train_loss :1.782 , val_loss :123.485 val_acc : 56.38 lr :0.0000655 epoch_time : 8 sec\n",
            "\n",
            "Epoch :  11\n",
            " [================================================================>]  Step: 41ms | Tot: 5s914ms | Loss: 1.769 | Acc: 35.126% (17563/50000) 98/98 \n",
            " [================================================================>]  Step: 14ms | Tot: 1s430ms | Loss: 1.218 | Acc: 57.110% (5711/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 06:53:04 2024 Epoch 11, lr: 0.0000655, val loss: 121.81633, acc: 57.11000\n",
            "Epoch : 11 train_loss :1.769 , val_loss :121.816 val_acc : 57.11 lr :0.0000578 epoch_time : 8 sec\n",
            "\n",
            "Epoch :  12\n",
            " [================================================================>]  Step: 41ms | Tot: 6s104ms | Loss: 1.760 | Acc: 35.142% (17571/50000) 98/98 \n",
            " [================================================================>]  Step: 14ms | Tot: 1s403ms | Loss: 1.199 | Acc: 57.120% (5712/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 06:53:13 2024 Epoch 12, lr: 0.0000578, val loss: 119.94339, acc: 57.12000\n",
            "Epoch : 12 train_loss :1.760 , val_loss :119.943 val_acc : 57.12 lr :0.0000500 epoch_time : 8 sec\n",
            "\n",
            "Epoch :  13\n",
            " [================================================================>]  Step: 41ms | Tot: 6s134ms | Loss: 1.743 | Acc: 35.960% (17980/50000) 98/98 \n",
            " [================================================================>]  Step: 14ms | Tot: 1s428ms | Loss: 1.176 | Acc: 58.430% (5843/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 06:53:22 2024 Epoch 13, lr: 0.0000500, val loss: 117.64394, acc: 58.43000\n",
            "Epoch : 13 train_loss :1.743 , val_loss :117.644 val_acc : 58.43 lr :0.0000422 epoch_time : 8 sec\n",
            "\n",
            "Epoch :  14\n",
            " [================================================================>]  Step: 41ms | Tot: 6s9ms | Loss: 1.737 | Acc: 36.020% (18010/50000) 98/98 \n",
            " [================================================================>]  Step: 14ms | Tot: 1s436ms | Loss: 1.187 | Acc: 58.230% (5823/10000) 100/100 \n",
            "Current time : Sat Apr 27 06:53:31 2024 Epoch 14, lr: 0.0000422, val loss: 118.66975, acc: 58.23000\n",
            "Epoch : 14 train_loss :1.737 , val_loss :118.670 val_acc : 58.23 lr :0.0000345 epoch_time : 8 sec\n",
            "\n",
            "Epoch :  15\n",
            " [================================================================>]  Step: 42ms | Tot: 5s906ms | Loss: 1.734 | Acc: 36.132% (18066/50000) 98/98 \n",
            " [================================================================>]  Step: 14ms | Tot: 1s431ms | Loss: 1.165 | Acc: 58.680% (5868/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 06:53:39 2024 Epoch 15, lr: 0.0000345, val loss: 116.51607, acc: 58.68000\n",
            "Epoch : 15 train_loss :1.734 , val_loss :116.516 val_acc : 58.68 lr :0.0000273 epoch_time : 8 sec\n",
            "\n",
            "Epoch :  16\n",
            " [================================================================>]  Step: 41ms | Tot: 5s998ms | Loss: 1.724 | Acc: 36.770% (18385/50000) 98/98 \n",
            " [================================================================>]  Step: 14ms | Tot: 1s411ms | Loss: 1.164 | Acc: 59.350% (5935/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 06:53:48 2024 Epoch 16, lr: 0.0000273, val loss: 116.38536, acc: 59.35000\n",
            "Epoch : 16 train_loss :1.724 , val_loss :116.385 val_acc : 59.35 lr :0.0000206 epoch_time : 8 sec\n",
            "\n",
            "Epoch :  17\n",
            " [================================================================>]  Step: 41ms | Tot: 5s950ms | Loss: 1.713 | Acc: 37.170% (18585/50000) 98/98 \n",
            " [================================================================>]  Step: 14ms | Tot: 1s428ms | Loss: 1.146 | Acc: 59.550% (5955/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 06:53:57 2024 Epoch 17, lr: 0.0000206, val loss: 114.63863, acc: 59.55000\n",
            "Epoch : 17 train_loss :1.713 , val_loss :114.639 val_acc : 59.55 lr :0.0000146 epoch_time : 8 sec\n",
            "\n",
            "Epoch :  18\n",
            " [================================================================>]  Step: 41ms | Tot: 5s952ms | Loss: 1.708 | Acc: 37.006% (18503/50000) 98/98 \n",
            " [================================================================>]  Step: 14ms | Tot: 1s438ms | Loss: 1.145 | Acc: 59.710% (5971/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 06:54:06 2024 Epoch 18, lr: 0.0000146, val loss: 114.45146, acc: 59.71000\n",
            "Epoch : 18 train_loss :1.708 , val_loss :114.451 val_acc : 59.71 lr :0.0000095 epoch_time : 8 sec\n",
            "\n",
            "Epoch :  19\n",
            " [================================================================>]  Step: 41ms | Tot: 6s101ms | Loss: 1.705 | Acc: 37.584% (18792/50000) 98/98 \n",
            " [================================================================>]  Step: 14ms | Tot: 1s422ms | Loss: 1.138 | Acc: 60.010% (6001/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 06:54:15 2024 Epoch 19, lr: 0.0000095, val loss: 113.75872, acc: 60.01000\n",
            "Epoch : 19 train_loss :1.705 , val_loss :113.759 val_acc : 60.01 lr :0.0000054 epoch_time : 8 sec\n",
            "\n",
            "Epoch :  20\n",
            " [================================================================>]  Step: 41ms | Tot: 6s207ms | Loss: 1.703 | Acc: 37.188% (18594/50000) 98/98 \n",
            " [================================================================>]  Step: 14ms | Tot: 1s408ms | Loss: 1.142 | Acc: 59.900% (5990/10000) 100/100 \n",
            "Current time : Sat Apr 27 06:54:23 2024 Epoch 20, lr: 0.0000054, val loss: 114.20483, acc: 59.90000\n",
            "Epoch : 20 train_loss :1.703 , val_loss :114.205 val_acc : 59.9 lr :0.0000024 epoch_time : 8 sec\n"
          ]
        }
      ],
      "source": [
        "! python train.py --net vit_small --n_epochs 20 # vit-small"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLbbB_dDPIir",
        "outputId": "9eb701f4-7647-46b8-d7cf-a543585689b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==> Preparing data..\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "==> Building model..\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/train.py\", line 185, in <module>\n",
            "    import timm\n",
            "ModuleNotFoundError: No module named 'timm'\n"
          ]
        }
      ],
      "source": [
        "! python train.py --net vit_timm --n_epochs 20 # train with pretrained vit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2jCgbz-o8jr",
        "outputId": "5d467d62-8885-4f48-f80c-65731542a278",
        "collapsed": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==> Preparing data..\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n",
            "100% 170498071/170498071 [00:13<00:00, 13012008.95it/s]\n",
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "==> Building model..\n",
            "cuda\n",
            "/content/train.py:270: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  with tqdm_notebook(total=len(train_ds), desc=f\"Train Epoch {epoch+1}\") as pbar:\n",
            "Train Epoch 1:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/content/train.py:308: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  with tqdm_notebook(total=len(test_ds), desc=f\"Test_ Epoch {epoch+1}\") as pbar:\n",
            "Test_ Epoch 1:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Sun Apr 28 22:23:23 2024 Epoch 1, lr: 0.0001000, val loss: 203.83789, acc: 26.36000\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "Epoch : 1 train_loss :2.389 , val_loss :203.838 val_acc : 26.36 lr :0.0001000 epoch_time : 12 sec\n",
            "Train Epoch 2:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 2:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Sun Apr 28 22:23:33 2024 Epoch 2, lr: 0.0001000, val loss: 191.13868, acc: 31.26000\n",
            "Epoch : 2 train_loss :2.231 , val_loss :191.139 val_acc : 31.26 lr :0.0001000 epoch_time : 10 sec\n",
            "Train Epoch 3:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 3:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Sun Apr 28 22:23:43 2024 Epoch 3, lr: 0.0001000, val loss: 183.45322, acc: 34.04000\n",
            "Epoch : 3 train_loss :2.197 , val_loss :183.453 val_acc : 34.04 lr :0.0001000 epoch_time : 10 sec\n",
            "Train Epoch 4:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 4:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Sun Apr 28 22:23:54 2024 Epoch 4, lr: 0.0001000, val loss: 179.76549, acc: 35.16000\n",
            "Epoch : 4 train_loss :2.175 , val_loss :179.765 val_acc : 35.16 lr :0.0000999 epoch_time : 10 sec\n",
            "Train Epoch 5:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 5:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Sun Apr 28 22:24:04 2024 Epoch 5, lr: 0.0000999, val loss: 177.41099, acc: 35.53000\n",
            "Epoch : 5 train_loss :2.148 , val_loss :177.411 val_acc : 35.53 lr :0.0000998 epoch_time : 10 sec\n",
            "Train Epoch 6:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 6:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Sun Apr 28 22:24:14 2024 Epoch 6, lr: 0.0000998, val loss: 175.59149, acc: 37.43000\n",
            "Epoch : 6 train_loss :2.135 , val_loss :175.591 val_acc : 37.43 lr :0.0000996 epoch_time : 10 sec\n",
            "Train Epoch 7:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 7:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Sun Apr 28 22:24:24 2024 Epoch 7, lr: 0.0000996, val loss: 171.63642, acc: 38.52000\n",
            "Epoch : 7 train_loss :2.120 , val_loss :171.636 val_acc : 38.52 lr :0.0000994 epoch_time : 10 sec\n",
            "Train Epoch 8:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 8:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Sun Apr 28 22:24:35 2024 Epoch 8, lr: 0.0000994, val loss: 168.88463, acc: 39.57000\n",
            "Epoch : 8 train_loss :2.102 , val_loss :168.885 val_acc : 39.57 lr :0.0000991 epoch_time : 10 sec\n",
            "Train Epoch 9:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 9:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Sun Apr 28 22:24:45 2024 Epoch 9, lr: 0.0000991, val loss: 166.78324, acc: 40.69000\n",
            "Epoch : 9 train_loss :2.091 , val_loss :166.783 val_acc : 40.69 lr :0.0000988 epoch_time : 10 sec\n",
            "Train Epoch 10:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 10:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Sun Apr 28 22:24:55 2024 Epoch 10, lr: 0.0000988, val loss: 163.62737, acc: 40.96000\n",
            "Epoch : 10 train_loss :2.070 , val_loss :163.627 val_acc : 40.96 lr :0.0000984 epoch_time : 10 sec\n",
            "Train Epoch 11:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 11:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Sun Apr 28 22:25:06 2024 Epoch 11, lr: 0.0000984, val loss: 159.96307, acc: 42.27000\n",
            "Epoch : 11 train_loss :2.044 , val_loss :159.963 val_acc : 42.27 lr :0.0000980 epoch_time : 10 sec\n",
            "Train Epoch 12:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 12:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Sun Apr 28 22:25:16 2024 Epoch 12, lr: 0.0000980, val loss: 159.12238, acc: 42.37000\n",
            "Epoch : 12 train_loss :2.025 , val_loss :159.122 val_acc : 42.37 lr :0.0000976 epoch_time : 10 sec\n",
            "Train Epoch 13:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 13:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Sun Apr 28 22:25:26 2024 Epoch 13, lr: 0.0000976, val loss: 153.89808, acc: 44.39000\n",
            "Epoch : 13 train_loss :2.005 , val_loss :153.898 val_acc : 44.39 lr :0.0000970 epoch_time : 9 sec\n",
            "Train Epoch 14:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 14:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Sun Apr 28 22:25:36 2024 Epoch 14, lr: 0.0000970, val loss: 152.01933, acc: 45.46000\n",
            "Epoch : 14 train_loss :1.988 , val_loss :152.019 val_acc : 45.46 lr :0.0000965 epoch_time : 9 sec\n",
            "Train Epoch 15:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 15:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Sun Apr 28 22:25:46 2024 Epoch 15, lr: 0.0000965, val loss: 151.32400, acc: 45.74000\n",
            "Epoch : 15 train_loss :1.976 , val_loss :151.324 val_acc : 45.74 lr :0.0000959 epoch_time : 10 sec\n",
            "Train Epoch 16:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 16:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Current time : Sun Apr 28 22:25:56 2024 Epoch 16, lr: 0.0000959, val loss: 150.75701, acc: 44.99000\n",
            "Epoch : 16 train_loss :1.964 , val_loss :150.757 val_acc : 44.99 lr :0.0000952 epoch_time : 9 sec\n",
            "Train Epoch 17:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 17:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Sun Apr 28 22:26:06 2024 Epoch 17, lr: 0.0000952, val loss: 147.00272, acc: 46.94000\n",
            "Epoch : 17 train_loss :1.951 , val_loss :147.003 val_acc : 46.94 lr :0.0000946 epoch_time : 10 sec\n",
            "Train Epoch 18:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 18:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Sun Apr 28 22:26:16 2024 Epoch 18, lr: 0.0000946, val loss: 144.48395, acc: 47.95000\n",
            "Epoch : 18 train_loss :1.936 , val_loss :144.484 val_acc : 47.95 lr :0.0000938 epoch_time : 10 sec\n",
            "Train Epoch 19:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 19:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Current time : Sun Apr 28 22:26:26 2024 Epoch 19, lr: 0.0000938, val loss: 146.38555, acc: 47.24000\n",
            "Epoch : 19 train_loss :1.928 , val_loss :146.386 val_acc : 47.24 lr :0.0000930 epoch_time : 9 sec\n",
            "Train Epoch 20:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 20:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Sun Apr 28 22:26:36 2024 Epoch 20, lr: 0.0000930, val loss: 142.26869, acc: 48.36000\n",
            "Epoch : 20 train_loss :1.917 , val_loss :142.269 val_acc : 48.36 lr :0.0000922 epoch_time : 9 sec\n",
            "Train Epoch 21:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 21:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Sun Apr 28 22:26:46 2024 Epoch 21, lr: 0.0000922, val loss: 142.95050, acc: 48.55000\n",
            "Epoch : 21 train_loss :1.905 , val_loss :142.950 val_acc : 48.55 lr :0.0000914 epoch_time : 9 sec\n",
            "Train Epoch 22:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 22:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Sun Apr 28 22:26:56 2024 Epoch 22, lr: 0.0000914, val loss: 140.41617, acc: 49.59000\n",
            "Epoch : 22 train_loss :1.893 , val_loss :140.416 val_acc : 49.59 lr :0.0000905 epoch_time : 10 sec\n",
            "Train Epoch 23:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 23:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Sun Apr 28 22:27:06 2024 Epoch 23, lr: 0.0000905, val loss: 137.41716, acc: 50.11000\n",
            "Epoch : 23 train_loss :1.886 , val_loss :137.417 val_acc : 50.11 lr :0.0000895 epoch_time : 10 sec\n",
            "Train Epoch 24:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 24:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Sun Apr 28 22:27:16 2024 Epoch 24, lr: 0.0000895, val loss: 137.45819, acc: 50.55000\n",
            "Epoch : 24 train_loss :1.877 , val_loss :137.458 val_acc : 50.55 lr :0.0000885 epoch_time : 10 sec\n",
            "Train Epoch 25:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 25:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Sun Apr 28 22:27:27 2024 Epoch 25, lr: 0.0000885, val loss: 137.14801, acc: 50.81000\n",
            "Epoch : 25 train_loss :1.871 , val_loss :137.148 val_acc : 50.81 lr :0.0000875 epoch_time : 10 sec\n",
            "Train Epoch 26:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 26:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Current time : Sun Apr 28 22:27:37 2024 Epoch 26, lr: 0.0000875, val loss: 135.19791, acc: 50.66000\n",
            "Epoch : 26 train_loss :1.867 , val_loss :135.198 val_acc : 50.66 lr :0.0000864 epoch_time : 9 sec\n",
            "Train Epoch 27:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 27:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Sun Apr 28 22:27:47 2024 Epoch 27, lr: 0.0000864, val loss: 134.70933, acc: 51.85000\n",
            "Epoch : 27 train_loss :1.852 , val_loss :134.709 val_acc : 51.85 lr :0.0000854 epoch_time : 10 sec\n",
            "Train Epoch 28:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 28:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Current time : Sun Apr 28 22:27:56 2024 Epoch 28, lr: 0.0000854, val loss: 134.40105, acc: 51.82000\n",
            "Epoch : 28 train_loss :1.850 , val_loss :134.401 val_acc : 51.82 lr :0.0000842 epoch_time : 9 sec\n",
            "Train Epoch 29:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 29:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Sun Apr 28 22:28:06 2024 Epoch 29, lr: 0.0000842, val loss: 133.49828, acc: 52.23000\n",
            "Epoch : 29 train_loss :1.839 , val_loss :133.498 val_acc : 52.23 lr :0.0000831 epoch_time : 9 sec\n",
            "Train Epoch 30:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 30:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Sun Apr 28 22:28:17 2024 Epoch 30, lr: 0.0000831, val loss: 130.93491, acc: 52.88000\n",
            "Epoch : 30 train_loss :1.838 , val_loss :130.935 val_acc : 52.88 lr :0.0000819 epoch_time : 10 sec\n",
            "Train Epoch 31:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 31:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Current time : Sun Apr 28 22:28:27 2024 Epoch 31, lr: 0.0000819, val loss: 131.54072, acc: 52.66000\n",
            "Epoch : 31 train_loss :1.835 , val_loss :131.541 val_acc : 52.66 lr :0.0000806 epoch_time : 10 sec\n",
            "Train Epoch 32:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 32:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Sun Apr 28 22:28:37 2024 Epoch 32, lr: 0.0000806, val loss: 130.17761, acc: 53.82000\n",
            "Epoch : 32 train_loss :1.824 , val_loss :130.178 val_acc : 53.82 lr :0.0000794 epoch_time : 10 sec\n",
            "Train Epoch 33:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 33:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Sun Apr 28 22:28:47 2024 Epoch 33, lr: 0.0000794, val loss: 128.65776, acc: 54.04000\n",
            "Epoch : 33 train_loss :1.818 , val_loss :128.658 val_acc : 54.04 lr :0.0000781 epoch_time : 10 sec\n",
            "Train Epoch 34:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 34:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Current time : Sun Apr 28 22:28:57 2024 Epoch 34, lr: 0.0000781, val loss: 130.75082, acc: 53.03000\n",
            "Epoch : 34 train_loss :1.815 , val_loss :130.751 val_acc : 53.03 lr :0.0000768 epoch_time : 9 sec\n",
            "Train Epoch 35:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 35:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Sun Apr 28 22:29:07 2024 Epoch 35, lr: 0.0000768, val loss: 127.23803, acc: 54.32000\n",
            "Epoch : 35 train_loss :1.803 , val_loss :127.238 val_acc : 54.32 lr :0.0000755 epoch_time : 10 sec\n",
            "Train Epoch 36:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 36:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Sun Apr 28 22:29:17 2024 Epoch 36, lr: 0.0000755, val loss: 126.35056, acc: 54.36000\n",
            "Epoch : 36 train_loss :1.791 , val_loss :126.351 val_acc : 54.36 lr :0.0000741 epoch_time : 10 sec\n",
            "Train Epoch 37:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 37:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Sun Apr 28 22:29:28 2024 Epoch 37, lr: 0.0000741, val loss: 126.42819, acc: 54.81000\n",
            "Epoch : 37 train_loss :1.789 , val_loss :126.428 val_acc : 54.81 lr :0.0000727 epoch_time : 10 sec\n",
            "Train Epoch 38:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 38:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Sun Apr 28 22:29:38 2024 Epoch 38, lr: 0.0000727, val loss: 124.72319, acc: 55.78000\n",
            "Epoch : 38 train_loss :1.787 , val_loss :124.723 val_acc : 55.78 lr :0.0000713 epoch_time : 10 sec\n",
            "Train Epoch 39:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 39:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Sun Apr 28 22:29:48 2024 Epoch 39, lr: 0.0000713, val loss: 124.10187, acc: 56.29000\n",
            "Epoch : 39 train_loss :1.786 , val_loss :124.102 val_acc : 56.29 lr :0.0000699 epoch_time : 10 sec\n",
            "Train Epoch 40:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 40:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Current time : Sun Apr 28 22:29:58 2024 Epoch 40, lr: 0.0000699, val loss: 123.17431, acc: 55.89000\n",
            "Epoch : 40 train_loss :1.768 , val_loss :123.174 val_acc : 55.89 lr :0.0000684 epoch_time : 9 sec\n",
            "Train Epoch 41:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 41:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Current time : Sun Apr 28 22:30:08 2024 Epoch 41, lr: 0.0000684, val loss: 123.72843, acc: 56.29000\n",
            "Epoch : 41 train_loss :1.769 , val_loss :123.728 val_acc : 56.29 lr :0.0000669 epoch_time : 9 sec\n",
            "Train Epoch 42:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 42:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Current time : Sun Apr 28 22:30:17 2024 Epoch 42, lr: 0.0000669, val loss: 122.87784, acc: 56.21000\n",
            "Epoch : 42 train_loss :1.768 , val_loss :122.878 val_acc : 56.21 lr :0.0000655 epoch_time : 9 sec\n",
            "Train Epoch 43:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 43:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Sun Apr 28 22:30:27 2024 Epoch 43, lr: 0.0000655, val loss: 120.71543, acc: 56.98000\n",
            "Epoch : 43 train_loss :1.765 , val_loss :120.715 val_acc : 56.98 lr :0.0000639 epoch_time : 9 sec\n",
            "Train Epoch 44:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 44:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Sun Apr 28 22:30:37 2024 Epoch 44, lr: 0.0000639, val loss: 120.01728, acc: 57.34000\n",
            "Epoch : 44 train_loss :1.756 , val_loss :120.017 val_acc : 57.34 lr :0.0000624 epoch_time : 9 sec\n",
            "Train Epoch 45:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 45:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Current time : Sun Apr 28 22:30:47 2024 Epoch 45, lr: 0.0000624, val loss: 119.87678, acc: 57.13000\n",
            "Epoch : 45 train_loss :1.741 , val_loss :119.877 val_acc : 57.13 lr :0.0000609 epoch_time : 9 sec\n",
            "Train Epoch 46:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 46:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Sun Apr 28 22:30:57 2024 Epoch 46, lr: 0.0000609, val loss: 119.38278, acc: 57.76000\n",
            "Epoch : 46 train_loss :1.740 , val_loss :119.383 val_acc : 57.76 lr :0.0000594 epoch_time : 10 sec\n",
            "Train Epoch 47:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 47:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Sun Apr 28 22:31:07 2024 Epoch 47, lr: 0.0000594, val loss: 117.81495, acc: 58.24000\n",
            "Epoch : 47 train_loss :1.735 , val_loss :117.815 val_acc : 58.24 lr :0.0000578 epoch_time : 10 sec\n",
            "Train Epoch 48:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 48:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Sun Apr 28 22:31:17 2024 Epoch 48, lr: 0.0000578, val loss: 117.64267, acc: 58.58000\n",
            "Epoch : 48 train_loss :1.734 , val_loss :117.643 val_acc : 58.58 lr :0.0000563 epoch_time : 9 sec\n",
            "Train Epoch 49:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 49:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Sun Apr 28 22:31:28 2024 Epoch 49, lr: 0.0000563, val loss: 116.49284, acc: 58.68000\n",
            "Epoch : 49 train_loss :1.730 , val_loss :116.493 val_acc : 58.68 lr :0.0000547 epoch_time : 10 sec\n",
            "Train Epoch 50:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 50:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Current time : Sun Apr 28 22:31:37 2024 Epoch 50, lr: 0.0000547, val loss: 117.22147, acc: 58.46000\n",
            "Epoch : 50 train_loss :1.729 , val_loss :117.221 val_acc : 58.46 lr :0.0000531 epoch_time : 9 sec\n",
            "Train Epoch 51:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 51:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Sun Apr 28 22:31:48 2024 Epoch 51, lr: 0.0000531, val loss: 115.64011, acc: 59.08000\n",
            "Epoch : 51 train_loss :1.716 , val_loss :115.640 val_acc : 59.08 lr :0.0000516 epoch_time : 10 sec\n",
            "Train Epoch 52:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 52:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Current time : Sun Apr 28 22:31:57 2024 Epoch 52, lr: 0.0000516, val loss: 115.82530, acc: 59.06000\n",
            "Epoch : 52 train_loss :1.718 , val_loss :115.825 val_acc : 59.06 lr :0.0000500 epoch_time : 9 sec\n",
            "Train Epoch 53:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 53:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Sun Apr 28 22:32:08 2024 Epoch 53, lr: 0.0000500, val loss: 114.10659, acc: 59.29000\n",
            "Epoch : 53 train_loss :1.715 , val_loss :114.107 val_acc : 59.29 lr :0.0000484 epoch_time : 10 sec\n",
            "Train Epoch 54:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 54:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Current time : Sun Apr 28 22:32:17 2024 Epoch 54, lr: 0.0000484, val loss: 114.84770, acc: 58.89000\n",
            "Epoch : 54 train_loss :1.711 , val_loss :114.848 val_acc : 58.89 lr :0.0000469 epoch_time : 9 sec\n",
            "Train Epoch 55:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 55:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Sun Apr 28 22:32:27 2024 Epoch 55, lr: 0.0000469, val loss: 112.53769, acc: 59.86000\n",
            "Epoch : 55 train_loss :1.711 , val_loss :112.538 val_acc : 59.86 lr :0.0000453 epoch_time : 10 sec\n",
            "Train Epoch 56:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 56:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Current time : Sun Apr 28 22:32:37 2024 Epoch 56, lr: 0.0000453, val loss: 112.90664, acc: 59.38000\n",
            "Epoch : 56 train_loss :1.692 , val_loss :112.907 val_acc : 59.38 lr :0.0000437 epoch_time : 9 sec\n",
            "Train Epoch 57:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 57:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Current time : Sun Apr 28 22:32:47 2024 Epoch 57, lr: 0.0000437, val loss: 112.53511, acc: 59.40000\n",
            "Epoch : 57 train_loss :1.696 , val_loss :112.535 val_acc : 59.4 lr :0.0000422 epoch_time : 9 sec\n",
            "Train Epoch 58:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 58:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Sun Apr 28 22:32:57 2024 Epoch 58, lr: 0.0000422, val loss: 111.30052, acc: 60.04000\n",
            "Epoch : 58 train_loss :1.691 , val_loss :111.301 val_acc : 60.04 lr :0.0000406 epoch_time : 10 sec\n",
            "Train Epoch 59:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 59:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Sun Apr 28 22:33:07 2024 Epoch 59, lr: 0.0000406, val loss: 111.59784, acc: 60.22000\n",
            "Epoch : 59 train_loss :1.688 , val_loss :111.598 val_acc : 60.22 lr :0.0000391 epoch_time : 10 sec\n",
            "Train Epoch 60:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 60:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Sun Apr 28 22:33:17 2024 Epoch 60, lr: 0.0000391, val loss: 110.12789, acc: 60.66000\n",
            "Epoch : 60 train_loss :1.692 , val_loss :110.128 val_acc : 60.66 lr :0.0000376 epoch_time : 10 sec\n",
            "Train Epoch 61:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 61:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Current time : Sun Apr 28 22:33:27 2024 Epoch 61, lr: 0.0000376, val loss: 110.11714, acc: 60.65000\n",
            "Epoch : 61 train_loss :1.690 , val_loss :110.117 val_acc : 60.65 lr :0.0000361 epoch_time : 9 sec\n",
            "Train Epoch 62:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 62:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Sun Apr 28 22:33:37 2024 Epoch 62, lr: 0.0000361, val loss: 109.22306, acc: 60.78000\n",
            "Epoch : 62 train_loss :1.680 , val_loss :109.223 val_acc : 60.78 lr :0.0000345 epoch_time : 10 sec\n",
            "Train Epoch 63:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 63:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Sun Apr 28 22:33:47 2024 Epoch 63, lr: 0.0000345, val loss: 108.99776, acc: 60.92000\n",
            "Epoch : 63 train_loss :1.679 , val_loss :108.998 val_acc : 60.92 lr :0.0000331 epoch_time : 10 sec\n",
            "Train Epoch 64:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 64:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Sun Apr 28 22:33:58 2024 Epoch 64, lr: 0.0000331, val loss: 109.55609, acc: 61.08000\n",
            "Epoch : 64 train_loss :1.678 , val_loss :109.556 val_acc : 61.08 lr :0.0000316 epoch_time : 10 sec\n",
            "Train Epoch 65:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 65:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Sun Apr 28 22:34:08 2024 Epoch 65, lr: 0.0000316, val loss: 108.38528, acc: 61.65000\n",
            "Epoch : 65 train_loss :1.671 , val_loss :108.385 val_acc : 61.65 lr :0.0000301 epoch_time : 10 sec\n",
            "Train Epoch 66:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 66:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Current time : Sun Apr 28 22:34:18 2024 Epoch 66, lr: 0.0000301, val loss: 107.59155, acc: 61.16000\n",
            "Epoch : 66 train_loss :1.665 , val_loss :107.592 val_acc : 61.16 lr :0.0000287 epoch_time : 10 sec\n",
            "Train Epoch 67:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 67:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Current time : Sun Apr 28 22:34:28 2024 Epoch 67, lr: 0.0000287, val loss: 108.61617, acc: 60.99000\n",
            "Epoch : 67 train_loss :1.669 , val_loss :108.616 val_acc : 60.99 lr :0.0000273 epoch_time : 9 sec\n",
            "Train Epoch 68:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 68:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Current time : Sun Apr 28 22:34:38 2024 Epoch 68, lr: 0.0000273, val loss: 107.62581, acc: 61.19000\n",
            "Epoch : 68 train_loss :1.661 , val_loss :107.626 val_acc : 61.19 lr :0.0000259 epoch_time : 10 sec\n",
            "Train Epoch 69:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 69:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Current time : Sun Apr 28 22:34:48 2024 Epoch 69, lr: 0.0000259, val loss: 107.48710, acc: 61.41000\n",
            "Epoch : 69 train_loss :1.654 , val_loss :107.487 val_acc : 61.41 lr :0.0000245 epoch_time : 9 sec\n",
            "Train Epoch 70:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 70:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Current time : Sun Apr 28 22:34:58 2024 Epoch 70, lr: 0.0000245, val loss: 108.01260, acc: 61.50000\n",
            "Epoch : 70 train_loss :1.655 , val_loss :108.013 val_acc : 61.5 lr :0.0000232 epoch_time : 10 sec\n",
            "Train Epoch 71:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 71:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Current time : Sun Apr 28 22:35:08 2024 Epoch 71, lr: 0.0000232, val loss: 107.56654, acc: 61.61000\n",
            "Epoch : 71 train_loss :1.660 , val_loss :107.567 val_acc : 61.61 lr :0.0000219 epoch_time : 9 sec\n",
            "Train Epoch 72:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 72:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Current time : Sun Apr 28 22:35:18 2024 Epoch 72, lr: 0.0000219, val loss: 106.82059, acc: 61.53000\n",
            "Epoch : 72 train_loss :1.652 , val_loss :106.821 val_acc : 61.53 lr :0.0000206 epoch_time : 10 sec\n",
            "Train Epoch 73:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 73:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Sun Apr 28 22:35:28 2024 Epoch 73, lr: 0.0000206, val loss: 106.62591, acc: 61.70000\n",
            "Epoch : 73 train_loss :1.653 , val_loss :106.626 val_acc : 61.7 lr :0.0000194 epoch_time : 10 sec\n",
            "Train Epoch 74:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 74:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Sun Apr 28 22:35:39 2024 Epoch 74, lr: 0.0000194, val loss: 106.17815, acc: 61.85000\n",
            "Epoch : 74 train_loss :1.645 , val_loss :106.178 val_acc : 61.85 lr :0.0000181 epoch_time : 10 sec\n",
            "Train Epoch 75:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 75:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Sun Apr 28 22:35:49 2024 Epoch 75, lr: 0.0000181, val loss: 105.40584, acc: 62.13000\n",
            "Epoch : 75 train_loss :1.648 , val_loss :105.406 val_acc : 62.13 lr :0.0000169 epoch_time : 10 sec\n",
            "Train Epoch 76:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 76:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Sun Apr 28 22:35:59 2024 Epoch 76, lr: 0.0000169, val loss: 105.76182, acc: 62.18000\n",
            "Epoch : 76 train_loss :1.647 , val_loss :105.762 val_acc : 62.18 lr :0.0000158 epoch_time : 10 sec\n",
            "Train Epoch 77:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 77:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Sun Apr 28 22:36:09 2024 Epoch 77, lr: 0.0000158, val loss: 105.54219, acc: 62.34000\n",
            "Epoch : 77 train_loss :1.637 , val_loss :105.542 val_acc : 62.34 lr :0.0000146 epoch_time : 10 sec\n",
            "Train Epoch 78:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 78:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Sun Apr 28 22:36:19 2024 Epoch 78, lr: 0.0000146, val loss: 105.55145, acc: 62.40000\n",
            "Epoch : 78 train_loss :1.636 , val_loss :105.551 val_acc : 62.4 lr :0.0000136 epoch_time : 10 sec\n",
            "Train Epoch 79:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 79:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Current time : Sun Apr 28 22:36:29 2024 Epoch 79, lr: 0.0000136, val loss: 105.55650, acc: 62.26000\n",
            "Epoch : 79 train_loss :1.642 , val_loss :105.557 val_acc : 62.26 lr :0.0000125 epoch_time : 9 sec\n",
            "Train Epoch 80:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 80:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Sun Apr 28 22:36:40 2024 Epoch 80, lr: 0.0000125, val loss: 104.89460, acc: 62.99000\n",
            "Epoch : 80 train_loss :1.633 , val_loss :104.895 val_acc : 62.99 lr :0.0000115 epoch_time : 10 sec\n",
            "Train Epoch 81:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 81:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Current time : Sun Apr 28 22:36:49 2024 Epoch 81, lr: 0.0000115, val loss: 104.94833, acc: 62.79000\n",
            "Epoch : 81 train_loss :1.637 , val_loss :104.948 val_acc : 62.79 lr :0.0000105 epoch_time : 9 sec\n",
            "Train Epoch 82:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 82:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Current time : Sun Apr 28 22:36:59 2024 Epoch 82, lr: 0.0000105, val loss: 104.44024, acc: 62.89000\n",
            "Epoch : 82 train_loss :1.637 , val_loss :104.440 val_acc : 62.89 lr :0.0000095 epoch_time : 9 sec\n",
            "Train Epoch 83:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 83:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Current time : Sun Apr 28 22:37:09 2024 Epoch 83, lr: 0.0000095, val loss: 104.18807, acc: 62.75000\n",
            "Epoch : 83 train_loss :1.635 , val_loss :104.188 val_acc : 62.75 lr :0.0000086 epoch_time : 9 sec\n",
            "Train Epoch 84:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 84:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Current time : Sun Apr 28 22:37:18 2024 Epoch 84, lr: 0.0000086, val loss: 104.60999, acc: 62.87000\n",
            "Epoch : 84 train_loss :1.638 , val_loss :104.610 val_acc : 62.87 lr :0.0000078 epoch_time : 9 sec\n",
            "Train Epoch 85:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 85:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Current time : Sun Apr 28 22:37:28 2024 Epoch 85, lr: 0.0000078, val loss: 104.42579, acc: 62.58000\n",
            "Epoch : 85 train_loss :1.635 , val_loss :104.426 val_acc : 62.58 lr :0.0000070 epoch_time : 9 sec\n",
            "Train Epoch 86:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 86:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Current time : Sun Apr 28 22:37:38 2024 Epoch 86, lr: 0.0000070, val loss: 104.20521, acc: 62.97000\n",
            "Epoch : 86 train_loss :1.636 , val_loss :104.205 val_acc : 62.97 lr :0.0000062 epoch_time : 9 sec\n",
            "Train Epoch 87:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 87:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Current time : Sun Apr 28 22:37:47 2024 Epoch 87, lr: 0.0000062, val loss: 104.71115, acc: 62.68000\n",
            "Epoch : 87 train_loss :1.636 , val_loss :104.711 val_acc : 62.68 lr :0.0000054 epoch_time : 9 sec\n",
            "Train Epoch 88:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 88:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Current time : Sun Apr 28 22:37:57 2024 Epoch 88, lr: 0.0000054, val loss: 104.26672, acc: 62.80000\n",
            "Epoch : 88 train_loss :1.633 , val_loss :104.267 val_acc : 62.8 lr :0.0000048 epoch_time : 9 sec\n",
            "Train Epoch 89:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 89:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Sun Apr 28 22:38:08 2024 Epoch 89, lr: 0.0000048, val loss: 104.27441, acc: 63.03000\n",
            "Epoch : 89 train_loss :1.633 , val_loss :104.274 val_acc : 63.03 lr :0.0000041 epoch_time : 10 sec\n",
            "Train Epoch 90:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 90:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Sun Apr 28 22:38:18 2024 Epoch 90, lr: 0.0000041, val loss: 104.08297, acc: 63.07000\n",
            "Epoch : 90 train_loss :1.629 , val_loss :104.083 val_acc : 63.07 lr :0.0000035 epoch_time : 10 sec\n",
            "Train Epoch 91:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 91:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Current time : Sun Apr 28 22:38:28 2024 Epoch 91, lr: 0.0000035, val loss: 104.38414, acc: 62.97000\n",
            "Epoch : 91 train_loss :1.627 , val_loss :104.384 val_acc : 62.97 lr :0.0000030 epoch_time : 9 sec\n",
            "Train Epoch 92:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 92:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Current time : Sun Apr 28 22:38:37 2024 Epoch 92, lr: 0.0000030, val loss: 103.97908, acc: 62.90000\n",
            "Epoch : 92 train_loss :1.631 , val_loss :103.979 val_acc : 62.9 lr :0.0000024 epoch_time : 9 sec\n",
            "Train Epoch 93:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 93:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Current time : Sun Apr 28 22:38:47 2024 Epoch 93, lr: 0.0000024, val loss: 104.11996, acc: 63.02000\n",
            "Epoch : 93 train_loss :1.637 , val_loss :104.120 val_acc : 63.02 lr :0.0000020 epoch_time : 9 sec\n",
            "Train Epoch 94:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 94:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Current time : Sun Apr 28 22:38:57 2024 Epoch 94, lr: 0.0000020, val loss: 103.83954, acc: 62.93000\n",
            "Epoch : 94 train_loss :1.625 , val_loss :103.840 val_acc : 62.93 lr :0.0000016 epoch_time : 9 sec\n",
            "Train Epoch 95:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 95:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Current time : Sun Apr 28 22:39:07 2024 Epoch 95, lr: 0.0000016, val loss: 103.82520, acc: 63.03000\n",
            "Epoch : 95 train_loss :1.626 , val_loss :103.825 val_acc : 63.03 lr :0.0000012 epoch_time : 9 sec\n",
            "Train Epoch 96:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 96:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Sun Apr 28 22:39:17 2024 Epoch 96, lr: 0.0000012, val loss: 103.72864, acc: 63.12000\n",
            "Epoch : 96 train_loss :1.628 , val_loss :103.729 val_acc : 63.12 lr :0.0000009 epoch_time : 9 sec\n",
            "Train Epoch 97:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 97:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Current time : Sun Apr 28 22:39:26 2024 Epoch 97, lr: 0.0000009, val loss: 103.76225, acc: 63.07000\n",
            "Epoch : 97 train_loss :1.631 , val_loss :103.762 val_acc : 63.07 lr :0.0000006 epoch_time : 9 sec\n",
            "Train Epoch 98:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 98:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Current time : Sun Apr 28 22:39:36 2024 Epoch 98, lr: 0.0000006, val loss: 103.70667, acc: 63.08000\n",
            "Epoch : 98 train_loss :1.628 , val_loss :103.707 val_acc : 63.08 lr :0.0000004 epoch_time : 10 sec\n",
            "Train Epoch 99:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 99:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Current time : Sun Apr 28 22:39:46 2024 Epoch 99, lr: 0.0000004, val loss: 103.66347, acc: 63.11000\n",
            "Epoch : 99 train_loss :1.625 , val_loss :103.663 val_acc : 63.11 lr :0.0000002 epoch_time : 9 sec\n",
            "Train Epoch 100:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 100:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Sun Apr 28 22:39:57 2024 Epoch 100, lr: 0.0000002, val loss: 103.68727, acc: 63.13000\n",
            "Epoch : 100 train_loss :1.633 , val_loss :103.687 val_acc : 63.13 lr :0.0000001 epoch_time : 10 sec\n"
          ]
        }
      ],
      "source": [
        "! python train.py --net Conv_tr --n_epochs 100 # vit-small"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14urPAg4PEUO",
        "outputId": "eb380f30-c160-4431-d3a8-6f2ba1aa3540",
        "collapsed": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==> Preparing data..\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "==> Building model..\n",
            "cuda\n",
            "\n",
            "Epoch :  1\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:456: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at ../aten/src/ATen/native/Convolution.cpp:1040.)\n",
            "  return F.conv2d(input, weight, bias, self.stride,\n",
            " [================================================================>]  Step: 601ms | Tot: 1m27s | Loss: 2.143 | Acc: 19.002% (9501/50000) 98/98 \n",
            " [================================================================>]  Step: 62ms | Tot: 6s184ms | Loss: 1.697 | Acc: 36.120% (3612/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 06:56:12 2024 Epoch 1, lr: 0.0001000, val loss: 169.74135, acc: 36.12000\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "Epoch : 1 train_loss :2.143 , val_loss :169.741 val_acc : 36.12 lr :0.0000994 epoch_time : 96 sec\n",
            "\n",
            "Epoch :  2\n",
            " [================================================================>]  Step: 596ms | Tot: 1m27s | Loss: 1.943 | Acc: 26.690% (13345/50000) 98/98 \n",
            " [================================================================>]  Step: 62ms | Tot: 6s204ms | Loss: 1.504 | Acc: 45.230% (4523/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 06:57:48 2024 Epoch 2, lr: 0.0000994, val loss: 150.38975, acc: 45.23000\n",
            "Epoch : 2 train_loss :1.943 , val_loss :150.390 val_acc : 45.23 lr :0.0001000 epoch_time : 95 sec\n",
            "\n",
            "Epoch :  3\n",
            " [================================================================>]  Step: 596ms | Tot: 1m27s | Loss: 1.861 | Acc: 30.544% (15272/50000) 98/98 \n",
            " [================================================================>]  Step: 62ms | Tot: 6s205ms | Loss: 1.425 | Acc: 47.800% (4780/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 06:59:23 2024 Epoch 3, lr: 0.0001000, val loss: 142.51343, acc: 47.80000\n",
            "Epoch : 3 train_loss :1.861 , val_loss :142.513 val_acc : 47.8 lr :0.0000994 epoch_time : 95 sec\n",
            "\n",
            "Epoch :  4\n",
            " [================================================================>]  Step: 596ms | Tot: 1m27s | Loss: 1.800 | Acc: 33.106% (16553/50000) 98/98 \n",
            " [================================================================>]  Step: 62ms | Tot: 6s197ms | Loss: 1.312 | Acc: 52.760% (5276/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 07:00:58 2024 Epoch 4, lr: 0.0000994, val loss: 131.23693, acc: 52.76000\n",
            "Epoch : 4 train_loss :1.800 , val_loss :131.237 val_acc : 52.76 lr :0.0000976 epoch_time : 95 sec\n",
            "\n",
            "Epoch :  5\n",
            " [================================================================>]  Step: 597ms | Tot: 1m27s | Loss: 1.746 | Acc: 35.208% (17604/50000) 98/98 \n",
            " [================================================================>]  Step: 62ms | Tot: 6s193ms | Loss: 1.274 | Acc: 53.940% (5394/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 07:02:34 2024 Epoch 5, lr: 0.0000976, val loss: 127.40881, acc: 53.94000\n",
            "Epoch : 5 train_loss :1.746 , val_loss :127.409 val_acc : 53.94 lr :0.0000946 epoch_time : 95 sec\n",
            "\n",
            "Epoch :  6\n",
            " [================================================================>]  Step: 597ms | Tot: 1m27s | Loss: 1.705 | Acc: 36.808% (18404/50000) 98/98 \n",
            " [================================================================>]  Step: 62ms | Tot: 6s202ms | Loss: 1.193 | Acc: 57.130% (5713/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 07:04:09 2024 Epoch 6, lr: 0.0000946, val loss: 119.26698, acc: 57.13000\n",
            "Epoch : 6 train_loss :1.705 , val_loss :119.267 val_acc : 57.13 lr :0.0000905 epoch_time : 95 sec\n",
            "\n",
            "Epoch :  7\n",
            " [================================================================>]  Step: 596ms | Tot: 1m27s | Loss: 1.669 | Acc: 38.244% (19122/50000) 98/98 \n",
            " [================================================================>]  Step: 62ms | Tot: 6s201ms | Loss: 1.192 | Acc: 56.970% (5697/10000) 100/100 \n",
            "Current time : Sat Apr 27 07:05:44 2024 Epoch 7, lr: 0.0000905, val loss: 119.22000, acc: 56.97000\n",
            "Epoch : 7 train_loss :1.669 , val_loss :119.220 val_acc : 56.97 lr :0.0000854 epoch_time : 95 sec\n",
            "\n",
            "Epoch :  8\n",
            " [================================================================>]  Step: 596ms | Tot: 1m27s | Loss: 1.635 | Acc: 39.344% (19672/50000) 98/98 \n",
            " [================================================================>]  Step: 62ms | Tot: 6s197ms | Loss: 1.078 | Acc: 61.650% (6165/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 07:07:20 2024 Epoch 8, lr: 0.0000854, val loss: 107.83618, acc: 61.65000\n",
            "Epoch : 8 train_loss :1.635 , val_loss :107.836 val_acc : 61.65 lr :0.0000794 epoch_time : 95 sec\n",
            "\n",
            "Epoch :  9\n",
            " [================================================================>]  Step: 597ms | Tot: 1m27s | Loss: 1.610 | Acc: 40.312% (20156/50000) 98/98 \n",
            " [================================================================>]  Step: 62ms | Tot: 6s198ms | Loss: 1.062 | Acc: 61.960% (6196/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 07:08:55 2024 Epoch 9, lr: 0.0000794, val loss: 106.19205, acc: 61.96000\n",
            "Epoch : 9 train_loss :1.610 , val_loss :106.192 val_acc : 61.96 lr :0.0000727 epoch_time : 95 sec\n",
            "\n",
            "Epoch :  10\n",
            " [================================================================>]  Step: 596ms | Tot: 1m27s | Loss: 1.581 | Acc: 41.636% (20818/50000) 98/98 \n",
            " [================================================================>]  Step: 62ms | Tot: 6s203ms | Loss: 1.067 | Acc: 62.120% (6212/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 07:10:30 2024 Epoch 10, lr: 0.0000727, val loss: 106.69808, acc: 62.12000\n",
            "Epoch : 10 train_loss :1.581 , val_loss :106.698 val_acc : 62.12 lr :0.0000655 epoch_time : 95 sec\n",
            "\n",
            "Epoch :  11\n",
            " [================================================================>]  Step: 596ms | Tot: 1m27s | Loss: 1.562 | Acc: 42.166% (21083/50000) 98/98 \n",
            " [================================================================>]  Step: 62ms | Tot: 6s191ms | Loss: 1.015 | Acc: 63.460% (6346/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 07:12:06 2024 Epoch 11, lr: 0.0000655, val loss: 101.53427, acc: 63.46000\n",
            "Epoch : 11 train_loss :1.562 , val_loss :101.534 val_acc : 63.46 lr :0.0000578 epoch_time : 95 sec\n",
            "\n",
            "Epoch :  12\n",
            " [================================================================>]  Step: 597ms | Tot: 1m27s | Loss: 1.537 | Acc: 43.264% (21632/50000) 98/98 \n",
            " [================================================================>]  Step: 62ms | Tot: 6s187ms | Loss: 1.017 | Acc: 63.580% (6358/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 07:13:41 2024 Epoch 12, lr: 0.0000578, val loss: 101.74127, acc: 63.58000\n",
            "Epoch : 12 train_loss :1.537 , val_loss :101.741 val_acc : 63.58 lr :0.0000500 epoch_time : 95 sec\n",
            "\n",
            "Epoch :  13\n",
            " [================================================================>]  Step: 596ms | Tot: 1m27s | Loss: 1.526 | Acc: 43.704% (21852/50000) 98/98 \n",
            " [================================================================>]  Step: 62ms | Tot: 6s201ms | Loss: 0.957 | Acc: 65.360% (6536/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 07:15:16 2024 Epoch 13, lr: 0.0000500, val loss: 95.69482, acc: 65.36000\n",
            "Epoch : 13 train_loss :1.526 , val_loss :95.695 val_acc : 65.36 lr :0.0000422 epoch_time : 95 sec\n",
            "\n",
            "Epoch :  14\n",
            " [================================================================>]  Step: 596ms | Tot: 1m27s | Loss: 1.513 | Acc: 44.184% (22092/50000) 98/98 \n",
            " [================================================================>]  Step: 62ms | Tot: 6s202ms | Loss: 0.963 | Acc: 65.500% (6550/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 07:16:52 2024 Epoch 14, lr: 0.0000422, val loss: 96.27361, acc: 65.50000\n",
            "Epoch : 14 train_loss :1.513 , val_loss :96.274 val_acc : 65.5 lr :0.0000345 epoch_time : 95 sec\n",
            "\n",
            "Epoch :  15\n",
            " [================================================================>]  Step: 597ms | Tot: 1m27s | Loss: 1.489 | Acc: 45.138% (22569/50000) 98/98 \n",
            " [================================================================>]  Step: 62ms | Tot: 6s191ms | Loss: 0.937 | Acc: 67.050% (6705/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 07:18:27 2024 Epoch 15, lr: 0.0000345, val loss: 93.68796, acc: 67.05000\n",
            "Epoch : 15 train_loss :1.489 , val_loss :93.688 val_acc : 67.05 lr :0.0000273 epoch_time : 95 sec\n",
            "\n",
            "Epoch :  16\n",
            " [================================================================>]  Step: 597ms | Tot: 1m27s | Loss: 1.477 | Acc: 45.616% (22808/50000) 98/98 \n",
            " [================================================================>]  Step: 62ms | Tot: 6s202ms | Loss: 0.909 | Acc: 67.600% (6760/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 07:20:03 2024 Epoch 16, lr: 0.0000273, val loss: 90.87957, acc: 67.60000\n",
            "Epoch : 16 train_loss :1.477 , val_loss :90.880 val_acc : 67.6 lr :0.0000206 epoch_time : 95 sec\n",
            "\n",
            "Epoch :  17\n",
            " [================================================================>]  Step: 597ms | Tot: 1m27s | Loss: 1.473 | Acc: 45.496% (22748/50000) 98/98 \n",
            " [================================================================>]  Step: 62ms | Tot: 6s189ms | Loss: 0.892 | Acc: 68.490% (6849/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 07:21:38 2024 Epoch 17, lr: 0.0000206, val loss: 89.20680, acc: 68.49000\n",
            "Epoch : 17 train_loss :1.473 , val_loss :89.207 val_acc : 68.49 lr :0.0000146 epoch_time : 95 sec\n",
            "\n",
            "Epoch :  18\n",
            " [================================================================>]  Step: 596ms | Tot: 1m27s | Loss: 1.464 | Acc: 45.970% (22985/50000) 98/98 \n",
            " [================================================================>]  Step: 62ms | Tot: 6s192ms | Loss: 0.887 | Acc: 68.850% (6885/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 07:23:13 2024 Epoch 18, lr: 0.0000146, val loss: 88.65391, acc: 68.85000\n",
            "Epoch : 18 train_loss :1.464 , val_loss :88.654 val_acc : 68.85 lr :0.0000095 epoch_time : 95 sec\n",
            "\n",
            "Epoch :  19\n",
            " [================================================================>]  Step: 596ms | Tot: 1m27s | Loss: 1.461 | Acc: 46.012% (23006/50000) 98/98 \n",
            " [================================================================>]  Step: 62ms | Tot: 6s202ms | Loss: 0.888 | Acc: 68.560% (6856/10000) 100/100 \n",
            "Current time : Sat Apr 27 07:24:49 2024 Epoch 19, lr: 0.0000095, val loss: 88.80928, acc: 68.56000\n",
            "Epoch : 19 train_loss :1.461 , val_loss :88.809 val_acc : 68.56 lr :0.0000054 epoch_time : 95 sec\n",
            "\n",
            "Epoch :  20\n",
            " [================================================================>]  Step: 597ms | Tot: 1m27s | Loss: 1.448 | Acc: 46.302% (23151/50000) 98/98 \n",
            " [================================================================>]  Step: 62ms | Tot: 6s194ms | Loss: 0.881 | Acc: 68.970% (6897/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 07:26:24 2024 Epoch 20, lr: 0.0000054, val loss: 88.11122, acc: 68.97000\n",
            "Epoch : 20 train_loss :1.448 , val_loss :88.111 val_acc : 68.97 lr :0.0000024 epoch_time : 95 sec\n"
          ]
        }
      ],
      "source": [
        "! python train.py --net convmixer --n_epochs 20 # train with convmixer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RTK99GloO1Vk",
        "outputId": "7db7ec1c-5724-4f3f-fab6-59a9b0b676b1",
        "collapsed": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==> Preparing data..\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "==> Building model..\n",
            "cuda\n",
            "\n",
            "Epoch :  1\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            " [================================================================>]  Step: 50ms | Tot: 5s203ms | Loss: 2.198 | Acc: 17.456% (8728/50000) 98/98 \n",
            " [================================================================>]  Step: 6ms | Tot: 852ms | Loss: 1.803 | Acc: 34.670% (3467/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 07:26:39 2024 Epoch 1, lr: 0.0010000, val loss: 180.33532, acc: 34.67000\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "Epoch : 1 train_loss :2.198 , val_loss :180.335 val_acc : 34.67 lr :0.0009938 epoch_time : 7 sec\n",
            "\n",
            "Epoch :  2\n",
            " [================================================================>]  Step: 26ms | Tot: 5s625ms | Loss: 2.016 | Acc: 24.814% (12407/50000) 98/98 \n",
            " [================================================================>]  Step: 6ms | Tot: 786ms | Loss: 1.547 | Acc: 43.650% (4365/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 07:26:46 2024 Epoch 2, lr: 0.0009938, val loss: 154.69671, acc: 43.65000\n",
            "Epoch : 2 train_loss :2.016 , val_loss :154.697 val_acc : 43.65 lr :0.0010000 epoch_time : 7 sec\n",
            "\n",
            "Epoch :  3\n",
            " [================================================================>]  Step: 26ms | Tot: 5s938ms | Loss: 1.906 | Acc: 29.296% (14648/50000) 98/98 \n",
            " [================================================================>]  Step: 6ms | Tot: 827ms | Loss: 1.478 | Acc: 47.190% (4719/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 07:26:54 2024 Epoch 3, lr: 0.0010000, val loss: 147.76300, acc: 47.19000\n",
            "Epoch : 3 train_loss :1.906 , val_loss :147.763 val_acc : 47.19 lr :0.0009938 epoch_time : 7 sec\n",
            "\n",
            "Epoch :  4\n",
            " [================================================================>]  Step: 27ms | Tot: 5s679ms | Loss: 1.830 | Acc: 32.520% (16260/50000) 98/98 \n",
            " [================================================================>]  Step: 7ms | Tot: 843ms | Loss: 1.331 | Acc: 52.340% (5234/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 07:27:02 2024 Epoch 4, lr: 0.0009938, val loss: 133.13059, acc: 52.34000\n",
            "Epoch : 4 train_loss :1.830 , val_loss :133.131 val_acc : 52.34 lr :0.0009755 epoch_time : 7 sec\n",
            "\n",
            "Epoch :  5\n",
            " [================================================================>]  Step: 28ms | Tot: 5s486ms | Loss: 1.761 | Acc: 35.130% (17565/50000) 98/98 \n",
            " [================================================================>]  Step: 5ms | Tot: 781ms | Loss: 1.206 | Acc: 55.970% (5597/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 07:27:09 2024 Epoch 5, lr: 0.0009755, val loss: 120.59540, acc: 55.97000\n",
            "Epoch : 5 train_loss :1.761 , val_loss :120.595 val_acc : 55.97 lr :0.0009455 epoch_time : 7 sec\n",
            "\n",
            "Epoch :  6\n",
            " [================================================================>]  Step: 28ms | Tot: 5s588ms | Loss: 1.713 | Acc: 37.058% (18529/50000) 98/98 \n",
            " [================================================================>]  Step: 6ms | Tot: 736ms | Loss: 1.162 | Acc: 57.850% (5785/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 07:27:17 2024 Epoch 6, lr: 0.0009455, val loss: 116.23819, acc: 57.85000\n",
            "Epoch : 6 train_loss :1.713 , val_loss :116.238 val_acc : 57.85 lr :0.0009045 epoch_time : 7 sec\n",
            "\n",
            "Epoch :  7\n",
            " [================================================================>]  Step: 27ms | Tot: 5s481ms | Loss: 1.670 | Acc: 38.486% (19243/50000) 98/98 \n",
            " [================================================================>]  Step: 7ms | Tot: 809ms | Loss: 1.084 | Acc: 61.890% (6189/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 07:27:24 2024 Epoch 7, lr: 0.0009045, val loss: 108.39624, acc: 61.89000\n",
            "Epoch : 7 train_loss :1.670 , val_loss :108.396 val_acc : 61.89 lr :0.0008536 epoch_time : 7 sec\n",
            "\n",
            "Epoch :  8\n",
            " [================================================================>]  Step: 27ms | Tot: 5s819ms | Loss: 1.632 | Acc: 40.004% (20002/50000) 98/98 \n",
            " [================================================================>]  Step: 6ms | Tot: 785ms | Loss: 1.072 | Acc: 61.640% (6164/10000) 100/100 \n",
            "Current time : Sat Apr 27 07:27:32 2024 Epoch 8, lr: 0.0008536, val loss: 107.15086, acc: 61.64000\n",
            "Epoch : 8 train_loss :1.632 , val_loss :107.151 val_acc : 61.64 lr :0.0007939 epoch_time : 7 sec\n",
            "\n",
            "Epoch :  9\n",
            " [================================================================>]  Step: 26ms | Tot: 5s764ms | Loss: 1.599 | Acc: 41.022% (20511/50000) 98/98 \n",
            " [================================================================>]  Step: 6ms | Tot: 853ms | Loss: 0.983 | Acc: 64.790% (6479/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 07:27:39 2024 Epoch 9, lr: 0.0007939, val loss: 98.34850, acc: 64.79000\n",
            "Epoch : 9 train_loss :1.599 , val_loss :98.348 val_acc : 64.79 lr :0.0007270 epoch_time : 7 sec\n",
            "\n",
            "Epoch :  10\n",
            " [================================================================>]  Step: 26ms | Tot: 5s591ms | Loss: 1.563 | Acc: 42.698% (21349/50000) 98/98 \n",
            " [================================================================>]  Step: 6ms | Tot: 770ms | Loss: 0.971 | Acc: 65.200% (6520/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 07:27:47 2024 Epoch 10, lr: 0.0007270, val loss: 97.12225, acc: 65.20000\n",
            "Epoch : 10 train_loss :1.563 , val_loss :97.122 val_acc : 65.2 lr :0.0006545 epoch_time : 7 sec\n",
            "\n",
            "Epoch :  11\n",
            " [================================================================>]  Step: 26ms | Tot: 5s704ms | Loss: 1.545 | Acc: 43.064% (21532/50000) 98/98 \n",
            " [================================================================>]  Step: 6ms | Tot: 842ms | Loss: 0.932 | Acc: 67.040% (6704/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 07:27:55 2024 Epoch 11, lr: 0.0006545, val loss: 93.17958, acc: 67.04000\n",
            "Epoch : 11 train_loss :1.545 , val_loss :93.180 val_acc : 67.04 lr :0.0005782 epoch_time : 7 sec\n",
            "\n",
            "Epoch :  12\n",
            " [================================================================>]  Step: 26ms | Tot: 5s650ms | Loss: 1.523 | Acc: 43.860% (21930/50000) 98/98 \n",
            " [================================================================>]  Step: 6ms | Tot: 760ms | Loss: 0.894 | Acc: 68.740% (6874/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 07:28:02 2024 Epoch 12, lr: 0.0005782, val loss: 89.43780, acc: 68.74000\n",
            "Epoch : 12 train_loss :1.523 , val_loss :89.438 val_acc : 68.74 lr :0.0005000 epoch_time : 7 sec\n",
            "\n",
            "Epoch :  13\n",
            " [================================================================>]  Step: 29ms | Tot: 5s428ms | Loss: 1.491 | Acc: 45.190% (22595/50000) 98/98 \n",
            " [================================================================>]  Step: 5ms | Tot: 765ms | Loss: 0.858 | Acc: 69.310% (6931/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 07:28:09 2024 Epoch 13, lr: 0.0005000, val loss: 85.78480, acc: 69.31000\n",
            "Epoch : 13 train_loss :1.491 , val_loss :85.785 val_acc : 69.31 lr :0.0004218 epoch_time : 7 sec\n",
            "\n",
            "Epoch :  14\n",
            " [================================================================>]  Step: 27ms | Tot: 5s837ms | Loss: 1.462 | Acc: 46.384% (23192/50000) 98/98 \n",
            " [================================================================>]  Step: 6ms | Tot: 823ms | Loss: 0.872 | Acc: 68.530% (6853/10000) 100/100 \n",
            "Current time : Sat Apr 27 07:28:17 2024 Epoch 14, lr: 0.0004218, val loss: 87.16304, acc: 68.53000\n",
            "Epoch : 14 train_loss :1.462 , val_loss :87.163 val_acc : 68.53 lr :0.0003455 epoch_time : 7 sec\n",
            "\n",
            "Epoch :  15\n",
            " [================================================================>]  Step: 26ms | Tot: 5s608ms | Loss: 1.452 | Acc: 46.682% (23341/50000) 98/98 \n",
            " [================================================================>]  Step: 6ms | Tot: 758ms | Loss: 0.821 | Acc: 70.830% (7083/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 07:28:24 2024 Epoch 15, lr: 0.0003455, val loss: 82.10527, acc: 70.83000\n",
            "Epoch : 15 train_loss :1.452 , val_loss :82.105 val_acc : 70.83 lr :0.0002730 epoch_time : 7 sec\n",
            "\n",
            "Epoch :  16\n",
            " [================================================================>]  Step: 28ms | Tot: 5s629ms | Loss: 1.419 | Acc: 47.754% (23877/50000) 98/98 \n",
            " [================================================================>]  Step: 6ms | Tot: 771ms | Loss: 0.809 | Acc: 71.130% (7113/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 07:28:32 2024 Epoch 16, lr: 0.0002730, val loss: 80.89553, acc: 71.13000\n",
            "Epoch : 16 train_loss :1.419 , val_loss :80.896 val_acc : 71.13 lr :0.0002061 epoch_time : 7 sec\n",
            "\n",
            "Epoch :  17\n",
            " [================================================================>]  Step: 26ms | Tot: 5s901ms | Loss: 1.407 | Acc: 48.216% (24108/50000) 98/98 \n",
            " [================================================================>]  Step: 6ms | Tot: 837ms | Loss: 0.791 | Acc: 72.210% (7221/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 07:28:40 2024 Epoch 17, lr: 0.0002061, val loss: 79.12408, acc: 72.21000\n",
            "Epoch : 17 train_loss :1.407 , val_loss :79.124 val_acc : 72.21 lr :0.0001464 epoch_time : 7 sec\n",
            "\n",
            "Epoch :  18\n",
            " [================================================================>]  Step: 26ms | Tot: 5s698ms | Loss: 1.394 | Acc: 48.770% (24385/50000) 98/98 \n",
            " [================================================================>]  Step: 6ms | Tot: 781ms | Loss: 0.775 | Acc: 72.730% (7273/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 07:28:47 2024 Epoch 18, lr: 0.0001464, val loss: 77.45036, acc: 72.73000\n",
            "Epoch : 18 train_loss :1.394 , val_loss :77.450 val_acc : 72.73 lr :0.0000955 epoch_time : 7 sec\n",
            "\n",
            "Epoch :  19\n",
            " [================================================================>]  Step: 27ms | Tot: 5s804ms | Loss: 1.379 | Acc: 49.190% (24595/50000) 98/98 \n",
            " [================================================================>]  Step: 6ms | Tot: 754ms | Loss: 0.758 | Acc: 73.200% (7320/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 07:28:55 2024 Epoch 19, lr: 0.0000955, val loss: 75.81500, acc: 73.20000\n",
            "Epoch : 19 train_loss :1.379 , val_loss :75.815 val_acc : 73.2 lr :0.0000545 epoch_time : 7 sec\n",
            "\n",
            "Epoch :  20\n",
            " [================================================================>]  Step: 26ms | Tot: 5s595ms | Loss: 1.369 | Acc: 49.534% (24767/50000) 98/98 \n",
            " [================================================================>]  Step: 5ms | Tot: 784ms | Loss: 0.750 | Acc: 73.480% (7348/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 07:29:02 2024 Epoch 20, lr: 0.0000545, val loss: 75.04808, acc: 73.48000\n",
            "Epoch : 20 train_loss :1.369 , val_loss :75.048 val_acc : 73.48 lr :0.0000245 epoch_time : 7 sec\n"
          ]
        }
      ],
      "source": [
        "! python train.py --net mlpmixer --n_epochs 20 --lr 1e-3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2C0r5BpPO1YE",
        "outputId": "655b917d-e80c-4e94-a137-f6dca2fbe3ba",
        "collapsed": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==> Preparing data..\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "==> Building model..\n",
            "cuda\n",
            "\n",
            "Epoch :  1\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            " [================================================================>]  Step: 63ms | Tot: 7s83ms | Loss: 2.272 | Acc: 14.046% (7023/50000) 98/98 \n",
            " [================================================================>]  Step: 15ms | Tot: 1s536ms | Loss: 1.982 | Acc: 30.350% (3035/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 07:29:19 2024 Epoch 1, lr: 0.0001000, val loss: 198.18606, acc: 30.35000\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "Epoch : 1 train_loss :2.272 , val_loss :198.186 val_acc : 30.35 lr :0.0000994 epoch_time : 10 sec\n",
            "\n",
            "Epoch :  2\n",
            " [================================================================>]  Step: 49ms | Tot: 7s103ms | Loss: 2.182 | Acc: 18.616% (9308/50000) 98/98 \n",
            " [================================================================>]  Step: 16ms | Tot: 1s472ms | Loss: 1.818 | Acc: 35.840% (3584/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 07:29:29 2024 Epoch 2, lr: 0.0000994, val loss: 181.75608, acc: 35.84000\n",
            "Epoch : 2 train_loss :2.182 , val_loss :181.756 val_acc : 35.84 lr :0.0001000 epoch_time : 9 sec\n",
            "\n",
            "Epoch :  3\n",
            " [================================================================>]  Step: 53ms | Tot: 7s128ms | Loss: 2.126 | Acc: 20.580% (10290/50000) 98/98 \n",
            " [================================================================>]  Step: 13ms | Tot: 1s491ms | Loss: 1.763 | Acc: 36.610% (3661/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 07:29:39 2024 Epoch 3, lr: 0.0001000, val loss: 176.32308, acc: 36.61000\n",
            "Epoch : 3 train_loss :2.126 , val_loss :176.323 val_acc : 36.61 lr :0.0000994 epoch_time : 10 sec\n",
            "\n",
            "Epoch :  4\n",
            " [================================================================>]  Step: 52ms | Tot: 7s117ms | Loss: 2.086 | Acc: 22.328% (11164/50000) 98/98 \n",
            " [================================================================>]  Step: 15ms | Tot: 1s526ms | Loss: 1.734 | Acc: 37.330% (3733/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 07:29:49 2024 Epoch 4, lr: 0.0000994, val loss: 173.41369, acc: 37.33000\n",
            "Epoch : 4 train_loss :2.086 , val_loss :173.414 val_acc : 37.33 lr :0.0000976 epoch_time : 10 sec\n",
            "\n",
            "Epoch :  5\n",
            " [================================================================>]  Step: 53ms | Tot: 7s76ms | Loss: 2.059 | Acc: 23.406% (11703/50000) 98/98 \n",
            " [================================================================>]  Step: 13ms | Tot: 1s482ms | Loss: 1.685 | Acc: 39.080% (3908/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 07:30:00 2024 Epoch 5, lr: 0.0000976, val loss: 168.54960, acc: 39.08000\n",
            "Epoch : 5 train_loss :2.059 , val_loss :168.550 val_acc : 39.08 lr :0.0000946 epoch_time : 10 sec\n",
            "\n",
            "Epoch :  6\n",
            " [================================================================>]  Step: 47ms | Tot: 7s154ms | Loss: 2.028 | Acc: 24.632% (12316/50000) 98/98 \n",
            " [================================================================>]  Step: 13ms | Tot: 1s499ms | Loss: 1.622 | Acc: 41.490% (4149/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 07:30:10 2024 Epoch 6, lr: 0.0000946, val loss: 162.22156, acc: 41.49000\n",
            "Epoch : 6 train_loss :2.028 , val_loss :162.222 val_acc : 41.49 lr :0.0000905 epoch_time : 10 sec\n",
            "\n",
            "Epoch :  7\n",
            " [================================================================>]  Step: 52ms | Tot: 7s77ms | Loss: 2.023 | Acc: 25.016% (12508/50000) 98/98 \n",
            " [================================================================>]  Step: 13ms | Tot: 1s498ms | Loss: 1.603 | Acc: 42.320% (4232/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 07:30:20 2024 Epoch 7, lr: 0.0000905, val loss: 160.26683, acc: 42.32000\n",
            "Epoch : 7 train_loss :2.023 , val_loss :160.267 val_acc : 42.32 lr :0.0000854 epoch_time : 9 sec\n",
            "\n",
            "Epoch :  8\n",
            " [================================================================>]  Step: 50ms | Tot: 7s60ms | Loss: 2.006 | Acc: 25.640% (12820/50000) 98/98 \n",
            " [================================================================>]  Step: 14ms | Tot: 1s493ms | Loss: 1.571 | Acc: 44.190% (4419/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 07:30:30 2024 Epoch 8, lr: 0.0000854, val loss: 157.08744, acc: 44.19000\n",
            "Epoch : 8 train_loss :2.006 , val_loss :157.087 val_acc : 44.19 lr :0.0000794 epoch_time : 9 sec\n",
            "\n",
            "Epoch :  9\n",
            " [================================================================>]  Step: 53ms | Tot: 6s954ms | Loss: 1.986 | Acc: 26.504% (13252/50000) 98/98 \n",
            " [================================================================>]  Step: 14ms | Tot: 1s483ms | Loss: 1.565 | Acc: 44.830% (4483/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 07:30:40 2024 Epoch 9, lr: 0.0000794, val loss: 156.52158, acc: 44.83000\n",
            "Epoch : 9 train_loss :1.986 , val_loss :156.522 val_acc : 44.83 lr :0.0000727 epoch_time : 9 sec\n",
            "\n",
            "Epoch :  10\n",
            " [================================================================>]  Step: 52ms | Tot: 6s937ms | Loss: 1.970 | Acc: 26.992% (13496/50000) 98/98 \n",
            " [================================================================>]  Step: 15ms | Tot: 1s478ms | Loss: 1.526 | Acc: 45.630% (4563/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 07:30:50 2024 Epoch 10, lr: 0.0000727, val loss: 152.57996, acc: 45.63000\n",
            "Epoch : 10 train_loss :1.970 , val_loss :152.580 val_acc : 45.63 lr :0.0000655 epoch_time : 10 sec\n",
            "\n",
            "Epoch :  11\n",
            " [================================================================>]  Step: 47ms | Tot: 6s898ms | Loss: 1.960 | Acc: 27.644% (13822/50000) 98/98 \n",
            " [================================================================>]  Step: 14ms | Tot: 1s465ms | Loss: 1.533 | Acc: 46.000% (4600/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 07:31:00 2024 Epoch 11, lr: 0.0000655, val loss: 153.25026, acc: 46.00000\n",
            "Epoch : 11 train_loss :1.960 , val_loss :153.250 val_acc : 46.0 lr :0.0000578 epoch_time : 9 sec\n",
            "\n",
            "Epoch :  12\n",
            " [================================================================>]  Step: 53ms | Tot: 7s52ms | Loss: 1.948 | Acc: 28.114% (14057/50000) 98/98 \n",
            " [================================================================>]  Step: 15ms | Tot: 1s464ms | Loss: 1.491 | Acc: 47.580% (4758/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 07:31:10 2024 Epoch 12, lr: 0.0000578, val loss: 149.11612, acc: 47.58000\n",
            "Epoch : 12 train_loss :1.948 , val_loss :149.116 val_acc : 47.58 lr :0.0000500 epoch_time : 9 sec\n",
            "\n",
            "Epoch :  13\n",
            " [================================================================>]  Step: 49ms | Tot: 7s113ms | Loss: 1.939 | Acc: 28.494% (14247/50000) 98/98 \n",
            " [================================================================>]  Step: 13ms | Tot: 1s459ms | Loss: 1.488 | Acc: 47.200% (4720/10000) 100/100 \n",
            "Current time : Sat Apr 27 07:31:19 2024 Epoch 13, lr: 0.0000500, val loss: 148.77229, acc: 47.20000\n",
            "Epoch : 13 train_loss :1.939 , val_loss :148.772 val_acc : 47.2 lr :0.0000422 epoch_time : 9 sec\n",
            "\n",
            "Epoch :  14\n",
            " [================================================================>]  Step: 52ms | Tot: 7s84ms | Loss: 1.927 | Acc: 29.162% (14581/50000) 98/98 \n",
            " [================================================================>]  Step: 13ms | Tot: 1s482ms | Loss: 1.474 | Acc: 47.670% (4767/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 07:31:29 2024 Epoch 14, lr: 0.0000422, val loss: 147.43807, acc: 47.67000\n",
            "Epoch : 14 train_loss :1.927 , val_loss :147.438 val_acc : 47.67 lr :0.0000345 epoch_time : 9 sec\n",
            "\n",
            "Epoch :  15\n",
            " [================================================================>]  Step: 53ms | Tot: 6s919ms | Loss: 1.922 | Acc: 29.206% (14603/50000) 98/98 \n",
            " [================================================================>]  Step: 13ms | Tot: 1s470ms | Loss: 1.444 | Acc: 48.570% (4857/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 07:31:39 2024 Epoch 15, lr: 0.0000345, val loss: 144.35737, acc: 48.57000\n",
            "Epoch : 15 train_loss :1.922 , val_loss :144.357 val_acc : 48.57 lr :0.0000273 epoch_time : 9 sec\n",
            "\n",
            "Epoch :  16\n",
            " [================================================================>]  Step: 42ms | Tot: 6s933ms | Loss: 1.917 | Acc: 29.306% (14653/50000) 98/98 \n",
            " [================================================================>]  Step: 14ms | Tot: 1s552ms | Loss: 1.452 | Acc: 48.410% (4841/10000) 100/100 \n",
            "Current time : Sat Apr 27 07:31:49 2024 Epoch 16, lr: 0.0000273, val loss: 145.21332, acc: 48.41000\n",
            "Epoch : 16 train_loss :1.917 , val_loss :145.213 val_acc : 48.41 lr :0.0000206 epoch_time : 9 sec\n",
            "\n",
            "Epoch :  17\n",
            " [================================================================>]  Step: 52ms | Tot: 6s855ms | Loss: 1.913 | Acc: 29.580% (14790/50000) 98/98 \n",
            " [================================================================>]  Step: 15ms | Tot: 1s492ms | Loss: 1.443 | Acc: 48.580% (4858/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 07:31:59 2024 Epoch 17, lr: 0.0000206, val loss: 144.30619, acc: 48.58000\n",
            "Epoch : 17 train_loss :1.913 , val_loss :144.306 val_acc : 48.58 lr :0.0000146 epoch_time : 9 sec\n",
            "\n",
            "Epoch :  18\n",
            " [================================================================>]  Step: 42ms | Tot: 7s178ms | Loss: 1.906 | Acc: 29.774% (14887/50000) 98/98 \n",
            " [================================================================>]  Step: 15ms | Tot: 1s466ms | Loss: 1.443 | Acc: 48.600% (4860/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 07:32:09 2024 Epoch 18, lr: 0.0000146, val loss: 144.26419, acc: 48.60000\n",
            "Epoch : 18 train_loss :1.906 , val_loss :144.264 val_acc : 48.6 lr :0.0000095 epoch_time : 10 sec\n",
            "\n",
            "Epoch :  19\n",
            " [================================================================>]  Step: 53ms | Tot: 6s987ms | Loss: 1.907 | Acc: 30.028% (15014/50000) 98/98 \n",
            " [================================================================>]  Step: 14ms | Tot: 1s575ms | Loss: 1.441 | Acc: 49.130% (4913/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 07:32:19 2024 Epoch 19, lr: 0.0000095, val loss: 144.11590, acc: 49.13000\n",
            "Epoch : 19 train_loss :1.907 , val_loss :144.116 val_acc : 49.13 lr :0.0000054 epoch_time : 10 sec\n",
            "\n",
            "Epoch :  20\n",
            " [================================================================>]  Step: 54ms | Tot: 7s158ms | Loss: 1.894 | Acc: 30.368% (15184/50000) 98/98 \n",
            " [================================================================>]  Step: 15ms | Tot: 1s473ms | Loss: 1.437 | Acc: 48.870% (4887/10000) 100/100 \n",
            "Current time : Sat Apr 27 07:32:28 2024 Epoch 20, lr: 0.0000054, val loss: 143.67944, acc: 48.87000\n",
            "Epoch : 20 train_loss :1.894 , val_loss :143.679 val_acc : 48.87 lr :0.0000024 epoch_time : 9 sec\n"
          ]
        }
      ],
      "source": [
        "! python train.py --net cait --n_epochs 20 # train with cait"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EEUN3BWLO1ak",
        "outputId": "e46d14b6-8d11-4e62-a9db-643a3765f3e6",
        "collapsed": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==> Preparing data..\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "==> Building model..\n",
            "cuda\n",
            "\n",
            "Epoch :  1\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            " [================================================================>]  Step: 76ms | Tot: 8s390ms | Loss: 2.306 | Acc: 13.298% (6649/50000) 98/98 \n",
            " [================================================================>]  Step: 20ms | Tot: 2s116ms | Loss: 2.071 | Acc: 22.800% (2280/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 07:32:48 2024 Epoch 1, lr: 0.0001000, val loss: 207.08647, acc: 22.80000\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "Epoch : 1 train_loss :2.306 , val_loss :207.086 val_acc : 22.8 lr :0.0000994 epoch_time : 13 sec\n",
            "\n",
            "Epoch :  2\n",
            " [================================================================>]  Step: 70ms | Tot: 8s529ms | Loss: 2.204 | Acc: 16.984% (8492/50000) 98/98 \n",
            " [================================================================>]  Step: 19ms | Tot: 2s35ms | Loss: 1.900 | Acc: 29.280% (2928/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 07:33:01 2024 Epoch 2, lr: 0.0000994, val loss: 189.98865, acc: 29.28000\n",
            "Epoch : 2 train_loss :2.204 , val_loss :189.989 val_acc : 29.28 lr :0.0001000 epoch_time : 12 sec\n",
            "\n",
            "Epoch :  3\n",
            " [================================================================>]  Step: 69ms | Tot: 8s566ms | Loss: 2.079 | Acc: 21.638% (10819/50000) 98/98 \n",
            " [================================================================>]  Step: 19ms | Tot: 2s66ms | Loss: 1.838 | Acc: 31.320% (3132/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 07:33:13 2024 Epoch 3, lr: 0.0001000, val loss: 183.75960, acc: 31.32000\n",
            "Epoch : 3 train_loss :2.079 , val_loss :183.760 val_acc : 31.32 lr :0.0000994 epoch_time : 12 sec\n",
            "\n",
            "Epoch :  4\n",
            " [================================================================>]  Step: 71ms | Tot: 8s831ms | Loss: 2.014 | Acc: 24.360% (12180/50000) 98/98 \n",
            " [================================================================>]  Step: 20ms | Tot: 2s93ms | Loss: 1.759 | Acc: 35.150% (3515/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 07:33:26 2024 Epoch 4, lr: 0.0000994, val loss: 175.85981, acc: 35.15000\n",
            "Epoch : 4 train_loss :2.014 , val_loss :175.860 val_acc : 35.15 lr :0.0000976 epoch_time : 12 sec\n",
            "\n",
            "Epoch :  5\n",
            " [================================================================>]  Step: 67ms | Tot: 8s530ms | Loss: 1.959 | Acc: 26.912% (13456/50000) 98/98 \n",
            " [================================================================>]  Step: 20ms | Tot: 2s55ms | Loss: 1.676 | Acc: 38.850% (3885/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 07:33:39 2024 Epoch 5, lr: 0.0000976, val loss: 167.63416, acc: 38.85000\n",
            "Epoch : 5 train_loss :1.959 , val_loss :167.634 val_acc : 38.85 lr :0.0000946 epoch_time : 12 sec\n",
            "\n",
            "Epoch :  6\n",
            " [================================================================>]  Step: 65ms | Tot: 8s565ms | Loss: 1.909 | Acc: 28.918% (14459/50000) 98/98 \n",
            " [================================================================>]  Step: 19ms | Tot: 2s29ms | Loss: 1.541 | Acc: 43.860% (4386/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 07:33:51 2024 Epoch 6, lr: 0.0000946, val loss: 154.06392, acc: 43.86000\n",
            "Epoch : 6 train_loss :1.909 , val_loss :154.064 val_acc : 43.86 lr :0.0000905 epoch_time : 12 sec\n",
            "\n",
            "Epoch :  7\n",
            " [================================================================>]  Step: 67ms | Tot: 8s481ms | Loss: 1.872 | Acc: 30.278% (15139/50000) 98/98 \n",
            " [================================================================>]  Step: 20ms | Tot: 2s78ms | Loss: 1.464 | Acc: 46.960% (4696/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 07:34:04 2024 Epoch 7, lr: 0.0000905, val loss: 146.40070, acc: 46.96000\n",
            "Epoch : 7 train_loss :1.872 , val_loss :146.401 val_acc : 46.96 lr :0.0000854 epoch_time : 12 sec\n",
            "\n",
            "Epoch :  8\n",
            " [================================================================>]  Step: 66ms | Tot: 8s723ms | Loss: 1.829 | Acc: 32.128% (16064/50000) 98/98 \n",
            " [================================================================>]  Step: 20ms | Tot: 2s61ms | Loss: 1.428 | Acc: 48.440% (4844/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 07:34:16 2024 Epoch 8, lr: 0.0000854, val loss: 142.80737, acc: 48.44000\n",
            "Epoch : 8 train_loss :1.829 , val_loss :142.807 val_acc : 48.44 lr :0.0000794 epoch_time : 12 sec\n",
            "\n",
            "Epoch :  9\n",
            " [================================================================>]  Step: 67ms | Tot: 8s584ms | Loss: 1.798 | Acc: 33.440% (16720/50000) 98/98 \n",
            " [================================================================>]  Step: 19ms | Tot: 2s49ms | Loss: 1.360 | Acc: 50.290% (5029/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 07:34:29 2024 Epoch 9, lr: 0.0000794, val loss: 136.02515, acc: 50.29000\n",
            "Epoch : 9 train_loss :1.798 , val_loss :136.025 val_acc : 50.29 lr :0.0000727 epoch_time : 12 sec\n",
            "\n",
            "Epoch :  10\n",
            " [================================================================>]  Step: 66ms | Tot: 8s785ms | Loss: 1.773 | Acc: 34.316% (17158/50000) 98/98 \n",
            " [================================================================>]  Step: 20ms | Tot: 2s30ms | Loss: 1.380 | Acc: 49.700% (4970/10000) 100/100 \n",
            "Current time : Sat Apr 27 07:34:41 2024 Epoch 10, lr: 0.0000727, val loss: 137.96342, acc: 49.70000\n",
            "Epoch : 10 train_loss :1.773 , val_loss :137.963 val_acc : 49.7 lr :0.0000655 epoch_time : 11 sec\n",
            "\n",
            "Epoch :  11\n",
            " [================================================================>]  Step: 66ms | Tot: 8s703ms | Loss: 1.741 | Acc: 35.510% (17755/50000) 98/98 \n",
            " [================================================================>]  Step: 20ms | Tot: 2s58ms | Loss: 1.309 | Acc: 52.380% (5238/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 07:34:53 2024 Epoch 11, lr: 0.0000655, val loss: 130.90002, acc: 52.38000\n",
            "Epoch : 11 train_loss :1.741 , val_loss :130.900 val_acc : 52.38 lr :0.0000578 epoch_time : 12 sec\n",
            "\n",
            "Epoch :  12\n",
            " [================================================================>]  Step: 83ms | Tot: 8s843ms | Loss: 1.721 | Acc: 36.458% (18229/50000) 98/98 \n",
            " [================================================================>]  Step: 20ms | Tot: 2s87ms | Loss: 1.243 | Acc: 55.080% (5508/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 07:35:06 2024 Epoch 12, lr: 0.0000578, val loss: 124.30613, acc: 55.08000\n",
            "Epoch : 12 train_loss :1.721 , val_loss :124.306 val_acc : 55.08 lr :0.0000500 epoch_time : 12 sec\n",
            "\n",
            "Epoch :  13\n",
            " [================================================================>]  Step: 68ms | Tot: 8s520ms | Loss: 1.697 | Acc: 36.960% (18480/50000) 98/98 \n",
            " [================================================================>]  Step: 20ms | Tot: 2s82ms | Loss: 1.254 | Acc: 54.950% (5495/10000) 100/100 \n",
            "Current time : Sat Apr 27 07:35:18 2024 Epoch 13, lr: 0.0000500, val loss: 125.37597, acc: 54.95000\n",
            "Epoch : 13 train_loss :1.697 , val_loss :125.376 val_acc : 54.95 lr :0.0000422 epoch_time : 11 sec\n",
            "\n",
            "Epoch :  14\n",
            " [================================================================>]  Step: 67ms | Tot: 8s399ms | Loss: 1.675 | Acc: 38.166% (19083/50000) 98/98 \n",
            " [================================================================>]  Step: 20ms | Tot: 2s34ms | Loss: 1.198 | Acc: 57.190% (5719/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 07:35:31 2024 Epoch 14, lr: 0.0000422, val loss: 119.83929, acc: 57.19000\n",
            "Epoch : 14 train_loss :1.675 , val_loss :119.839 val_acc : 57.19 lr :0.0000345 epoch_time : 12 sec\n",
            "\n",
            "Epoch :  15\n",
            " [================================================================>]  Step: 74ms | Tot: 8s563ms | Loss: 1.656 | Acc: 38.846% (19423/50000) 98/98 \n",
            " [================================================================>]  Step: 20ms | Tot: 2s56ms | Loss: 1.181 | Acc: 57.640% (5764/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 07:35:43 2024 Epoch 15, lr: 0.0000345, val loss: 118.08237, acc: 57.64000\n",
            "Epoch : 15 train_loss :1.656 , val_loss :118.082 val_acc : 57.64 lr :0.0000273 epoch_time : 12 sec\n",
            "\n",
            "Epoch :  16\n",
            " [================================================================>]  Step: 67ms | Tot: 8s709ms | Loss: 1.633 | Acc: 39.678% (19839/50000) 98/98 \n",
            " [================================================================>]  Step: 19ms | Tot: 2s129ms | Loss: 1.165 | Acc: 57.930% (5793/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 07:35:56 2024 Epoch 16, lr: 0.0000273, val loss: 116.49095, acc: 57.93000\n",
            "Epoch : 16 train_loss :1.633 , val_loss :116.491 val_acc : 57.93 lr :0.0000206 epoch_time : 12 sec\n",
            "\n",
            "Epoch :  17\n",
            " [================================================================>]  Step: 66ms | Tot: 8s395ms | Loss: 1.625 | Acc: 39.918% (19959/50000) 98/98 \n",
            " [================================================================>]  Step: 19ms | Tot: 2s83ms | Loss: 1.177 | Acc: 57.950% (5795/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 07:36:08 2024 Epoch 17, lr: 0.0000206, val loss: 117.70519, acc: 57.95000\n",
            "Epoch : 17 train_loss :1.625 , val_loss :117.705 val_acc : 57.95 lr :0.0000146 epoch_time : 12 sec\n",
            "\n",
            "Epoch :  18\n",
            " [================================================================>]  Step: 65ms | Tot: 8s385ms | Loss: 1.600 | Acc: 40.828% (20414/50000) 98/98 \n",
            " [================================================================>]  Step: 20ms | Tot: 2s107ms | Loss: 1.125 | Acc: 59.670% (5967/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 07:36:21 2024 Epoch 18, lr: 0.0000146, val loss: 112.48461, acc: 59.67000\n",
            "Epoch : 18 train_loss :1.600 , val_loss :112.485 val_acc : 59.67 lr :0.0000095 epoch_time : 12 sec\n",
            "\n",
            "Epoch :  19\n",
            " [================================================================>]  Step: 67ms | Tot: 8s606ms | Loss: 1.592 | Acc: 41.148% (20574/50000) 98/98 \n",
            " [================================================================>]  Step: 20ms | Tot: 2s55ms | Loss: 1.119 | Acc: 59.810% (5981/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 07:36:34 2024 Epoch 19, lr: 0.0000095, val loss: 111.89152, acc: 59.81000\n",
            "Epoch : 19 train_loss :1.592 , val_loss :111.892 val_acc : 59.81 lr :0.0000054 epoch_time : 12 sec\n",
            "\n",
            "Epoch :  20\n",
            " [================================================================>]  Step: 67ms | Tot: 8s739ms | Loss: 1.583 | Acc: 41.420% (20710/50000) 98/98 \n",
            " [================================================================>]  Step: 20ms | Tot: 2s194ms | Loss: 1.115 | Acc: 60.230% (6023/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 07:36:47 2024 Epoch 20, lr: 0.0000054, val loss: 111.49020, acc: 60.23000\n",
            "Epoch : 20 train_loss :1.583 , val_loss :111.490 val_acc : 60.23 lr :0.0000024 epoch_time : 12 sec\n"
          ]
        }
      ],
      "source": [
        "! python train.py --net swin --n_epochs 20 # train with SwinTransformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjTlTaTLO1dk",
        "outputId": "0bb9fce9-93be-4c8c-e922-94a3a7763aaf",
        "collapsed": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==> Preparing data..\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "==> Building model..\n",
            "cuda\n",
            "\n",
            "Epoch :  1\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            " [================================================================>]  Step: 210ms | Tot: 28s832ms | Loss: 2.075 | Acc: 23.690% (11845/50000) 98/98 \n",
            " [================================================================>]  Step: 29ms | Tot: 2s902ms | Loss: 1.559 | Acc: 42.480% (4248/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 07:43:31 2024 Epoch 1, lr: 0.0001000, val loss: 155.85830, acc: 42.48000\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "Epoch : 1 train_loss :2.075 , val_loss :155.858 val_acc : 42.48 lr :0.0000994 epoch_time : 34 sec\n",
            "\n",
            "Epoch :  2\n",
            " [================================================================>]  Step: 202ms | Tot: 28s840ms | Loss: 1.831 | Acc: 32.636% (16318/50000) 98/98 \n",
            " [================================================================>]  Step: 28ms | Tot: 2s915ms | Loss: 1.431 | Acc: 50.400% (5040/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 07:44:05 2024 Epoch 2, lr: 0.0000994, val loss: 143.05953, acc: 50.40000\n",
            "Epoch : 2 train_loss :1.831 , val_loss :143.060 val_acc : 50.4 lr :0.0001000 epoch_time : 33 sec\n",
            "\n",
            "Epoch :  3\n",
            " [================================================================>]  Step: 202ms | Tot: 28s900ms | Loss: 1.666 | Acc: 39.148% (19574/50000) 98/98 \n",
            " [================================================================>]  Step: 29ms | Tot: 2s884ms | Loss: 1.085 | Acc: 61.860% (6186/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 07:44:38 2024 Epoch 3, lr: 0.0001000, val loss: 108.46910, acc: 61.86000\n",
            "Epoch : 3 train_loss :1.666 , val_loss :108.469 val_acc : 61.86 lr :0.0000994 epoch_time : 33 sec\n",
            "\n",
            "Epoch :  4\n",
            " [================================================================>]  Step: 202ms | Tot: 28s886ms | Loss: 1.536 | Acc: 43.852% (21926/50000) 98/98 \n",
            " [================================================================>]  Step: 29ms | Tot: 2s893ms | Loss: 0.969 | Acc: 66.840% (6684/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 07:45:12 2024 Epoch 4, lr: 0.0000994, val loss: 96.88352, acc: 66.84000\n",
            "Epoch : 4 train_loss :1.536 , val_loss :96.884 val_acc : 66.84 lr :0.0000976 epoch_time : 33 sec\n",
            "\n",
            "Epoch :  5\n",
            " [================================================================>]  Step: 201ms | Tot: 28s831ms | Loss: 1.450 | Acc: 47.036% (23518/50000) 98/98 \n",
            " [================================================================>]  Step: 29ms | Tot: 2s909ms | Loss: 0.920 | Acc: 68.670% (6867/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 07:45:46 2024 Epoch 5, lr: 0.0000976, val loss: 92.02031, acc: 68.67000\n",
            "Epoch : 5 train_loss :1.450 , val_loss :92.020 val_acc : 68.67 lr :0.0000946 epoch_time : 33 sec\n",
            "\n",
            "Epoch :  6\n",
            " [================================================================>]  Step: 201ms | Tot: 28s797ms | Loss: 1.393 | Acc: 48.976% (24488/50000) 98/98 \n",
            " [================================================================>]  Step: 29ms | Tot: 2s900ms | Loss: 0.853 | Acc: 70.400% (7040/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 07:46:19 2024 Epoch 6, lr: 0.0000946, val loss: 85.27161, acc: 70.40000\n",
            "Epoch : 6 train_loss :1.393 , val_loss :85.272 val_acc : 70.4 lr :0.0000905 epoch_time : 33 sec\n",
            "\n",
            "Epoch :  7\n",
            " [================================================================>]  Step: 201ms | Tot: 28s790ms | Loss: 1.351 | Acc: 50.412% (25206/50000) 98/98 \n",
            " [================================================================>]  Step: 29ms | Tot: 2s895ms | Loss: 0.796 | Acc: 72.740% (7274/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 07:46:53 2024 Epoch 7, lr: 0.0000905, val loss: 79.64855, acc: 72.74000\n",
            "Epoch : 7 train_loss :1.351 , val_loss :79.649 val_acc : 72.74 lr :0.0000854 epoch_time : 33 sec\n",
            "\n",
            "Epoch :  8\n",
            " [================================================================>]  Step: 202ms | Tot: 28s850ms | Loss: 1.302 | Acc: 52.324% (26162/50000) 98/98 \n",
            " [================================================================>]  Step: 29ms | Tot: 2s911ms | Loss: 0.886 | Acc: 71.560% (7156/10000) 100/100 \n",
            "Current time : Sat Apr 27 07:47:27 2024 Epoch 8, lr: 0.0000854, val loss: 88.64438, acc: 71.56000\n",
            "Epoch : 8 train_loss :1.302 , val_loss :88.644 val_acc : 71.56 lr :0.0000794 epoch_time : 33 sec\n",
            "\n",
            "Epoch :  9\n",
            " [================================================================>]  Step: 201ms | Tot: 28s824ms | Loss: 1.269 | Acc: 53.394% (26697/50000) 98/98 \n",
            " [================================================================>]  Step: 30ms | Tot: 2s910ms | Loss: 0.774 | Acc: 73.820% (7382/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 07:48:00 2024 Epoch 9, lr: 0.0000794, val loss: 77.40729, acc: 73.82000\n",
            "Epoch : 9 train_loss :1.269 , val_loss :77.407 val_acc : 73.82 lr :0.0000727 epoch_time : 33 sec\n",
            "\n",
            "Epoch :  10\n",
            " [================================================================>]  Step: 201ms | Tot: 28s796ms | Loss: 1.247 | Acc: 54.412% (27206/50000) 98/98 \n",
            " [================================================================>]  Step: 28ms | Tot: 2s922ms | Loss: 0.647 | Acc: 77.830% (7783/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 07:48:34 2024 Epoch 10, lr: 0.0000727, val loss: 64.73848, acc: 77.83000\n",
            "Epoch : 10 train_loss :1.247 , val_loss :64.738 val_acc : 77.83 lr :0.0000655 epoch_time : 33 sec\n",
            "\n",
            "Epoch :  11\n",
            " [================================================================>]  Step: 201ms | Tot: 28s856ms | Loss: 1.215 | Acc: 55.312% (27656/50000) 98/98 \n",
            " [================================================================>]  Step: 29ms | Tot: 2s902ms | Loss: 0.640 | Acc: 78.460% (7846/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 07:49:08 2024 Epoch 11, lr: 0.0000655, val loss: 63.95494, acc: 78.46000\n",
            "Epoch : 11 train_loss :1.215 , val_loss :63.955 val_acc : 78.46 lr :0.0000578 epoch_time : 33 sec\n",
            "\n",
            "Epoch :  12\n",
            " [================================================================>]  Step: 203ms | Tot: 28s871ms | Loss: 1.191 | Acc: 56.252% (28126/50000) 98/98 \n",
            " [================================================================>]  Step: 29ms | Tot: 2s910ms | Loss: 0.670 | Acc: 76.810% (7681/10000) 100/100 \n",
            "Current time : Sat Apr 27 07:49:41 2024 Epoch 12, lr: 0.0000578, val loss: 67.03885, acc: 76.81000\n",
            "Epoch : 12 train_loss :1.191 , val_loss :67.039 val_acc : 76.81 lr :0.0000500 epoch_time : 33 sec\n",
            "\n",
            "Epoch :  13\n",
            " [================================================================>]  Step: 201ms | Tot: 28s836ms | Loss: 1.162 | Acc: 57.300% (28650/50000) 98/98 \n",
            " [================================================================>]  Step: 28ms | Tot: 2s917ms | Loss: 0.562 | Acc: 80.410% (8041/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 07:50:15 2024 Epoch 13, lr: 0.0000500, val loss: 56.18973, acc: 80.41000\n",
            "Epoch : 13 train_loss :1.162 , val_loss :56.190 val_acc : 80.41 lr :0.0000422 epoch_time : 33 sec\n",
            "\n",
            "Epoch :  14\n",
            " [================================================================>]  Step: 201ms | Tot: 28s845ms | Loss: 1.145 | Acc: 58.072% (29036/50000) 98/98 \n",
            " [================================================================>]  Step: 29ms | Tot: 2s900ms | Loss: 0.616 | Acc: 79.120% (7912/10000) 100/100 \n",
            "Current time : Sat Apr 27 07:50:48 2024 Epoch 14, lr: 0.0000422, val loss: 61.58067, acc: 79.12000\n",
            "Epoch : 14 train_loss :1.145 , val_loss :61.581 val_acc : 79.12 lr :0.0000345 epoch_time : 33 sec\n",
            "\n",
            "Epoch :  15\n",
            " [================================================================>]  Step: 201ms | Tot: 28s821ms | Loss: 1.127 | Acc: 58.360% (29180/50000) 98/98 \n",
            " [================================================================>]  Step: 29ms | Tot: 2s912ms | Loss: 0.557 | Acc: 80.970% (8097/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 07:51:22 2024 Epoch 15, lr: 0.0000345, val loss: 55.67802, acc: 80.97000\n",
            "Epoch : 15 train_loss :1.127 , val_loss :55.678 val_acc : 80.97 lr :0.0000273 epoch_time : 33 sec\n",
            "\n",
            "Epoch :  16\n",
            " [================================================================>]  Step: 201ms | Tot: 28s827ms | Loss: 1.113 | Acc: 59.026% (29513/50000) 98/98 \n",
            " [================================================================>]  Step: 29ms | Tot: 2s942ms | Loss: 0.515 | Acc: 82.050% (8205/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 07:51:56 2024 Epoch 16, lr: 0.0000273, val loss: 51.52504, acc: 82.05000\n",
            "Epoch : 16 train_loss :1.113 , val_loss :51.525 val_acc : 82.05 lr :0.0000206 epoch_time : 33 sec\n",
            "\n",
            "Epoch :  17\n",
            " [================================================================>]  Step: 201ms | Tot: 28s785ms | Loss: 1.094 | Acc: 59.690% (29845/50000) 98/98 \n",
            " [================================================================>]  Step: 29ms | Tot: 2s906ms | Loss: 0.493 | Acc: 83.260% (8326/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 07:52:29 2024 Epoch 17, lr: 0.0000206, val loss: 49.30305, acc: 83.26000\n",
            "Epoch : 17 train_loss :1.094 , val_loss :49.303 val_acc : 83.26 lr :0.0000146 epoch_time : 33 sec\n",
            "\n",
            "Epoch :  18\n",
            " [================================================================>]  Step: 202ms | Tot: 28s848ms | Loss: 1.072 | Acc: 60.456% (30228/50000) 98/98 \n",
            " [================================================================>]  Step: 29ms | Tot: 2s904ms | Loss: 0.456 | Acc: 84.140% (8414/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 07:53:03 2024 Epoch 18, lr: 0.0000146, val loss: 45.56917, acc: 84.14000\n",
            "Epoch : 18 train_loss :1.072 , val_loss :45.569 val_acc : 84.14 lr :0.0000095 epoch_time : 33 sec\n",
            "\n",
            "Epoch :  19\n",
            " [================================================================>]  Step: 201ms | Tot: 28s851ms | Loss: 1.068 | Acc: 60.574% (30287/50000) 98/98 \n",
            " [================================================================>]  Step: 29ms | Tot: 2s940ms | Loss: 0.461 | Acc: 83.840% (8384/10000) 100/100 \n",
            "Current time : Sat Apr 27 07:53:36 2024 Epoch 19, lr: 0.0000095, val loss: 46.10294, acc: 83.84000\n",
            "Epoch : 19 train_loss :1.068 , val_loss :46.103 val_acc : 83.84 lr :0.0000054 epoch_time : 33 sec\n",
            "\n",
            "Epoch :  20\n",
            " [================================================================>]  Step: 201ms | Tot: 28s798ms | Loss: 1.061 | Acc: 60.718% (30359/50000) 98/98 \n",
            " [================================================================>]  Step: 28ms | Tot: 2s896ms | Loss: 0.443 | Acc: 85.020% (8502/10000) 100/100 \n",
            "Saving..\n",
            "Current time : Sat Apr 27 07:54:10 2024 Epoch 20, lr: 0.0000054, val loss: 44.29365, acc: 85.02000\n",
            "Epoch : 20 train_loss :1.061 , val_loss :44.294 val_acc : 85.02 lr :0.0000024 epoch_time : 33 sec\n"
          ]
        }
      ],
      "source": [
        "! python train.py --net res18 --size 96 --n_epochs 20 # resnet18+randaug"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIcB_Mt__4ZR",
        "outputId": "677c7164-d977-4f88-c5d1-5ddb146fdc2a",
        "collapsed": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==> Preparing data..\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n",
            "100% 170498071/170498071 [00:03<00:00, 45015157.55it/s]\n",
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "==> Building model..\n",
            "cuda\n",
            "/content/train.py:282: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  with tqdm_notebook(total=len(train_ds), desc=f\"Train Epoch {epoch+1}\") as pbar:\n",
            "Train Epoch 1:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/content/train.py:320: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  with tqdm_notebook(total=len(test_ds), desc=f\"Test_ Epoch {epoch+1}\") as pbar:\n",
            "Test_ Epoch 1:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May  8 07:16:16 2024 Epoch 1, lr: 0.0001000, val loss: 177.08899, acc: 36.65000\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "Epoch : 1 train_loss :2.221 , val_loss :177.089 val_acc : 36.65 lr :0.0000994 epoch_time : 9 sec\n",
            "Train Epoch 2:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 2:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May  8 07:16:25 2024 Epoch 2, lr: 0.0000994, val loss: 165.72331, acc: 40.57000\n",
            "Epoch : 2 train_loss :2.073 , val_loss :165.723 val_acc : 40.57 lr :0.0001000 epoch_time : 8 sec\n",
            "Train Epoch 3:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 3:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May  8 07:16:33 2024 Epoch 3, lr: 0.0001000, val loss: 155.70410, acc: 44.57000\n",
            "Epoch : 3 train_loss :1.997 , val_loss :155.704 val_acc : 44.57 lr :0.0000994 epoch_time : 8 sec\n",
            "Train Epoch 4:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 4:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May  8 07:16:41 2024 Epoch 4, lr: 0.0000994, val loss: 152.58137, acc: 45.29000\n",
            "Epoch : 4 train_loss :1.946 , val_loss :152.581 val_acc : 45.29 lr :0.0000976 epoch_time : 8 sec\n",
            "Train Epoch 5:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 5:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May  8 07:16:49 2024 Epoch 5, lr: 0.0000976, val loss: 143.92761, acc: 48.31000\n",
            "Epoch : 5 train_loss :1.907 , val_loss :143.928 val_acc : 48.31 lr :0.0000946 epoch_time : 8 sec\n",
            "Train Epoch 6:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 6:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May  8 07:16:58 2024 Epoch 6, lr: 0.0000946, val loss: 139.59828, acc: 49.73000\n",
            "Epoch : 6 train_loss :1.881 , val_loss :139.598 val_acc : 49.73 lr :0.0000905 epoch_time : 8 sec\n",
            "Train Epoch 7:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 7:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May  8 07:17:06 2024 Epoch 7, lr: 0.0000905, val loss: 136.90455, acc: 50.94000\n",
            "Epoch : 7 train_loss :1.847 , val_loss :136.905 val_acc : 50.94 lr :0.0000854 epoch_time : 8 sec\n",
            "Train Epoch 8:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 8:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May  8 07:17:14 2024 Epoch 8, lr: 0.0000854, val loss: 133.00105, acc: 52.42000\n",
            "Epoch : 8 train_loss :1.824 , val_loss :133.001 val_acc : 52.42 lr :0.0000794 epoch_time : 8 sec\n",
            "Train Epoch 9:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 9:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Current time : Wed May  8 07:17:22 2024 Epoch 9, lr: 0.0000794, val loss: 132.38522, acc: 52.26000\n",
            "Epoch : 9 train_loss :1.801 , val_loss :132.385 val_acc : 52.26 lr :0.0000727 epoch_time : 7 sec\n",
            "Train Epoch 10:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 10:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May  8 07:17:30 2024 Epoch 10, lr: 0.0000727, val loss: 126.93014, acc: 54.94000\n",
            "Epoch : 10 train_loss :1.767 , val_loss :126.930 val_acc : 54.94 lr :0.0000655 epoch_time : 8 sec\n",
            "Train Epoch 11:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 11:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May  8 07:17:38 2024 Epoch 11, lr: 0.0000655, val loss: 120.55696, acc: 57.34000\n",
            "Epoch : 11 train_loss :1.747 , val_loss :120.557 val_acc : 57.34 lr :0.0000578 epoch_time : 8 sec\n",
            "Train Epoch 12:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 12:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Current time : Wed May  8 07:17:46 2024 Epoch 12, lr: 0.0000578, val loss: 120.85710, acc: 56.67000\n",
            "Epoch : 12 train_loss :1.724 , val_loss :120.857 val_acc : 56.67 lr :0.0000500 epoch_time : 7 sec\n",
            "Train Epoch 13:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 13:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May  8 07:17:53 2024 Epoch 13, lr: 0.0000500, val loss: 116.11128, acc: 58.62000\n",
            "Epoch : 13 train_loss :1.703 , val_loss :116.111 val_acc : 58.62 lr :0.0000422 epoch_time : 7 sec\n",
            "Train Epoch 14:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 14:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May  8 07:18:02 2024 Epoch 14, lr: 0.0000422, val loss: 114.12779, acc: 59.96000\n",
            "Epoch : 14 train_loss :1.687 , val_loss :114.128 val_acc : 59.96 lr :0.0000345 epoch_time : 8 sec\n",
            "Train Epoch 15:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 15:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May  8 07:18:10 2024 Epoch 15, lr: 0.0000345, val loss: 112.42549, acc: 60.08000\n",
            "Epoch : 15 train_loss :1.671 , val_loss :112.425 val_acc : 60.08 lr :0.0000273 epoch_time : 7 sec\n",
            "Train Epoch 16:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 16:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May  8 07:18:18 2024 Epoch 16, lr: 0.0000273, val loss: 112.23066, acc: 60.28000\n",
            "Epoch : 16 train_loss :1.652 , val_loss :112.231 val_acc : 60.28 lr :0.0000206 epoch_time : 8 sec\n",
            "Train Epoch 17:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 17:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May  8 07:18:26 2024 Epoch 17, lr: 0.0000206, val loss: 110.34608, acc: 61.08000\n",
            "Epoch : 17 train_loss :1.640 , val_loss :110.346 val_acc : 61.08 lr :0.0000146 epoch_time : 8 sec\n",
            "Train Epoch 18:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 18:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May  8 07:18:35 2024 Epoch 18, lr: 0.0000146, val loss: 108.83475, acc: 61.57000\n",
            "Epoch : 18 train_loss :1.630 , val_loss :108.835 val_acc : 61.57 lr :0.0000095 epoch_time : 8 sec\n",
            "Train Epoch 19:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 19:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May  8 07:18:43 2024 Epoch 19, lr: 0.0000095, val loss: 107.25594, acc: 62.07000\n",
            "Epoch : 19 train_loss :1.616 , val_loss :107.256 val_acc : 62.07 lr :0.0000054 epoch_time : 8 sec\n",
            "Train Epoch 20:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 20:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Current time : Wed May  8 07:18:50 2024 Epoch 20, lr: 0.0000054, val loss: 107.21949, acc: 61.81000\n",
            "Epoch : 20 train_loss :1.610 , val_loss :107.219 val_acc : 61.81 lr :0.0000024 epoch_time : 7 sec\n"
          ]
        }
      ],
      "source": [
        "! python train.py --net t2t --n_epochs 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59iCrBSg_4P_",
        "outputId": "d53b09c9-2e05-4bf1-90e7-b785a94e1cbc",
        "collapsed": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==> Preparing data..\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n",
            "100% 170498071/170498071 [00:13<00:00, 12611830.26it/s]\n",
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "==> Building model..\n",
            "cuda\n",
            "/content/train_ats_vit.py:150: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  with tqdm_notebook(total=len(train_ds), desc=f\"Train Epoch {epoch+1}\") as pbar:\n",
            "Train Epoch 1:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/content/train_ats_vit.py:188: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  with tqdm_notebook(total=len(test_ds), desc=f\"Test_ Epoch {epoch+1}\") as pbar:\n",
            "Test_ Epoch 1:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Tue May 14 22:28:45 2024 Epoch 1, lr: 0.0001000, val loss: 194.72805, acc: 30.49000\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "Epoch : 1 train_loss :2.311 , val_loss :194.728 val_acc : 30.49 lr :0.0000994 epoch_time : 9 sec\n",
            "Train Epoch 2:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 2:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Tue May 14 22:28:53 2024 Epoch 2, lr: 0.0000994, val loss: 184.22382, acc: 32.81000\n",
            "Epoch : 2 train_loss :2.187 , val_loss :184.224 val_acc : 32.81 lr :0.0001000 epoch_time : 8 sec\n",
            "Train Epoch 3:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 3:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Tue May 14 22:29:02 2024 Epoch 3, lr: 0.0001000, val loss: 175.44844, acc: 35.91000\n",
            "Epoch : 3 train_loss :2.121 , val_loss :175.448 val_acc : 35.91 lr :0.0000994 epoch_time : 8 sec\n",
            "Train Epoch 4:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 4:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Tue May 14 22:29:12 2024 Epoch 4, lr: 0.0000994, val loss: 172.03453, acc: 38.26000\n",
            "Epoch : 4 train_loss :2.085 , val_loss :172.035 val_acc : 38.26 lr :0.0000976 epoch_time : 9 sec\n",
            "Train Epoch 5:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 5:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Tue May 14 22:29:21 2024 Epoch 5, lr: 0.0000976, val loss: 166.23232, acc: 40.88000\n",
            "Epoch : 5 train_loss :2.054 , val_loss :166.232 val_acc : 40.88 lr :0.0000946 epoch_time : 8 sec\n",
            "Train Epoch 6:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 6:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Current time : Tue May 14 22:29:28 2024 Epoch 6, lr: 0.0000946, val loss: 166.14197, acc: 40.40000\n",
            "Epoch : 6 train_loss :2.031 , val_loss :166.142 val_acc : 40.4 lr :0.0000905 epoch_time : 7 sec\n",
            "Train Epoch 7:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 7:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Tue May 14 22:29:36 2024 Epoch 7, lr: 0.0000905, val loss: 163.30928, acc: 41.25000\n",
            "Epoch : 7 train_loss :2.009 , val_loss :163.309 val_acc : 41.25 lr :0.0000854 epoch_time : 8 sec\n",
            "Train Epoch 8:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 8:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Tue May 14 22:29:44 2024 Epoch 8, lr: 0.0000854, val loss: 157.78582, acc: 43.71000\n",
            "Epoch : 8 train_loss :1.994 , val_loss :157.786 val_acc : 43.71 lr :0.0000794 epoch_time : 7 sec\n",
            "Train Epoch 9:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 9:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Tue May 14 22:29:53 2024 Epoch 9, lr: 0.0000794, val loss: 158.24871, acc: 44.12000\n",
            "Epoch : 9 train_loss :1.979 , val_loss :158.249 val_acc : 44.12 lr :0.0000727 epoch_time : 8 sec\n",
            "Train Epoch 10:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 10:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Tue May 14 22:30:01 2024 Epoch 10, lr: 0.0000727, val loss: 151.76916, acc: 46.26000\n",
            "Epoch : 10 train_loss :1.964 , val_loss :151.769 val_acc : 46.26 lr :0.0000655 epoch_time : 8 sec\n",
            "Train Epoch 11:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 11:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Current time : Tue May 14 22:30:09 2024 Epoch 11, lr: 0.0000655, val loss: 153.80178, acc: 45.49000\n",
            "Epoch : 11 train_loss :1.944 , val_loss :153.802 val_acc : 45.49 lr :0.0000578 epoch_time : 7 sec\n",
            "Train Epoch 12:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 12:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Tue May 14 22:30:17 2024 Epoch 12, lr: 0.0000578, val loss: 149.40847, acc: 47.44000\n",
            "Epoch : 12 train_loss :1.931 , val_loss :149.408 val_acc : 47.44 lr :0.0000500 epoch_time : 8 sec\n",
            "Train Epoch 13:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 13:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Tue May 14 22:30:26 2024 Epoch 13, lr: 0.0000500, val loss: 145.74530, acc: 47.89000\n",
            "Epoch : 13 train_loss :1.916 , val_loss :145.745 val_acc : 47.89 lr :0.0000422 epoch_time : 8 sec\n",
            "Train Epoch 14:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 14:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Tue May 14 22:30:34 2024 Epoch 14, lr: 0.0000422, val loss: 146.10898, acc: 48.10000\n",
            "Epoch : 14 train_loss :1.906 , val_loss :146.109 val_acc : 48.1 lr :0.0000345 epoch_time : 8 sec\n",
            "Train Epoch 15:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 15:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Tue May 14 22:30:42 2024 Epoch 15, lr: 0.0000345, val loss: 144.64100, acc: 48.56000\n",
            "Epoch : 15 train_loss :1.895 , val_loss :144.641 val_acc : 48.56 lr :0.0000273 epoch_time : 8 sec\n",
            "Train Epoch 16:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 16:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Tue May 14 22:30:51 2024 Epoch 16, lr: 0.0000273, val loss: 142.78268, acc: 49.59000\n",
            "Epoch : 16 train_loss :1.879 , val_loss :142.783 val_acc : 49.59 lr :0.0000206 epoch_time : 8 sec\n",
            "Train Epoch 17:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 17:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Tue May 14 22:30:59 2024 Epoch 17, lr: 0.0000206, val loss: 141.26610, acc: 50.14000\n",
            "Epoch : 17 train_loss :1.872 , val_loss :141.266 val_acc : 50.14 lr :0.0000146 epoch_time : 8 sec\n",
            "Train Epoch 18:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 18:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Current time : Tue May 14 22:31:06 2024 Epoch 18, lr: 0.0000146, val loss: 140.69134, acc: 50.08000\n",
            "Epoch : 18 train_loss :1.865 , val_loss :140.691 val_acc : 50.08 lr :0.0000095 epoch_time : 7 sec\n",
            "Train Epoch 19:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 19:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Tue May 14 22:31:15 2024 Epoch 19, lr: 0.0000095, val loss: 139.52052, acc: 50.70000\n",
            "Epoch : 19 train_loss :1.858 , val_loss :139.521 val_acc : 50.7 lr :0.0000054 epoch_time : 8 sec\n",
            "Train Epoch 20:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 20:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Current time : Tue May 14 22:31:22 2024 Epoch 20, lr: 0.0000054, val loss: 139.31316, acc: 50.67000\n",
            "Epoch : 20 train_loss :1.857 , val_loss :139.313 val_acc : 50.67 lr :0.0000024 epoch_time : 6 sec\n"
          ]
        }
      ],
      "source": [
        "! python train_ats_vit.py --net ats_vit --n_epochs 20 # train with ats_vit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "944WgMYPOGS4",
        "outputId": "27124440-2626-4f87-f545-4b022fbf7070"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==> Preparing data..\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n",
            "100% 170498071/170498071 [00:16<00:00, 10388158.10it/s]\n",
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "==> Building model..\n",
            "cuda\n",
            "/content/train.py:353: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  with tqdm_notebook(total=len(train_ds), desc=f\"Train Epoch {epoch+1}\") as pbar:\n",
            "Train Epoch 1:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/content/train.py:391: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  with tqdm_notebook(total=len(test_ds), desc=f\"Test_ Epoch {epoch+1}\") as pbar:\n",
            "Test_ Epoch 1:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 00:42:49 2024 Epoch 1, lr: 0.0001000, val loss: 175.77789, acc: 35.69000\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "Epoch : 1 train_loss :2.203 , val_loss :175.778 val_acc : 35.69 lr :0.0000994 epoch_time : 66 sec\n",
            "Train Epoch 2:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 2:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 00:43:53 2024 Epoch 2, lr: 0.0000994, val loss: 168.80932, acc: 37.69000\n",
            "Epoch : 2 train_loss :2.014 , val_loss :168.809 val_acc : 37.69 lr :0.0001000 epoch_time : 64 sec\n",
            "Train Epoch 3:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 3:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 00:44:58 2024 Epoch 3, lr: 0.0001000, val loss: 153.95682, acc: 44.00000\n",
            "Epoch : 3 train_loss :1.930 , val_loss :153.957 val_acc : 44.0 lr :0.0000994 epoch_time : 64 sec\n",
            "Train Epoch 4:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 4:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 00:46:02 2024 Epoch 4, lr: 0.0000994, val loss: 144.31244, acc: 48.08000\n",
            "Epoch : 4 train_loss :1.857 , val_loss :144.312 val_acc : 48.08 lr :0.0000976 epoch_time : 64 sec\n",
            "Train Epoch 5:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 5:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 00:47:07 2024 Epoch 5, lr: 0.0000976, val loss: 133.98479, acc: 51.67000\n",
            "Epoch : 5 train_loss :1.806 , val_loss :133.985 val_acc : 51.67 lr :0.0000946 epoch_time : 64 sec\n",
            "Train Epoch 6:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 6:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 00:48:12 2024 Epoch 6, lr: 0.0000946, val loss: 126.87800, acc: 54.84000\n",
            "Epoch : 6 train_loss :1.743 , val_loss :126.878 val_acc : 54.84 lr :0.0000905 epoch_time : 64 sec\n",
            "Train Epoch 7:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 7:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 00:49:16 2024 Epoch 7, lr: 0.0000905, val loss: 120.58383, acc: 57.24000\n",
            "Epoch : 7 train_loss :1.700 , val_loss :120.584 val_acc : 57.24 lr :0.0000854 epoch_time : 64 sec\n",
            "Train Epoch 8:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 8:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 00:50:21 2024 Epoch 8, lr: 0.0000854, val loss: 120.51423, acc: 57.58000\n",
            "Epoch : 8 train_loss :1.660 , val_loss :120.514 val_acc : 57.58 lr :0.0000794 epoch_time : 64 sec\n",
            "Train Epoch 9:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 9:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 00:51:25 2024 Epoch 9, lr: 0.0000794, val loss: 101.92503, acc: 63.94000\n",
            "Epoch : 9 train_loss :1.608 , val_loss :101.925 val_acc : 63.94 lr :0.0000727 epoch_time : 64 sec\n",
            "Train Epoch 10:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 10:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 00:52:30 2024 Epoch 10, lr: 0.0000727, val loss: 98.53645, acc: 65.25000\n",
            "Epoch : 10 train_loss :1.563 , val_loss :98.536 val_acc : 65.25 lr :0.0000655 epoch_time : 64 sec\n",
            "Train Epoch 11:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 11:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 00:53:35 2024 Epoch 11, lr: 0.0000655, val loss: 96.75803, acc: 65.76000\n",
            "Epoch : 11 train_loss :1.533 , val_loss :96.758 val_acc : 65.76 lr :0.0000578 epoch_time : 64 sec\n",
            "Train Epoch 12:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 12:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 00:54:40 2024 Epoch 12, lr: 0.0000578, val loss: 92.21462, acc: 66.93000\n",
            "Epoch : 12 train_loss :1.501 , val_loss :92.215 val_acc : 66.93 lr :0.0000500 epoch_time : 65 sec\n",
            "Train Epoch 13:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 13:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 00:55:45 2024 Epoch 13, lr: 0.0000500, val loss: 90.00591, acc: 68.31000\n",
            "Epoch : 13 train_loss :1.486 , val_loss :90.006 val_acc : 68.31 lr :0.0000422 epoch_time : 64 sec\n",
            "Train Epoch 14:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 14:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 00:56:49 2024 Epoch 14, lr: 0.0000422, val loss: 85.87385, acc: 70.00000\n",
            "Epoch : 14 train_loss :1.460 , val_loss :85.874 val_acc : 70.0 lr :0.0000345 epoch_time : 64 sec\n",
            "Train Epoch 15:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 15:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 00:57:54 2024 Epoch 15, lr: 0.0000345, val loss: 82.61789, acc: 71.00000\n",
            "Epoch : 15 train_loss :1.442 , val_loss :82.618 val_acc : 71.0 lr :0.0000273 epoch_time : 64 sec\n",
            "Train Epoch 16:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 16:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 00:58:59 2024 Epoch 16, lr: 0.0000273, val loss: 80.62338, acc: 71.65000\n",
            "Epoch : 16 train_loss :1.423 , val_loss :80.623 val_acc : 71.65 lr :0.0000206 epoch_time : 64 sec\n",
            "Train Epoch 17:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 17:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Current time : Wed May 15 01:00:03 2024 Epoch 17, lr: 0.0000206, val loss: 81.36495, acc: 71.29000\n",
            "Epoch : 17 train_loss :1.404 , val_loss :81.365 val_acc : 71.29 lr :0.0000146 epoch_time : 64 sec\n",
            "Train Epoch 18:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 18:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 01:01:08 2024 Epoch 18, lr: 0.0000146, val loss: 78.16060, acc: 72.12000\n",
            "Epoch : 18 train_loss :1.385 , val_loss :78.161 val_acc : 72.12 lr :0.0000095 epoch_time : 64 sec\n",
            "Train Epoch 19:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 19:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 01:02:12 2024 Epoch 19, lr: 0.0000095, val loss: 77.22592, acc: 72.35000\n",
            "Epoch : 19 train_loss :1.377 , val_loss :77.226 val_acc : 72.35 lr :0.0000054 epoch_time : 64 sec\n",
            "Train Epoch 20:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 20:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 01:03:17 2024 Epoch 20, lr: 0.0000054, val loss: 76.12075, acc: 72.97000\n",
            "Epoch : 20 train_loss :1.375 , val_loss :76.121 val_acc : 72.97 lr :0.0000024 epoch_time : 64 sec\n"
          ]
        }
      ],
      "source": [
        "! python train.py --net cct --size 224 --n_epochs 20  # A-100 # A-100 35GB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZZiChBlTo5v",
        "outputId": "38cd8376-8dac-4d14-86a6-91123f37b549",
        "collapsed": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==> Preparing data..\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n",
            "100% 170498071/170498071 [00:02<00:00, 78720502.68it/s]\n",
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "==> Building model..\n",
            "cuda\n",
            "/content/train.py:359: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  with tqdm_notebook(total=len(train_ds), desc=f\"Train Epoch {epoch+1}\") as pbar:\n",
            "Train Epoch 1:   0%|          | 0/782 [00:00<?, ?it/s]\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/content/train.py:397: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  with tqdm_notebook(total=len(test_ds), desc=f\"Test_ Epoch {epoch+1}\") as pbar:\n",
            "Test_ Epoch 1:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Tue May 14 23:42:56 2024 Epoch 1, lr: 0.0001000, val loss: 201.44671, acc: 27.57000\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "Epoch : 1 train_loss :2.251 , val_loss :201.447 val_acc : 27.57 lr :0.0000994 epoch_time : 179 sec\n",
            "Train Epoch 2:   0%|          | 0/782 [00:00<?, ?it/s]\n",
            "Test_ Epoch 2:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Tue May 14 23:45:52 2024 Epoch 2, lr: 0.0000994, val loss: 188.32601, acc: 29.72000\n",
            "Epoch : 2 train_loss :2.168 , val_loss :188.326 val_acc : 29.72 lr :0.0001000 epoch_time : 176 sec\n",
            "Train Epoch 3:   0%|          | 0/782 [00:00<?, ?it/s]\n",
            "Test_ Epoch 3:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Tue May 14 23:48:48 2024 Epoch 3, lr: 0.0001000, val loss: 178.57993, acc: 32.95000\n",
            "Epoch : 3 train_loss :2.075 , val_loss :178.580 val_acc : 32.95 lr :0.0000994 epoch_time : 176 sec\n",
            "Train Epoch 4:   0%|          | 0/782 [00:00<?, ?it/s]\n",
            "Test_ Epoch 4:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Tue May 14 23:51:44 2024 Epoch 4, lr: 0.0000994, val loss: 175.18560, acc: 33.79000\n",
            "Epoch : 4 train_loss :2.034 , val_loss :175.186 val_acc : 33.79 lr :0.0000976 epoch_time : 176 sec\n",
            "Train Epoch 5:   0%|          | 0/782 [00:00<?, ?it/s]\n",
            "Test_ Epoch 5:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Tue May 14 23:54:41 2024 Epoch 5, lr: 0.0000976, val loss: 170.82568, acc: 36.25000\n",
            "Epoch : 5 train_loss :1.997 , val_loss :170.826 val_acc : 36.25 lr :0.0000946 epoch_time : 176 sec\n",
            "Train Epoch 6:   0%|          | 0/782 [00:00<?, ?it/s]\n",
            "Test_ Epoch 6:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Tue May 14 23:57:37 2024 Epoch 6, lr: 0.0000946, val loss: 168.49071, acc: 37.65000\n",
            "Epoch : 6 train_loss :1.977 , val_loss :168.491 val_acc : 37.65 lr :0.0000905 epoch_time : 176 sec\n",
            "Train Epoch 7:   0%|          | 0/782 [00:00<?, ?it/s]\n",
            "Test_ Epoch 7:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 00:00:33 2024 Epoch 7, lr: 0.0000905, val loss: 164.25581, acc: 38.82000\n",
            "Epoch : 7 train_loss :1.964 , val_loss :164.256 val_acc : 38.82 lr :0.0000854 epoch_time : 176 sec\n",
            "Train Epoch 8:   0%|          | 0/782 [00:00<?, ?it/s]\n",
            "Test_ Epoch 8:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Current time : Wed May 15 00:03:30 2024 Epoch 8, lr: 0.0000854, val loss: 165.44053, acc: 38.41000\n",
            "Epoch : 8 train_loss :1.945 , val_loss :165.441 val_acc : 38.41 lr :0.0000794 epoch_time : 176 sec\n",
            "Train Epoch 9:   0%|          | 0/782 [00:00<?, ?it/s]\n",
            "Test_ Epoch 9:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 00:06:26 2024 Epoch 9, lr: 0.0000794, val loss: 158.62512, acc: 41.55000\n",
            "Epoch : 9 train_loss :1.930 , val_loss :158.625 val_acc : 41.55 lr :0.0000727 epoch_time : 176 sec\n",
            "Train Epoch 10:   0%|          | 0/782 [00:00<?, ?it/s]\n",
            "Test_ Epoch 10:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Current time : Wed May 15 00:09:22 2024 Epoch 10, lr: 0.0000727, val loss: 159.54407, acc: 40.99000\n",
            "Epoch : 10 train_loss :1.910 , val_loss :159.544 val_acc : 40.99 lr :0.0000655 epoch_time : 176 sec\n",
            "Train Epoch 11:   0%|          | 0/782 [00:00<?, ?it/s]\n",
            "Test_ Epoch 11:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 00:12:18 2024 Epoch 11, lr: 0.0000655, val loss: 156.39916, acc: 42.38000\n",
            "Epoch : 11 train_loss :1.903 , val_loss :156.399 val_acc : 42.38 lr :0.0000578 epoch_time : 176 sec\n",
            "Train Epoch 12:   0%|          | 0/782 [00:00<?, ?it/s]\n",
            "Test_ Epoch 12:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 00:15:15 2024 Epoch 12, lr: 0.0000578, val loss: 155.02413, acc: 43.25000\n",
            "Epoch : 12 train_loss :1.890 , val_loss :155.024 val_acc : 43.25 lr :0.0000500 epoch_time : 176 sec\n",
            "Train Epoch 13:   0%|          | 0/782 [00:00<?, ?it/s]\n",
            "Test_ Epoch 13:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Current time : Wed May 15 00:18:11 2024 Epoch 13, lr: 0.0000500, val loss: 155.32157, acc: 43.25000\n",
            "Epoch : 13 train_loss :1.876 , val_loss :155.322 val_acc : 43.25 lr :0.0000422 epoch_time : 176 sec\n",
            "Train Epoch 14:   0%|          | 0/782 [00:00<?, ?it/s]\n",
            "Test_ Epoch 14:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 00:21:07 2024 Epoch 14, lr: 0.0000422, val loss: 150.49761, acc: 44.77000\n",
            "Epoch : 14 train_loss :1.874 , val_loss :150.498 val_acc : 44.77 lr :0.0000345 epoch_time : 176 sec\n",
            "Train Epoch 15:   0%|          | 0/782 [00:00<?, ?it/s]\n",
            "Test_ Epoch 15:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 00:24:04 2024 Epoch 15, lr: 0.0000345, val loss: 149.84701, acc: 45.03000\n",
            "Epoch : 15 train_loss :1.859 , val_loss :149.847 val_acc : 45.03 lr :0.0000273 epoch_time : 176 sec\n",
            "Train Epoch 16:   0%|          | 0/782 [00:00<?, ?it/s]\n",
            "Test_ Epoch 16:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Current time : Wed May 15 00:27:00 2024 Epoch 16, lr: 0.0000273, val loss: 151.88067, acc: 44.50000\n",
            "Epoch : 16 train_loss :1.853 , val_loss :151.881 val_acc : 44.5 lr :0.0000206 epoch_time : 176 sec\n",
            "Train Epoch 17:   0%|          | 0/782 [00:00<?, ?it/s]\n",
            "Test_ Epoch 17:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 00:29:57 2024 Epoch 17, lr: 0.0000206, val loss: 149.07055, acc: 45.66000\n",
            "Epoch : 17 train_loss :1.851 , val_loss :149.071 val_acc : 45.66 lr :0.0000146 epoch_time : 176 sec\n",
            "Train Epoch 18:   0%|          | 0/782 [00:00<?, ?it/s]\n",
            "Test_ Epoch 18:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 00:32:53 2024 Epoch 18, lr: 0.0000146, val loss: 148.43736, acc: 46.03000\n",
            "Epoch : 18 train_loss :1.845 , val_loss :148.437 val_acc : 46.03 lr :0.0000095 epoch_time : 176 sec\n",
            "Train Epoch 19:   0%|          | 0/782 [00:00<?, ?it/s]\n",
            "Test_ Epoch 19:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 00:35:49 2024 Epoch 19, lr: 0.0000095, val loss: 147.83976, acc: 46.55000\n",
            "Epoch : 19 train_loss :1.834 , val_loss :147.840 val_acc : 46.55 lr :0.0000054 epoch_time : 176 sec\n",
            "Train Epoch 20:   0%|          | 0/782 [00:00<?, ?it/s]\n",
            "Test_ Epoch 20:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Current time : Wed May 15 00:38:46 2024 Epoch 20, lr: 0.0000054, val loss: 147.53883, acc: 46.43000\n",
            "Epoch : 20 train_loss :1.838 , val_loss :147.539 val_acc : 46.43 lr :0.0000024 epoch_time : 176 sec\n"
          ]
        }
      ],
      "source": [
        "! python train.py --net cct_2 --bs 64 --size 224 --n_epochs 20 # A-100 # A-100 37GB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8AtB6ys0W7k7",
        "outputId": "ae50973b-5be1-4f81-e214-9f64b5892358",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Preparing data..\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "==> Building model..\n",
            "cuda\n",
            "/content/train.py:353: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  with tqdm_notebook(total=len(train_ds), desc=f\"Train Epoch {epoch+1}\") as pbar:\n",
            "Train Epoch 1:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/content/train.py:391: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  with tqdm_notebook(total=len(test_ds), desc=f\"Test_ Epoch {epoch+1}\") as pbar:\n",
            "Test_ Epoch 1:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 07:04:01 2024 Epoch 1, lr: 0.0001000, val loss: 189.59302, acc: 29.59000\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "Epoch : 1 train_loss :2.321 , val_loss :189.593 val_acc : 29.59 lr :0.0000994 epoch_time : 81 sec\n",
            "Train Epoch 2:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 2:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 07:05:22 2024 Epoch 2, lr: 0.0000994, val loss: 166.70947, acc: 39.14000\n",
            "Epoch : 2 train_loss :2.078 , val_loss :166.709 val_acc : 39.14 lr :0.0001000 epoch_time : 80 sec\n",
            "Train Epoch 3:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 3:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 07:06:43 2024 Epoch 3, lr: 0.0001000, val loss: 162.37209, acc: 41.20000\n",
            "Epoch : 3 train_loss :1.998 , val_loss :162.372 val_acc : 41.2 lr :0.0000994 epoch_time : 80 sec\n",
            "Train Epoch 4:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 4:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 07:08:04 2024 Epoch 4, lr: 0.0000994, val loss: 149.28509, acc: 46.11000\n",
            "Epoch : 4 train_loss :1.920 , val_loss :149.285 val_acc : 46.11 lr :0.0000976 epoch_time : 80 sec\n",
            "Train Epoch 5:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 5:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 07:09:25 2024 Epoch 5, lr: 0.0000976, val loss: 140.75824, acc: 48.41000\n",
            "Epoch : 5 train_loss :1.862 , val_loss :140.758 val_acc : 48.41 lr :0.0000946 epoch_time : 80 sec\n",
            "Train Epoch 6:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 6:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 07:10:45 2024 Epoch 6, lr: 0.0000946, val loss: 133.95417, acc: 51.25000\n",
            "Epoch : 6 train_loss :1.818 , val_loss :133.954 val_acc : 51.25 lr :0.0000905 epoch_time : 80 sec\n",
            "Train Epoch 7:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 7:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 07:12:06 2024 Epoch 7, lr: 0.0000905, val loss: 129.56950, acc: 52.29000\n",
            "Epoch : 7 train_loss :1.774 , val_loss :129.569 val_acc : 52.29 lr :0.0000854 epoch_time : 80 sec\n",
            "Train Epoch 8:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 8:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 07:13:27 2024 Epoch 8, lr: 0.0000854, val loss: 122.74640, acc: 55.22000\n",
            "Epoch : 8 train_loss :1.748 , val_loss :122.746 val_acc : 55.22 lr :0.0000794 epoch_time : 80 sec\n",
            "Train Epoch 9:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 9:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Current time : Wed May 15 07:14:46 2024 Epoch 9, lr: 0.0000794, val loss: 124.09119, acc: 54.99000\n",
            "Epoch : 9 train_loss :1.715 , val_loss :124.091 val_acc : 54.99 lr :0.0000727 epoch_time : 79 sec\n",
            "Train Epoch 10:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 10:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 07:16:07 2024 Epoch 10, lr: 0.0000727, val loss: 123.38154, acc: 55.42000\n",
            "Epoch : 10 train_loss :1.690 , val_loss :123.382 val_acc : 55.42 lr :0.0000655 epoch_time : 80 sec\n",
            "Train Epoch 11:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 11:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 07:17:28 2024 Epoch 11, lr: 0.0000655, val loss: 118.63835, acc: 57.23000\n",
            "Epoch : 11 train_loss :1.661 , val_loss :118.638 val_acc : 57.23 lr :0.0000578 epoch_time : 80 sec\n",
            "Train Epoch 12:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 12:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 07:18:48 2024 Epoch 12, lr: 0.0000578, val loss: 111.92751, acc: 59.19000\n",
            "Epoch : 12 train_loss :1.643 , val_loss :111.928 val_acc : 59.19 lr :0.0000500 epoch_time : 80 sec\n",
            "Train Epoch 13:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 13:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 07:20:09 2024 Epoch 13, lr: 0.0000500, val loss: 108.12717, acc: 61.16000\n",
            "Epoch : 13 train_loss :1.620 , val_loss :108.127 val_acc : 61.16 lr :0.0000422 epoch_time : 80 sec\n",
            "Train Epoch 14:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 14:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Current time : Wed May 15 07:21:29 2024 Epoch 14, lr: 0.0000422, val loss: 108.72307, acc: 61.01000\n",
            "Epoch : 14 train_loss :1.606 , val_loss :108.723 val_acc : 61.01 lr :0.0000345 epoch_time : 79 sec\n",
            "Train Epoch 15:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 15:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 07:22:50 2024 Epoch 15, lr: 0.0000345, val loss: 104.68139, acc: 61.55000\n",
            "Epoch : 15 train_loss :1.580 , val_loss :104.681 val_acc : 61.55 lr :0.0000273 epoch_time : 81 sec\n",
            "Train Epoch 16:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 16:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 07:24:10 2024 Epoch 16, lr: 0.0000273, val loss: 105.58268, acc: 62.14000\n",
            "Epoch : 16 train_loss :1.566 , val_loss :105.583 val_acc : 62.14 lr :0.0000206 epoch_time : 80 sec\n",
            "Train Epoch 17:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 17:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 07:25:31 2024 Epoch 17, lr: 0.0000206, val loss: 100.41079, acc: 64.02000\n",
            "Epoch : 17 train_loss :1.554 , val_loss :100.411 val_acc : 64.02 lr :0.0000146 epoch_time : 80 sec\n",
            "Train Epoch 18:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 18:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 07:26:52 2024 Epoch 18, lr: 0.0000146, val loss: 99.31263, acc: 64.49000\n",
            "Epoch : 18 train_loss :1.542 , val_loss :99.313 val_acc : 64.49 lr :0.0000095 epoch_time : 80 sec\n",
            "Train Epoch 19:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 19:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Current time : Wed May 15 07:28:12 2024 Epoch 19, lr: 0.0000095, val loss: 99.15527, acc: 64.34000\n",
            "Epoch : 19 train_loss :1.524 , val_loss :99.155 val_acc : 64.34 lr :0.0000054 epoch_time : 80 sec\n",
            "Train Epoch 20:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 20:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 07:29:34 2024 Epoch 20, lr: 0.0000054, val loss: 96.92974, acc: 65.41000\n",
            "Epoch : 20 train_loss :1.520 , val_loss :96.930 val_acc : 65.41 lr :0.0000024 epoch_time : 82 sec\n"
          ]
        }
      ],
      "source": [
        "! python train.py --net cross_vit --bs 256 --size 256 --n_epochs 20 # A-100 with bs 256 GPU 22GB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4k-feZdPW-4O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60daf7b6-1655-45dc-d153-ad46acb8ef27",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Preparing data..\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "==> Building model..\n",
            "cuda\n",
            "/content/train.py:365: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  with tqdm_notebook(total=len(train_ds), desc=f\"Train Epoch {epoch+1}\") as pbar:\n",
            "Train Epoch 1:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/content/train.py:403: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  with tqdm_notebook(total=len(test_ds), desc=f\"Test_ Epoch {epoch+1}\") as pbar:\n",
            "Test_ Epoch 1:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 07:33:08 2024 Epoch 1, lr: 0.0001000, val loss: 185.68032, acc: 32.11000\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "Epoch : 1 train_loss :2.351 , val_loss :185.680 val_acc : 32.11 lr :0.0000994 epoch_time : 136 sec\n",
            "Train Epoch 2:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 2:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 07:35:22 2024 Epoch 2, lr: 0.0000994, val loss: 172.38200, acc: 38.43000\n",
            "Epoch : 2 train_loss :2.067 , val_loss :172.382 val_acc : 38.43 lr :0.0001000 epoch_time : 133 sec\n",
            "Train Epoch 3:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 3:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 07:37:36 2024 Epoch 3, lr: 0.0001000, val loss: 148.52063, acc: 45.37000\n",
            "Epoch : 3 train_loss :1.937 , val_loss :148.521 val_acc : 45.37 lr :0.0000994 epoch_time : 133 sec\n",
            "Train Epoch 4:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 4:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 07:39:50 2024 Epoch 4, lr: 0.0000994, val loss: 135.23883, acc: 52.36000\n",
            "Epoch : 4 train_loss :1.822 , val_loss :135.239 val_acc : 52.36 lr :0.0000976 epoch_time : 133 sec\n",
            "Train Epoch 5:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 5:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 07:42:03 2024 Epoch 5, lr: 0.0000976, val loss: 116.65373, acc: 57.82000\n",
            "Epoch : 5 train_loss :1.727 , val_loss :116.654 val_acc : 57.82 lr :0.0000946 epoch_time : 133 sec\n",
            "Train Epoch 6:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 6:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 07:44:17 2024 Epoch 6, lr: 0.0000946, val loss: 109.36479, acc: 61.01000\n",
            "Epoch : 6 train_loss :1.635 , val_loss :109.365 val_acc : 61.01 lr :0.0000905 epoch_time : 133 sec\n",
            "Train Epoch 7:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 7:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 07:46:31 2024 Epoch 7, lr: 0.0000905, val loss: 96.93083, acc: 66.00000\n",
            "Epoch : 7 train_loss :1.556 , val_loss :96.931 val_acc : 66.0 lr :0.0000854 epoch_time : 133 sec\n",
            "Train Epoch 8:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 8:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 07:48:45 2024 Epoch 8, lr: 0.0000854, val loss: 93.91893, acc: 67.04000\n",
            "Epoch : 8 train_loss :1.495 , val_loss :93.919 val_acc : 67.04 lr :0.0000794 epoch_time : 133 sec\n",
            "Train Epoch 9:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 9:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 07:50:58 2024 Epoch 9, lr: 0.0000794, val loss: 83.35443, acc: 70.30000\n",
            "Epoch : 9 train_loss :1.432 , val_loss :83.354 val_acc : 70.3 lr :0.0000727 epoch_time : 133 sec\n",
            "Train Epoch 10:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 10:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 07:53:12 2024 Epoch 10, lr: 0.0000727, val loss: 78.81414, acc: 72.31000\n",
            "Epoch : 10 train_loss :1.379 , val_loss :78.814 val_acc : 72.31 lr :0.0000655 epoch_time : 133 sec\n",
            "Train Epoch 11:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 11:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 07:55:26 2024 Epoch 11, lr: 0.0000655, val loss: 72.93141, acc: 74.61000\n",
            "Epoch : 11 train_loss :1.336 , val_loss :72.931 val_acc : 74.61 lr :0.0000578 epoch_time : 133 sec\n",
            "Train Epoch 12:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 12:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 07:57:40 2024 Epoch 12, lr: 0.0000578, val loss: 71.95802, acc: 75.21000\n",
            "Epoch : 12 train_loss :1.305 , val_loss :71.958 val_acc : 75.21 lr :0.0000500 epoch_time : 133 sec\n",
            "Train Epoch 13:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 13:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 07:59:54 2024 Epoch 13, lr: 0.0000500, val loss: 67.08745, acc: 76.34000\n",
            "Epoch : 13 train_loss :1.276 , val_loss :67.087 val_acc : 76.34 lr :0.0000422 epoch_time : 133 sec\n",
            "Train Epoch 14:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 14:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 08:02:08 2024 Epoch 14, lr: 0.0000422, val loss: 62.26719, acc: 78.31000\n",
            "Epoch : 14 train_loss :1.227 , val_loss :62.267 val_acc : 78.31 lr :0.0000345 epoch_time : 133 sec\n",
            "Train Epoch 15:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 15:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 08:04:22 2024 Epoch 15, lr: 0.0000345, val loss: 59.34883, acc: 79.39000\n",
            "Epoch : 15 train_loss :1.210 , val_loss :59.349 val_acc : 79.39 lr :0.0000273 epoch_time : 133 sec\n",
            "Train Epoch 16:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 16:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 08:06:35 2024 Epoch 16, lr: 0.0000273, val loss: 57.65596, acc: 80.10000\n",
            "Epoch : 16 train_loss :1.190 , val_loss :57.656 val_acc : 80.1 lr :0.0000206 epoch_time : 133 sec\n",
            "Train Epoch 17:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 17:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Current time : Wed May 15 08:08:48 2024 Epoch 17, lr: 0.0000206, val loss: 57.90216, acc: 80.09000\n",
            "Epoch : 17 train_loss :1.165 , val_loss :57.902 val_acc : 80.09 lr :0.0000146 epoch_time : 132 sec\n",
            "Train Epoch 18:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 18:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 08:11:02 2024 Epoch 18, lr: 0.0000146, val loss: 53.83396, acc: 81.49000\n",
            "Epoch : 18 train_loss :1.144 , val_loss :53.834 val_acc : 81.49 lr :0.0000095 epoch_time : 133 sec\n",
            "Train Epoch 19:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 19:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 08:13:17 2024 Epoch 19, lr: 0.0000095, val loss: 52.64878, acc: 81.73000\n",
            "Epoch : 19 train_loss :1.129 , val_loss :52.649 val_acc : 81.73 lr :0.0000054 epoch_time : 134 sec\n",
            "Train Epoch 20:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 20:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 08:15:31 2024 Epoch 20, lr: 0.0000054, val loss: 51.13585, acc: 82.38000\n",
            "Epoch : 20 train_loss :1.111 , val_loss :51.136 val_acc : 82.38 lr :0.0000024 epoch_time : 133 sec\n"
          ]
        }
      ],
      "source": [
        "! python train.py --net crossformer --bs 256 --size 224 --n_epochs 20 # A-100 with bs 256 GPU 38.1 GB"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! python train.py --net cvt --bs 256 --size 224 --n_epochs 20 # A-100 with bs 256 GPU 30 GB"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GqvPWlO2MngN",
        "outputId": "ea323c7d-5ec2-4d33-91b2-4d6cc9297fb3",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Preparing data..\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "==> Building model..\n",
            "cuda\n",
            "/content/train.py:393: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  with tqdm_notebook(total=len(train_ds), desc=f\"Train Epoch {epoch+1}\") as pbar:\n",
            "Train Epoch 1:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/content/train.py:431: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  with tqdm_notebook(total=len(test_ds), desc=f\"Test_ Epoch {epoch+1}\") as pbar:\n",
            "Test_ Epoch 1:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 09:10:17 2024 Epoch 1, lr: 0.0001000, val loss: 179.66960, acc: 33.11000\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "Epoch : 1 train_loss :2.243 , val_loss :179.670 val_acc : 33.11 lr :0.0000994 epoch_time : 91 sec\n",
            "Train Epoch 2:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 2:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 09:11:46 2024 Epoch 2, lr: 0.0000994, val loss: 159.96873, acc: 42.64000\n",
            "Epoch : 2 train_loss :2.038 , val_loss :159.969 val_acc : 42.64 lr :0.0001000 epoch_time : 88 sec\n",
            "Train Epoch 3:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 3:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 09:13:14 2024 Epoch 3, lr: 0.0001000, val loss: 148.13705, acc: 46.27000\n",
            "Epoch : 3 train_loss :1.904 , val_loss :148.137 val_acc : 46.27 lr :0.0000994 epoch_time : 88 sec\n",
            "Train Epoch 4:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 4:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 09:14:43 2024 Epoch 4, lr: 0.0000994, val loss: 139.59472, acc: 50.02000\n",
            "Epoch : 4 train_loss :1.821 , val_loss :139.595 val_acc : 50.02 lr :0.0000976 epoch_time : 88 sec\n",
            "Train Epoch 5:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 5:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 09:16:12 2024 Epoch 5, lr: 0.0000976, val loss: 129.46855, acc: 52.99000\n",
            "Epoch : 5 train_loss :1.756 , val_loss :129.469 val_acc : 52.99 lr :0.0000946 epoch_time : 88 sec\n",
            "Train Epoch 6:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 6:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 09:17:40 2024 Epoch 6, lr: 0.0000946, val loss: 118.62247, acc: 56.63000\n",
            "Epoch : 6 train_loss :1.708 , val_loss :118.622 val_acc : 56.63 lr :0.0000905 epoch_time : 88 sec\n",
            "Train Epoch 7:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 7:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 09:19:09 2024 Epoch 7, lr: 0.0000905, val loss: 109.98271, acc: 60.56000\n",
            "Epoch : 7 train_loss :1.651 , val_loss :109.983 val_acc : 60.56 lr :0.0000854 epoch_time : 88 sec\n",
            "Train Epoch 8:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 8:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 09:20:38 2024 Epoch 8, lr: 0.0000854, val loss: 107.41183, acc: 61.92000\n",
            "Epoch : 8 train_loss :1.613 , val_loss :107.412 val_acc : 61.92 lr :0.0000794 epoch_time : 88 sec\n",
            "Train Epoch 9:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 9:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 09:22:06 2024 Epoch 9, lr: 0.0000794, val loss: 101.71505, acc: 63.35000\n",
            "Epoch : 9 train_loss :1.572 , val_loss :101.715 val_acc : 63.35 lr :0.0000727 epoch_time : 88 sec\n",
            "Train Epoch 10:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 10:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 09:23:35 2024 Epoch 10, lr: 0.0000727, val loss: 100.19738, acc: 64.12000\n",
            "Epoch : 10 train_loss :1.545 , val_loss :100.197 val_acc : 64.12 lr :0.0000655 epoch_time : 88 sec\n",
            "Train Epoch 11:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 11:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 09:25:03 2024 Epoch 11, lr: 0.0000655, val loss: 94.17118, acc: 66.52000\n",
            "Epoch : 11 train_loss :1.503 , val_loss :94.171 val_acc : 66.52 lr :0.0000578 epoch_time : 88 sec\n",
            "Train Epoch 12:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 12:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 09:26:32 2024 Epoch 12, lr: 0.0000578, val loss: 90.47470, acc: 67.93000\n",
            "Epoch : 12 train_loss :1.468 , val_loss :90.475 val_acc : 67.93 lr :0.0000500 epoch_time : 88 sec\n",
            "Train Epoch 13:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 13:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 09:28:01 2024 Epoch 13, lr: 0.0000500, val loss: 86.96274, acc: 69.60000\n",
            "Epoch : 13 train_loss :1.443 , val_loss :86.963 val_acc : 69.6 lr :0.0000422 epoch_time : 88 sec\n",
            "Train Epoch 14:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 14:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 09:29:29 2024 Epoch 14, lr: 0.0000422, val loss: 84.58937, acc: 69.95000\n",
            "Epoch : 14 train_loss :1.409 , val_loss :84.589 val_acc : 69.95 lr :0.0000345 epoch_time : 88 sec\n",
            "Train Epoch 15:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 15:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 09:30:58 2024 Epoch 15, lr: 0.0000345, val loss: 80.18850, acc: 71.80000\n",
            "Epoch : 15 train_loss :1.387 , val_loss :80.188 val_acc : 71.8 lr :0.0000273 epoch_time : 88 sec\n",
            "Train Epoch 16:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 16:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 09:32:27 2024 Epoch 16, lr: 0.0000273, val loss: 78.29329, acc: 72.62000\n",
            "Epoch : 16 train_loss :1.361 , val_loss :78.293 val_acc : 72.62 lr :0.0000206 epoch_time : 88 sec\n",
            "Train Epoch 17:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 17:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 09:33:56 2024 Epoch 17, lr: 0.0000206, val loss: 75.03929, acc: 73.55000\n",
            "Epoch : 17 train_loss :1.336 , val_loss :75.039 val_acc : 73.55 lr :0.0000146 epoch_time : 88 sec\n",
            "Train Epoch 18:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 18:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 09:35:25 2024 Epoch 18, lr: 0.0000146, val loss: 73.68079, acc: 73.86000\n",
            "Epoch : 18 train_loss :1.329 , val_loss :73.681 val_acc : 73.86 lr :0.0000095 epoch_time : 89 sec\n",
            "Train Epoch 19:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 19:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 09:36:54 2024 Epoch 19, lr: 0.0000095, val loss: 71.80133, acc: 74.61000\n",
            "Epoch : 19 train_loss :1.314 , val_loss :71.801 val_acc : 74.61 lr :0.0000054 epoch_time : 88 sec\n",
            "Train Epoch 20:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 20:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 09:38:23 2024 Epoch 20, lr: 0.0000054, val loss: 71.28930, acc: 74.98000\n",
            "Epoch : 20 train_loss :1.303 , val_loss :71.289 val_acc : 74.98 lr :0.0000024 epoch_time : 89 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! python train.py --net deepvit --size 256 --n_epochs 20 # A-100 with bs 256 GPU XX GB"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JwILpM_XhkUJ",
        "outputId": "c19ae425-5504-4712-cdbb-c0ed2d242625",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Preparing data..\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "==> Building model..\n",
            "cuda\n",
            "/content/train.py:406: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  with tqdm_notebook(total=len(train_ds), desc=f\"Train Epoch {epoch+1}\") as pbar:\n",
            "Train Epoch 1:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/content/train.py:444: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  with tqdm_notebook(total=len(test_ds), desc=f\"Test_ Epoch {epoch+1}\") as pbar:\n",
            "Test_ Epoch 1:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 09:39:30 2024 Epoch 1, lr: 0.0001000, val loss: 190.97347, acc: 27.04000\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "Epoch : 1 train_loss :2.251 , val_loss :190.973 val_acc : 27.04 lr :0.0000994 epoch_time : 61 sec\n",
            "Train Epoch 2:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 2:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 09:40:31 2024 Epoch 2, lr: 0.0000994, val loss: 181.61664, acc: 32.19000\n",
            "Epoch : 2 train_loss :2.103 , val_loss :181.617 val_acc : 32.19 lr :0.0001000 epoch_time : 60 sec\n",
            "Train Epoch 3:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 3:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 09:41:31 2024 Epoch 3, lr: 0.0001000, val loss: 172.08346, acc: 36.28000\n",
            "Epoch : 3 train_loss :2.050 , val_loss :172.083 val_acc : 36.28 lr :0.0000994 epoch_time : 60 sec\n",
            "Train Epoch 4:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 4:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 09:42:33 2024 Epoch 4, lr: 0.0000994, val loss: 165.43187, acc: 39.40000\n",
            "Epoch : 4 train_loss :2.000 , val_loss :165.432 val_acc : 39.4 lr :0.0000976 epoch_time : 61 sec\n",
            "Train Epoch 5:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 5:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 09:43:33 2024 Epoch 5, lr: 0.0000976, val loss: 164.02789, acc: 39.49000\n",
            "Epoch : 5 train_loss :1.973 , val_loss :164.028 val_acc : 39.49 lr :0.0000946 epoch_time : 60 sec\n",
            "Train Epoch 6:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 6:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 09:44:34 2024 Epoch 6, lr: 0.0000946, val loss: 155.78860, acc: 43.39000\n",
            "Epoch : 6 train_loss :1.943 , val_loss :155.789 val_acc : 43.39 lr :0.0000905 epoch_time : 60 sec\n",
            "Train Epoch 7:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 7:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 09:45:34 2024 Epoch 7, lr: 0.0000905, val loss: 153.99793, acc: 44.16000\n",
            "Epoch : 7 train_loss :1.924 , val_loss :153.998 val_acc : 44.16 lr :0.0000854 epoch_time : 60 sec\n",
            "Train Epoch 8:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 8:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 09:46:35 2024 Epoch 8, lr: 0.0000854, val loss: 152.56968, acc: 44.26000\n",
            "Epoch : 8 train_loss :1.908 , val_loss :152.570 val_acc : 44.26 lr :0.0000794 epoch_time : 60 sec\n",
            "Train Epoch 9:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 9:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 09:47:36 2024 Epoch 9, lr: 0.0000794, val loss: 150.51953, acc: 45.11000\n",
            "Epoch : 9 train_loss :1.891 , val_loss :150.520 val_acc : 45.11 lr :0.0000727 epoch_time : 60 sec\n",
            "Train Epoch 10:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 10:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 09:48:36 2024 Epoch 10, lr: 0.0000727, val loss: 148.94697, acc: 45.83000\n",
            "Epoch : 10 train_loss :1.879 , val_loss :148.947 val_acc : 45.83 lr :0.0000655 epoch_time : 60 sec\n",
            "Train Epoch 11:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 11:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 09:49:37 2024 Epoch 11, lr: 0.0000655, val loss: 144.76669, acc: 46.95000\n",
            "Epoch : 11 train_loss :1.865 , val_loss :144.767 val_acc : 46.95 lr :0.0000578 epoch_time : 60 sec\n",
            "Train Epoch 12:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 12:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 09:50:37 2024 Epoch 12, lr: 0.0000578, val loss: 143.58825, acc: 47.65000\n",
            "Epoch : 12 train_loss :1.853 , val_loss :143.588 val_acc : 47.65 lr :0.0000500 epoch_time : 60 sec\n",
            "Train Epoch 13:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 13:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 09:51:38 2024 Epoch 13, lr: 0.0000500, val loss: 142.53158, acc: 48.23000\n",
            "Epoch : 13 train_loss :1.834 , val_loss :142.532 val_acc : 48.23 lr :0.0000422 epoch_time : 60 sec\n",
            "Train Epoch 14:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 14:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Current time : Wed May 15 09:52:37 2024 Epoch 14, lr: 0.0000422, val loss: 142.66329, acc: 48.13000\n",
            "Epoch : 14 train_loss :1.816 , val_loss :142.663 val_acc : 48.13 lr :0.0000345 epoch_time : 59 sec\n",
            "Train Epoch 15:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 15:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 09:53:38 2024 Epoch 15, lr: 0.0000345, val loss: 137.30543, acc: 49.92000\n",
            "Epoch : 15 train_loss :1.809 , val_loss :137.305 val_acc : 49.92 lr :0.0000273 epoch_time : 60 sec\n",
            "Train Epoch 16:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 16:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 09:54:38 2024 Epoch 16, lr: 0.0000273, val loss: 137.75973, acc: 50.04000\n",
            "Epoch : 16 train_loss :1.798 , val_loss :137.760 val_acc : 50.04 lr :0.0000206 epoch_time : 60 sec\n",
            "Train Epoch 17:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 17:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 09:55:39 2024 Epoch 17, lr: 0.0000206, val loss: 134.88880, acc: 51.37000\n",
            "Epoch : 17 train_loss :1.786 , val_loss :134.889 val_acc : 51.37 lr :0.0000146 epoch_time : 60 sec\n",
            "Train Epoch 18:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 18:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 09:56:39 2024 Epoch 18, lr: 0.0000146, val loss: 132.99557, acc: 52.03000\n",
            "Epoch : 18 train_loss :1.777 , val_loss :132.996 val_acc : 52.03 lr :0.0000095 epoch_time : 60 sec\n",
            "Train Epoch 19:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 19:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Current time : Wed May 15 09:57:38 2024 Epoch 19, lr: 0.0000095, val loss: 133.03800, acc: 51.97000\n",
            "Epoch : 19 train_loss :1.767 , val_loss :133.038 val_acc : 51.97 lr :0.0000054 epoch_time : 59 sec\n",
            "Train Epoch 20:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 20:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Current time : Wed May 15 09:58:37 2024 Epoch 20, lr: 0.0000054, val loss: 132.74584, acc: 51.94000\n",
            "Epoch : 20 train_loss :1.764 , val_loss :132.746 val_acc : 51.94 lr :0.0000024 epoch_time : 58 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cannot test at A-100\n",
        "# ! python train.py --net efficient_att_vit --bs 32 --size 256 --n_epochs 20 # Cannot test at A-100"
      ],
      "metadata": {
        "id": "v3xKVrrHMgYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! python train.py --net levit --size 224 --n_epochs 20 # A-100 with bs 256 GPU 11.5 GB"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "P7uYLQi6R3hX",
        "outputId": "37b6ce2b-603d-4d21-b429-5a90917755aa"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Preparing data..\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n",
            "100% 170498071/170498071 [00:12<00:00, 13146807.31it/s]\n",
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "==> Building model..\n",
            "cuda\n",
            "/content/train.py:457: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  with tqdm_notebook(total=len(train_ds), desc=f\"Train Epoch {epoch+1}\") as pbar:\n",
            "Train Epoch 1:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/content/train.py:495: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  with tqdm_notebook(total=len(test_ds), desc=f\"Test_ Epoch {epoch+1}\") as pbar:\n",
            "Test_ Epoch 1:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 22:04:23 2024 Epoch 1, lr: 0.0001000, val loss: 213.61463, acc: 15.56000\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "Epoch : 1 train_loss :2.259 , val_loss :213.615 val_acc : 15.56 lr :0.0000994 epoch_time : 42 sec\n",
            "Train Epoch 2:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 2:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 22:05:02 2024 Epoch 2, lr: 0.0000994, val loss: 199.31730, acc: 21.77000\n",
            "Epoch : 2 train_loss :2.154 , val_loss :199.317 val_acc : 21.77 lr :0.0001000 epoch_time : 39 sec\n",
            "Train Epoch 3:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 3:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 22:05:41 2024 Epoch 3, lr: 0.0001000, val loss: 192.35096, acc: 23.82000\n",
            "Epoch : 3 train_loss :2.099 , val_loss :192.351 val_acc : 23.82 lr :0.0000994 epoch_time : 38 sec\n",
            "Train Epoch 4:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 4:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 22:06:20 2024 Epoch 4, lr: 0.0000994, val loss: 188.89245, acc: 25.66000\n",
            "Epoch : 4 train_loss :2.050 , val_loss :188.892 val_acc : 25.66 lr :0.0000976 epoch_time : 39 sec\n",
            "Train Epoch 5:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 5:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 22:06:59 2024 Epoch 5, lr: 0.0000976, val loss: 176.36600, acc: 30.19000\n",
            "Epoch : 5 train_loss :2.030 , val_loss :176.366 val_acc : 30.19 lr :0.0000946 epoch_time : 39 sec\n",
            "Train Epoch 6:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 6:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Current time : Wed May 15 22:07:38 2024 Epoch 6, lr: 0.0000946, val loss: 202.55225, acc: 26.64000\n",
            "Epoch : 6 train_loss :1.993 , val_loss :202.552 val_acc : 26.64 lr :0.0000905 epoch_time : 38 sec\n",
            "Train Epoch 7:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 7:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 22:08:17 2024 Epoch 7, lr: 0.0000905, val loss: 164.60676, acc: 36.73000\n",
            "Epoch : 7 train_loss :1.959 , val_loss :164.607 val_acc : 36.73 lr :0.0000854 epoch_time : 39 sec\n",
            "Train Epoch 8:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 8:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 22:08:56 2024 Epoch 8, lr: 0.0000854, val loss: 166.06374, acc: 38.83000\n",
            "Epoch : 8 train_loss :1.932 , val_loss :166.064 val_acc : 38.83 lr :0.0000794 epoch_time : 39 sec\n",
            "Train Epoch 9:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 9:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 22:09:35 2024 Epoch 9, lr: 0.0000794, val loss: 155.21814, acc: 42.29000\n",
            "Epoch : 9 train_loss :1.905 , val_loss :155.218 val_acc : 42.29 lr :0.0000727 epoch_time : 39 sec\n",
            "Train Epoch 10:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 10:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 22:10:15 2024 Epoch 10, lr: 0.0000727, val loss: 153.46866, acc: 43.16000\n",
            "Epoch : 10 train_loss :1.885 , val_loss :153.469 val_acc : 43.16 lr :0.0000655 epoch_time : 39 sec\n",
            "Train Epoch 11:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 11:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 22:10:54 2024 Epoch 11, lr: 0.0000655, val loss: 149.57842, acc: 45.54000\n",
            "Epoch : 11 train_loss :1.859 , val_loss :149.578 val_acc : 45.54 lr :0.0000578 epoch_time : 39 sec\n",
            "Train Epoch 12:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 12:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 22:11:33 2024 Epoch 12, lr: 0.0000578, val loss: 143.38922, acc: 47.48000\n",
            "Epoch : 12 train_loss :1.828 , val_loss :143.389 val_acc : 47.48 lr :0.0000500 epoch_time : 38 sec\n",
            "Train Epoch 13:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 13:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 22:12:12 2024 Epoch 13, lr: 0.0000500, val loss: 139.48395, acc: 49.10000\n",
            "Epoch : 13 train_loss :1.808 , val_loss :139.484 val_acc : 49.1 lr :0.0000422 epoch_time : 38 sec\n",
            "Train Epoch 14:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 14:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 22:12:50 2024 Epoch 14, lr: 0.0000422, val loss: 135.82813, acc: 50.82000\n",
            "Epoch : 14 train_loss :1.790 , val_loss :135.828 val_acc : 50.82 lr :0.0000345 epoch_time : 38 sec\n",
            "Train Epoch 15:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 15:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 22:13:30 2024 Epoch 15, lr: 0.0000345, val loss: 131.69844, acc: 51.92000\n",
            "Epoch : 15 train_loss :1.770 , val_loss :131.698 val_acc : 51.92 lr :0.0000273 epoch_time : 39 sec\n",
            "Train Epoch 16:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 16:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 22:14:09 2024 Epoch 16, lr: 0.0000273, val loss: 130.92925, acc: 52.85000\n",
            "Epoch : 16 train_loss :1.755 , val_loss :130.929 val_acc : 52.85 lr :0.0000206 epoch_time : 39 sec\n",
            "Train Epoch 17:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 17:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 22:14:48 2024 Epoch 17, lr: 0.0000206, val loss: 130.05922, acc: 53.20000\n",
            "Epoch : 17 train_loss :1.744 , val_loss :130.059 val_acc : 53.2 lr :0.0000146 epoch_time : 39 sec\n",
            "Train Epoch 18:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 18:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 22:15:27 2024 Epoch 18, lr: 0.0000146, val loss: 127.68232, acc: 53.39000\n",
            "Epoch : 18 train_loss :1.736 , val_loss :127.682 val_acc : 53.39 lr :0.0000095 epoch_time : 38 sec\n",
            "Train Epoch 19:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 19:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 22:16:06 2024 Epoch 19, lr: 0.0000095, val loss: 126.01989, acc: 53.82000\n",
            "Epoch : 19 train_loss :1.727 , val_loss :126.020 val_acc : 53.82 lr :0.0000054 epoch_time : 38 sec\n",
            "Train Epoch 20:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 20:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 22:16:45 2024 Epoch 20, lr: 0.0000054, val loss: 126.18343, acc: 54.45000\n",
            "Epoch : 20 train_loss :1.716 , val_loss :126.183 val_acc : 54.45 lr :0.0000024 epoch_time : 39 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! python train.py --net max_vit --bs 128 --size 224 --n_epochs 20 # A-100 with bs 256 GPU 36 GB"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "9JGR7NsfWhV2",
        "outputId": "790f0098-0249-4c6f-b340-94c099ea03ec"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Preparing data..\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "==> Building model..\n",
            "cuda\n",
            "/content/train.py:473: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  with tqdm_notebook(total=len(train_ds), desc=f\"Train Epoch {epoch+1}\") as pbar:\n",
            "Train Epoch 1:   0%|          | 0/391 [00:00<?, ?it/s]\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/content/train.py:511: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  with tqdm_notebook(total=len(test_ds), desc=f\"Test_ Epoch {epoch+1}\") as pbar:\n",
            "Test_ Epoch 1:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 22:33:26 2024 Epoch 1, lr: 0.0001000, val loss: 158.40040, acc: 41.07000\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "Epoch : 1 train_loss :2.063 , val_loss :158.400 val_acc : 41.07 lr :0.0000994 epoch_time : 209 sec\n",
            "Train Epoch 2:   0%|          | 0/391 [00:00<?, ?it/s]\n",
            "Test_ Epoch 2:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 22:36:55 2024 Epoch 2, lr: 0.0000994, val loss: 114.44480, acc: 59.19000\n",
            "Epoch : 2 train_loss :1.670 , val_loss :114.445 val_acc : 59.19 lr :0.0001000 epoch_time : 208 sec\n",
            "Train Epoch 3:   0%|          | 0/391 [00:00<?, ?it/s]\n",
            "Test_ Epoch 3:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 22:40:23 2024 Epoch 3, lr: 0.0001000, val loss: 77.15729, acc: 73.15000\n",
            "Epoch : 3 train_loss :1.471 , val_loss :77.157 val_acc : 73.15 lr :0.0000994 epoch_time : 208 sec\n",
            "Train Epoch 4:   0%|          | 0/391 [00:00<?, ?it/s]\n",
            "Test_ Epoch 4:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 22:43:51 2024 Epoch 4, lr: 0.0000994, val loss: 76.29005, acc: 73.60000\n",
            "Epoch : 4 train_loss :1.333 , val_loss :76.290 val_acc : 73.6 lr :0.0000976 epoch_time : 208 sec\n",
            "Train Epoch 5:   0%|          | 0/391 [00:00<?, ?it/s]\n",
            "Test_ Epoch 5:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 22:47:20 2024 Epoch 5, lr: 0.0000976, val loss: 60.38518, acc: 79.11000\n",
            "Epoch : 5 train_loss :1.250 , val_loss :60.385 val_acc : 79.11 lr :0.0000946 epoch_time : 208 sec\n",
            "Train Epoch 6:   0%|          | 0/391 [00:00<?, ?it/s]\n",
            "Test_ Epoch 6:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 22:50:48 2024 Epoch 6, lr: 0.0000946, val loss: 55.27499, acc: 80.37000\n",
            "Epoch : 6 train_loss :1.193 , val_loss :55.275 val_acc : 80.37 lr :0.0000905 epoch_time : 208 sec\n",
            "Train Epoch 7:   0%|          | 0/391 [00:00<?, ?it/s]\n",
            "Test_ Epoch 7:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 22:54:17 2024 Epoch 7, lr: 0.0000905, val loss: 51.30817, acc: 82.49000\n",
            "Epoch : 7 train_loss :1.148 , val_loss :51.308 val_acc : 82.49 lr :0.0000854 epoch_time : 208 sec\n",
            "Train Epoch 8:   0%|          | 0/391 [00:00<?, ?it/s]\n",
            "Test_ Epoch 8:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 22:57:45 2024 Epoch 8, lr: 0.0000854, val loss: 48.63546, acc: 83.31000\n",
            "Epoch : 8 train_loss :1.108 , val_loss :48.635 val_acc : 83.31 lr :0.0000794 epoch_time : 208 sec\n",
            "Train Epoch 9:   0%|          | 0/391 [00:00<?, ?it/s]\n",
            "Test_ Epoch 9:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 23:01:13 2024 Epoch 9, lr: 0.0000794, val loss: 45.92471, acc: 84.36000\n",
            "Epoch : 9 train_loss :1.072 , val_loss :45.925 val_acc : 84.36 lr :0.0000727 epoch_time : 208 sec\n",
            "Train Epoch 10:   0%|          | 0/391 [00:00<?, ?it/s]\n",
            "Test_ Epoch 10:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 23:04:42 2024 Epoch 10, lr: 0.0000727, val loss: 40.27593, acc: 86.25000\n",
            "Epoch : 10 train_loss :1.040 , val_loss :40.276 val_acc : 86.25 lr :0.0000655 epoch_time : 208 sec\n",
            "Train Epoch 11:   0%|          | 0/391 [00:00<?, ?it/s]\n",
            "Test_ Epoch 11:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Current time : Wed May 15 23:08:08 2024 Epoch 11, lr: 0.0000655, val loss: 42.18268, acc: 85.49000\n",
            "Epoch : 11 train_loss :1.016 , val_loss :42.183 val_acc : 85.49 lr :0.0000578 epoch_time : 206 sec\n",
            "Train Epoch 12:   0%|          | 0/391 [00:00<?, ?it/s]\n",
            "Test_ Epoch 12:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Current time : Wed May 15 23:11:35 2024 Epoch 12, lr: 0.0000578, val loss: 40.28922, acc: 85.83000\n",
            "Epoch : 12 train_loss :0.989 , val_loss :40.289 val_acc : 85.83 lr :0.0000500 epoch_time : 206 sec\n",
            "Train Epoch 13:   0%|          | 0/391 [00:00<?, ?it/s]\n",
            "Test_ Epoch 13:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 23:15:04 2024 Epoch 13, lr: 0.0000500, val loss: 34.19375, acc: 88.06000\n",
            "Epoch : 13 train_loss :0.967 , val_loss :34.194 val_acc : 88.06 lr :0.0000422 epoch_time : 208 sec\n",
            "Train Epoch 14:   0%|          | 0/391 [00:00<?, ?it/s]\n",
            "Test_ Epoch 14:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 23:18:33 2024 Epoch 14, lr: 0.0000422, val loss: 31.55533, acc: 89.39000\n",
            "Epoch : 14 train_loss :0.939 , val_loss :31.555 val_acc : 89.39 lr :0.0000345 epoch_time : 209 sec\n",
            "Train Epoch 15:   0%|          | 0/391 [00:00<?, ?it/s]\n",
            "Test_ Epoch 15:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 23:22:02 2024 Epoch 15, lr: 0.0000345, val loss: 30.01726, acc: 89.64000\n",
            "Epoch : 15 train_loss :0.926 , val_loss :30.017 val_acc : 89.64 lr :0.0000273 epoch_time : 208 sec\n",
            "Train Epoch 16:   0%|          | 0/391 [00:00<?, ?it/s]\n",
            "Test_ Epoch 16:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 23:25:30 2024 Epoch 16, lr: 0.0000273, val loss: 29.02551, acc: 89.95000\n",
            "Epoch : 16 train_loss :0.907 , val_loss :29.026 val_acc : 89.95 lr :0.0000206 epoch_time : 208 sec\n",
            "Train Epoch 17:   0%|          | 0/391 [00:00<?, ?it/s]\n",
            "Test_ Epoch 17:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 23:28:59 2024 Epoch 17, lr: 0.0000206, val loss: 26.87867, acc: 90.87000\n",
            "Epoch : 17 train_loss :0.883 , val_loss :26.879 val_acc : 90.87 lr :0.0000146 epoch_time : 208 sec\n",
            "Train Epoch 18:   0%|          | 0/391 [00:00<?, ?it/s]\n",
            "Test_ Epoch 18:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Current time : Wed May 15 23:32:26 2024 Epoch 18, lr: 0.0000146, val loss: 27.16054, acc: 90.72000\n",
            "Epoch : 18 train_loss :0.883 , val_loss :27.161 val_acc : 90.72 lr :0.0000095 epoch_time : 206 sec\n",
            "Train Epoch 19:   0%|          | 0/391 [00:00<?, ?it/s]\n",
            "Test_ Epoch 19:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 23:35:55 2024 Epoch 19, lr: 0.0000095, val loss: 25.02263, acc: 91.29000\n",
            "Epoch : 19 train_loss :0.862 , val_loss :25.023 val_acc : 91.29 lr :0.0000054 epoch_time : 208 sec\n",
            "Train Epoch 20:   0%|          | 0/391 [00:00<?, ?it/s]\n",
            "Test_ Epoch 20:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 23:39:23 2024 Epoch 20, lr: 0.0000054, val loss: 24.69637, acc: 91.35000\n",
            "Epoch : 20 train_loss :0.860 , val_loss :24.696 val_acc : 91.35 lr :0.0000024 epoch_time : 208 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! python train.py --net max_vit_with_registers --bs 128 --size 224 --n_epochs 20 # A-100 with bs 256 GPU 38 GB"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4732edfc-7e08-4713-e10a-eeaf001ab0cd",
        "collapsed": true,
        "id": "A0C0HR-qtUO4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Preparing data..\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n",
            "100% 170498071/170498071 [00:01<00:00, 101997376.36it/s]\n",
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "==> Building model..\n",
            "cuda\n",
            "/content/train.py:489: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  with tqdm_notebook(total=len(train_ds), desc=f\"Train Epoch {epoch+1}\") as pbar:\n",
            "Train Epoch 1:   0%|          | 0/391 [00:00<?, ?it/s]\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/content/train.py:527: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  with tqdm_notebook(total=len(test_ds), desc=f\"Test_ Epoch {epoch+1}\") as pbar:\n",
            "Test_ Epoch 1:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 22:44:07 2024 Epoch 1, lr: 0.0001000, val loss: 169.27771, acc: 42.07000\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "Epoch : 1 train_loss :2.049 , val_loss :169.278 val_acc : 42.07 lr :0.0000994 epoch_time : 228 sec\n",
            "Train Epoch 2:   0%|          | 0/391 [00:00<?, ?it/s]\n",
            "Test_ Epoch 2:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 22:47:54 2024 Epoch 2, lr: 0.0000994, val loss: 108.79947, acc: 61.36000\n",
            "Epoch : 2 train_loss :1.661 , val_loss :108.799 val_acc : 61.36 lr :0.0001000 epoch_time : 226 sec\n",
            "Train Epoch 3:   0%|          | 0/391 [00:00<?, ?it/s]\n",
            "Test_ Epoch 3:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 22:51:40 2024 Epoch 3, lr: 0.0001000, val loss: 104.79639, acc: 65.11000\n",
            "Epoch : 3 train_loss :1.469 , val_loss :104.796 val_acc : 65.11 lr :0.0000994 epoch_time : 226 sec\n",
            "Train Epoch 4:   0%|          | 0/391 [00:00<?, ?it/s]\n",
            "Test_ Epoch 4:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 22:55:26 2024 Epoch 4, lr: 0.0000994, val loss: 72.13274, acc: 74.98000\n",
            "Epoch : 4 train_loss :1.338 , val_loss :72.133 val_acc : 74.98 lr :0.0000976 epoch_time : 226 sec\n",
            "Train Epoch 5:   0%|          | 0/391 [00:00<?, ?it/s]\n",
            "Test_ Epoch 5:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 22:59:13 2024 Epoch 5, lr: 0.0000976, val loss: 63.34025, acc: 77.61000\n",
            "Epoch : 5 train_loss :1.247 , val_loss :63.340 val_acc : 77.61 lr :0.0000946 epoch_time : 226 sec\n",
            "Train Epoch 6:   0%|          | 0/391 [00:00<?, ?it/s]\n",
            "Test_ Epoch 6:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 23:03:00 2024 Epoch 6, lr: 0.0000946, val loss: 56.11573, acc: 80.56000\n",
            "Epoch : 6 train_loss :1.189 , val_loss :56.116 val_acc : 80.56 lr :0.0000905 epoch_time : 226 sec\n",
            "Train Epoch 7:   0%|          | 0/391 [00:00<?, ?it/s]\n",
            "Test_ Epoch 7:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Current time : Wed May 15 23:06:44 2024 Epoch 7, lr: 0.0000905, val loss: 58.85737, acc: 79.34000\n",
            "Epoch : 7 train_loss :1.146 , val_loss :58.857 val_acc : 79.34 lr :0.0000854 epoch_time : 224 sec\n",
            "Train Epoch 8:   0%|          | 0/391 [00:00<?, ?it/s]\n",
            "Test_ Epoch 8:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 23:10:31 2024 Epoch 8, lr: 0.0000854, val loss: 45.89906, acc: 84.20000\n",
            "Epoch : 8 train_loss :1.113 , val_loss :45.899 val_acc : 84.2 lr :0.0000794 epoch_time : 226 sec\n",
            "Train Epoch 9:   0%|          | 0/391 [00:00<?, ?it/s]\n",
            "Test_ Epoch 9:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 23:14:18 2024 Epoch 9, lr: 0.0000794, val loss: 44.79505, acc: 84.46000\n",
            "Epoch : 9 train_loss :1.075 , val_loss :44.795 val_acc : 84.46 lr :0.0000727 epoch_time : 226 sec\n",
            "Train Epoch 10:   0%|          | 0/391 [00:00<?, ?it/s]\n",
            "Test_ Epoch 10:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 23:18:04 2024 Epoch 10, lr: 0.0000727, val loss: 44.10424, acc: 85.33000\n",
            "Epoch : 10 train_loss :1.047 , val_loss :44.104 val_acc : 85.33 lr :0.0000655 epoch_time : 226 sec\n",
            "Train Epoch 11:   0%|          | 0/391 [00:00<?, ?it/s]\n",
            "Test_ Epoch 11:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 23:21:50 2024 Epoch 11, lr: 0.0000655, val loss: 37.61861, acc: 87.07000\n",
            "Epoch : 11 train_loss :1.025 , val_loss :37.619 val_acc : 87.07 lr :0.0000578 epoch_time : 226 sec\n",
            "Train Epoch 12:   0%|          | 0/391 [00:00<?, ?it/s]\n",
            "Test_ Epoch 12:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 23:25:37 2024 Epoch 12, lr: 0.0000578, val loss: 35.84482, acc: 87.50000\n",
            "Epoch : 12 train_loss :0.997 , val_loss :35.845 val_acc : 87.5 lr :0.0000500 epoch_time : 226 sec\n",
            "Train Epoch 13:   0%|          | 0/391 [00:00<?, ?it/s]\n",
            "Test_ Epoch 13:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 23:29:23 2024 Epoch 13, lr: 0.0000500, val loss: 33.80164, acc: 88.44000\n",
            "Epoch : 13 train_loss :0.976 , val_loss :33.802 val_acc : 88.44 lr :0.0000422 epoch_time : 226 sec\n",
            "Train Epoch 14:   0%|          | 0/391 [00:00<?, ?it/s]\n",
            "Test_ Epoch 14:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Current time : Wed May 15 23:33:08 2024 Epoch 14, lr: 0.0000422, val loss: 33.46777, acc: 88.44000\n",
            "Epoch : 14 train_loss :0.949 , val_loss :33.468 val_acc : 88.44 lr :0.0000345 epoch_time : 224 sec\n",
            "Train Epoch 15:   0%|          | 0/391 [00:00<?, ?it/s]\n",
            "Test_ Epoch 15:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 23:36:54 2024 Epoch 15, lr: 0.0000345, val loss: 29.39293, acc: 89.86000\n",
            "Epoch : 15 train_loss :0.936 , val_loss :29.393 val_acc : 89.86 lr :0.0000273 epoch_time : 226 sec\n",
            "Train Epoch 16:   0%|          | 0/391 [00:00<?, ?it/s]\n",
            "Test_ Epoch 16:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 23:40:41 2024 Epoch 16, lr: 0.0000273, val loss: 28.92879, acc: 90.14000\n",
            "Epoch : 16 train_loss :0.915 , val_loss :28.929 val_acc : 90.14 lr :0.0000206 epoch_time : 226 sec\n",
            "Train Epoch 17:   0%|          | 0/391 [00:00<?, ?it/s]\n",
            "Test_ Epoch 17:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 23:44:27 2024 Epoch 17, lr: 0.0000206, val loss: 27.69993, acc: 90.64000\n",
            "Epoch : 17 train_loss :0.892 , val_loss :27.700 val_acc : 90.64 lr :0.0000146 epoch_time : 226 sec\n",
            "Train Epoch 18:   0%|          | 0/391 [00:00<?, ?it/s]\n",
            "Test_ Epoch 18:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 23:48:14 2024 Epoch 18, lr: 0.0000146, val loss: 26.11795, acc: 91.11000\n",
            "Epoch : 18 train_loss :0.883 , val_loss :26.118 val_acc : 91.11 lr :0.0000095 epoch_time : 226 sec\n",
            "Train Epoch 19:   0%|          | 0/391 [00:00<?, ?it/s]\n",
            "Test_ Epoch 19:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 23:52:00 2024 Epoch 19, lr: 0.0000095, val loss: 25.71677, acc: 91.31000\n",
            "Epoch : 19 train_loss :0.869 , val_loss :25.717 val_acc : 91.31 lr :0.0000054 epoch_time : 226 sec\n",
            "Train Epoch 20:   0%|          | 0/391 [00:00<?, ?it/s]\n",
            "Test_ Epoch 20:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 23:55:47 2024 Epoch 20, lr: 0.0000054, val loss: 25.16879, acc: 91.41000\n",
            "Epoch : 20 train_loss :0.857 , val_loss :25.169 val_acc : 91.41 lr :0.0000024 epoch_time : 226 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! python train.py --net mobile_vit --bs 256 --size 256 --n_epochs 20 # A-100 with bs 256 GPU 25.4 GB"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "sN-VoLDPnvHr",
        "outputId": "a554437c-9073-4525-9516-92881511772e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Preparing data..\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "==> Building model..\n",
            "cuda\n",
            "/content/train.py:508: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  with tqdm_notebook(total=len(train_ds), desc=f\"Train Epoch {epoch+1}\") as pbar:\n",
            "Train Epoch 1:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/content/train.py:546: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  with tqdm_notebook(total=len(test_ds), desc=f\"Test_ Epoch {epoch+1}\") as pbar:\n",
            "Test_ Epoch 1:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 23:43:54 2024 Epoch 1, lr: 0.0001000, val loss: 165.16268, acc: 37.57000\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "Epoch : 1 train_loss :2.102 , val_loss :165.163 val_acc : 37.57 lr :0.0000994 epoch_time : 82 sec\n",
            "Train Epoch 2:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 2:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 23:45:15 2024 Epoch 2, lr: 0.0000994, val loss: 144.92901, acc: 46.57000\n",
            "Epoch : 2 train_loss :1.908 , val_loss :144.929 val_acc : 46.57 lr :0.0001000 epoch_time : 81 sec\n",
            "Train Epoch 3:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 3:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 23:46:37 2024 Epoch 3, lr: 0.0001000, val loss: 125.79147, acc: 53.71000\n",
            "Epoch : 3 train_loss :1.768 , val_loss :125.791 val_acc : 53.71 lr :0.0000994 epoch_time : 81 sec\n",
            "Train Epoch 4:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 4:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 23:47:58 2024 Epoch 4, lr: 0.0000994, val loss: 113.13839, acc: 58.69000\n",
            "Epoch : 4 train_loss :1.670 , val_loss :113.138 val_acc : 58.69 lr :0.0000976 epoch_time : 81 sec\n",
            "Train Epoch 5:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 5:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 23:49:19 2024 Epoch 5, lr: 0.0000976, val loss: 103.79280, acc: 62.26000\n",
            "Epoch : 5 train_loss :1.584 , val_loss :103.793 val_acc : 62.26 lr :0.0000946 epoch_time : 80 sec\n",
            "Train Epoch 6:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 6:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 23:50:40 2024 Epoch 6, lr: 0.0000946, val loss: 95.53804, acc: 65.28000\n",
            "Epoch : 6 train_loss :1.529 , val_loss :95.538 val_acc : 65.28 lr :0.0000905 epoch_time : 81 sec\n",
            "Train Epoch 7:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 7:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 23:52:02 2024 Epoch 7, lr: 0.0000905, val loss: 92.64006, acc: 66.35000\n",
            "Epoch : 7 train_loss :1.475 , val_loss :92.640 val_acc : 66.35 lr :0.0000854 epoch_time : 81 sec\n",
            "Train Epoch 8:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 8:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 23:53:23 2024 Epoch 8, lr: 0.0000854, val loss: 85.85031, acc: 69.24000\n",
            "Epoch : 8 train_loss :1.443 , val_loss :85.850 val_acc : 69.24 lr :0.0000794 epoch_time : 81 sec\n",
            "Train Epoch 9:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 9:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Current time : Wed May 15 23:54:44 2024 Epoch 9, lr: 0.0000794, val loss: 86.41621, acc: 68.69000\n",
            "Epoch : 9 train_loss :1.412 , val_loss :86.416 val_acc : 68.69 lr :0.0000727 epoch_time : 81 sec\n",
            "Train Epoch 10:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 10:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 23:56:05 2024 Epoch 10, lr: 0.0000727, val loss: 79.88251, acc: 71.82000\n",
            "Epoch : 10 train_loss :1.382 , val_loss :79.883 val_acc : 71.82 lr :0.0000655 epoch_time : 81 sec\n",
            "Train Epoch 11:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 11:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 23:57:26 2024 Epoch 11, lr: 0.0000655, val loss: 77.94563, acc: 72.26000\n",
            "Epoch : 11 train_loss :1.352 , val_loss :77.946 val_acc : 72.26 lr :0.0000578 epoch_time : 81 sec\n",
            "Train Epoch 12:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 12:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Wed May 15 23:58:47 2024 Epoch 12, lr: 0.0000578, val loss: 74.38780, acc: 73.45000\n",
            "Epoch : 12 train_loss :1.323 , val_loss :74.388 val_acc : 73.45 lr :0.0000500 epoch_time : 81 sec\n",
            "Train Epoch 13:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 13:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 00:00:09 2024 Epoch 13, lr: 0.0000500, val loss: 69.52516, acc: 75.43000\n",
            "Epoch : 13 train_loss :1.305 , val_loss :69.525 val_acc : 75.43 lr :0.0000422 epoch_time : 81 sec\n",
            "Train Epoch 14:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 14:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 00:01:30 2024 Epoch 14, lr: 0.0000422, val loss: 67.15624, acc: 76.25000\n",
            "Epoch : 14 train_loss :1.278 , val_loss :67.156 val_acc : 76.25 lr :0.0000345 epoch_time : 81 sec\n",
            "Train Epoch 15:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 15:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 00:02:52 2024 Epoch 15, lr: 0.0000345, val loss: 66.68257, acc: 76.57000\n",
            "Epoch : 15 train_loss :1.265 , val_loss :66.683 val_acc : 76.57 lr :0.0000273 epoch_time : 81 sec\n",
            "Train Epoch 16:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 16:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 00:04:13 2024 Epoch 16, lr: 0.0000273, val loss: 63.78237, acc: 77.77000\n",
            "Epoch : 16 train_loss :1.247 , val_loss :63.782 val_acc : 77.77 lr :0.0000206 epoch_time : 81 sec\n",
            "Train Epoch 17:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 17:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Current time : Thu May 16 00:05:34 2024 Epoch 17, lr: 0.0000206, val loss: 62.41057, acc: 77.62000\n",
            "Epoch : 17 train_loss :1.241 , val_loss :62.411 val_acc : 77.62 lr :0.0000146 epoch_time : 80 sec\n",
            "Train Epoch 18:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 18:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 00:06:55 2024 Epoch 18, lr: 0.0000146, val loss: 61.50506, acc: 78.15000\n",
            "Epoch : 18 train_loss :1.223 , val_loss :61.505 val_acc : 78.15 lr :0.0000095 epoch_time : 81 sec\n",
            "Train Epoch 19:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 19:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 00:08:16 2024 Epoch 19, lr: 0.0000095, val loss: 61.33742, acc: 78.30000\n",
            "Epoch : 19 train_loss :1.230 , val_loss :61.337 val_acc : 78.3 lr :0.0000054 epoch_time : 81 sec\n",
            "Train Epoch 20:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 20:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 00:09:37 2024 Epoch 20, lr: 0.0000054, val loss: 60.51565, acc: 78.61000\n",
            "Epoch : 20 train_loss :1.226 , val_loss :60.516 val_acc : 78.61 lr :0.0000024 epoch_time : 81 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! python train.py --net nest --bs 256 --size 224 --n_epochs 20 # A-100 with bs 256 GPU 38.2 GB"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "d9SkVOQynykD",
        "outputId": "c6af93ec-2504-4fd9-93b6-d5891534ecd5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Preparing data..\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "==> Building model..\n",
            "cuda\n",
            "/content/train.py:508: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  with tqdm_notebook(total=len(train_ds), desc=f\"Train Epoch {epoch+1}\") as pbar:\n",
            "Train Epoch 1:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/content/train.py:546: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  with tqdm_notebook(total=len(test_ds), desc=f\"Test_ Epoch {epoch+1}\") as pbar:\n",
            "Test_ Epoch 1:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 00:12:21 2024 Epoch 1, lr: 0.0001000, val loss: 206.46173, acc: 23.69000\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "Epoch : 1 train_loss :2.287 , val_loss :206.462 val_acc : 23.69 lr :0.0000994 epoch_time : 124 sec\n",
            "Train Epoch 2:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 2:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 00:14:23 2024 Epoch 2, lr: 0.0000994, val loss: 184.71162, acc: 29.90000\n",
            "Epoch : 2 train_loss :2.145 , val_loss :184.712 val_acc : 29.9 lr :0.0001000 epoch_time : 122 sec\n",
            "Train Epoch 3:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 3:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 00:16:25 2024 Epoch 3, lr: 0.0001000, val loss: 157.37670, acc: 40.73000\n",
            "Epoch : 3 train_loss :1.961 , val_loss :157.377 val_acc : 40.73 lr :0.0000994 epoch_time : 122 sec\n",
            "Train Epoch 4:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 4:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 00:18:28 2024 Epoch 4, lr: 0.0000994, val loss: 144.40819, acc: 47.46000\n",
            "Epoch : 4 train_loss :1.843 , val_loss :144.408 val_acc : 47.46 lr :0.0000976 epoch_time : 122 sec\n",
            "Train Epoch 5:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 5:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 00:20:30 2024 Epoch 5, lr: 0.0000976, val loss: 135.74191, acc: 52.12000\n",
            "Epoch : 5 train_loss :1.754 , val_loss :135.742 val_acc : 52.12 lr :0.0000946 epoch_time : 122 sec\n",
            "Train Epoch 6:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 6:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 00:22:33 2024 Epoch 6, lr: 0.0000946, val loss: 123.54141, acc: 55.02000\n",
            "Epoch : 6 train_loss :1.685 , val_loss :123.541 val_acc : 55.02 lr :0.0000905 epoch_time : 122 sec\n",
            "Train Epoch 7:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 7:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 00:24:36 2024 Epoch 7, lr: 0.0000905, val loss: 117.36793, acc: 57.92000\n",
            "Epoch : 7 train_loss :1.629 , val_loss :117.368 val_acc : 57.92 lr :0.0000854 epoch_time : 122 sec\n",
            "Train Epoch 8:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 8:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 00:26:38 2024 Epoch 8, lr: 0.0000854, val loss: 108.37691, acc: 60.58000\n",
            "Epoch : 8 train_loss :1.573 , val_loss :108.377 val_acc : 60.58 lr :0.0000794 epoch_time : 122 sec\n",
            "Train Epoch 9:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 9:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 00:28:41 2024 Epoch 9, lr: 0.0000794, val loss: 102.68569, acc: 63.07000\n",
            "Epoch : 9 train_loss :1.524 , val_loss :102.686 val_acc : 63.07 lr :0.0000727 epoch_time : 122 sec\n",
            "Train Epoch 10:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 10:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 00:30:43 2024 Epoch 10, lr: 0.0000727, val loss: 100.83412, acc: 63.95000\n",
            "Epoch : 10 train_loss :1.493 , val_loss :100.834 val_acc : 63.95 lr :0.0000655 epoch_time : 122 sec\n",
            "Train Epoch 11:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 11:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 00:32:46 2024 Epoch 11, lr: 0.0000655, val loss: 93.56469, acc: 66.32000\n",
            "Epoch : 11 train_loss :1.456 , val_loss :93.565 val_acc : 66.32 lr :0.0000578 epoch_time : 122 sec\n",
            "Train Epoch 12:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 12:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 00:34:49 2024 Epoch 12, lr: 0.0000578, val loss: 88.13746, acc: 68.38000\n",
            "Epoch : 12 train_loss :1.423 , val_loss :88.137 val_acc : 68.38 lr :0.0000500 epoch_time : 122 sec\n",
            "Train Epoch 13:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 13:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 00:36:51 2024 Epoch 13, lr: 0.0000500, val loss: 85.48376, acc: 69.55000\n",
            "Epoch : 13 train_loss :1.392 , val_loss :85.484 val_acc : 69.55 lr :0.0000422 epoch_time : 122 sec\n",
            "Train Epoch 14:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 14:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 00:38:54 2024 Epoch 14, lr: 0.0000422, val loss: 80.53564, acc: 71.26000\n",
            "Epoch : 14 train_loss :1.363 , val_loss :80.536 val_acc : 71.26 lr :0.0000345 epoch_time : 122 sec\n",
            "Train Epoch 15:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 15:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 00:40:56 2024 Epoch 15, lr: 0.0000345, val loss: 77.49213, acc: 72.59000\n",
            "Epoch : 15 train_loss :1.336 , val_loss :77.492 val_acc : 72.59 lr :0.0000273 epoch_time : 122 sec\n",
            "Train Epoch 16:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 16:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 00:42:59 2024 Epoch 16, lr: 0.0000273, val loss: 77.41052, acc: 72.94000\n",
            "Epoch : 16 train_loss :1.318 , val_loss :77.411 val_acc : 72.94 lr :0.0000206 epoch_time : 122 sec\n",
            "Train Epoch 17:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 17:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 00:45:01 2024 Epoch 17, lr: 0.0000206, val loss: 73.77146, acc: 74.46000\n",
            "Epoch : 17 train_loss :1.299 , val_loss :73.771 val_acc : 74.46 lr :0.0000146 epoch_time : 122 sec\n",
            "Train Epoch 18:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 18:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Current time : Thu May 16 00:47:04 2024 Epoch 18, lr: 0.0000146, val loss: 73.20062, acc: 74.25000\n",
            "Epoch : 18 train_loss :1.283 , val_loss :73.201 val_acc : 74.25 lr :0.0000095 epoch_time : 122 sec\n",
            "Train Epoch 19:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 19:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 00:49:06 2024 Epoch 19, lr: 0.0000095, val loss: 70.54477, acc: 74.88000\n",
            "Epoch : 19 train_loss :1.277 , val_loss :70.545 val_acc : 74.88 lr :0.0000054 epoch_time : 122 sec\n",
            "Train Epoch 20:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 20:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 00:51:08 2024 Epoch 20, lr: 0.0000054, val loss: 69.70570, acc: 74.99000\n",
            "Epoch : 20 train_loss :1.260 , val_loss :69.706 val_acc : 74.99 lr :0.0000024 epoch_time : 122 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! python train.py --net parallel_vit --bs 256 --size 256 --n_epochs 20 # A-100 with bs 256 GPU 31.8 GB"
      ],
      "metadata": {
        "id": "EEjdOeimn1kC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79717cb6-0ed8-4a96-b480-b9f7f2c3346a",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Preparing data..\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "==> Building model..\n",
            "cuda\n",
            "/content/train.py:537: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  with tqdm_notebook(total=len(train_ds), desc=f\"Train Epoch {epoch+1}\") as pbar:\n",
            "Train Epoch 1:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/content/train.py:575: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  with tqdm_notebook(total=len(test_ds), desc=f\"Test_ Epoch {epoch+1}\") as pbar:\n",
            "Test_ Epoch 1:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 00:23:12 2024 Epoch 1, lr: 0.0001000, val loss: 200.66426, acc: 26.43000\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "Epoch : 1 train_loss :2.309 , val_loss :200.664 val_acc : 26.43 lr :0.0000994 epoch_time : 134 sec\n",
            "Train Epoch 2:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 2:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 00:25:26 2024 Epoch 2, lr: 0.0000994, val loss: 170.81259, acc: 38.54000\n",
            "Epoch : 2 train_loss :2.152 , val_loss :170.813 val_acc : 38.54 lr :0.0001000 epoch_time : 133 sec\n",
            "Train Epoch 3:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 3:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 00:27:39 2024 Epoch 3, lr: 0.0001000, val loss: 157.83918, acc: 43.80000\n",
            "Epoch : 3 train_loss :2.016 , val_loss :157.839 val_acc : 43.8 lr :0.0000994 epoch_time : 133 sec\n",
            "Train Epoch 4:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 4:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 00:29:54 2024 Epoch 4, lr: 0.0000994, val loss: 141.85187, acc: 49.35000\n",
            "Epoch : 4 train_loss :1.924 , val_loss :141.852 val_acc : 49.35 lr :0.0000976 epoch_time : 134 sec\n",
            "Train Epoch 5:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 5:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 00:32:08 2024 Epoch 5, lr: 0.0000976, val loss: 132.33502, acc: 52.21000\n",
            "Epoch : 5 train_loss :1.871 , val_loss :132.335 val_acc : 52.21 lr :0.0000946 epoch_time : 134 sec\n",
            "Train Epoch 6:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 6:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 00:34:22 2024 Epoch 6, lr: 0.0000946, val loss: 130.67459, acc: 52.61000\n",
            "Epoch : 6 train_loss :1.811 , val_loss :130.675 val_acc : 52.61 lr :0.0000905 epoch_time : 134 sec\n",
            "Train Epoch 7:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 7:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 00:36:37 2024 Epoch 7, lr: 0.0000905, val loss: 120.40148, acc: 56.73000\n",
            "Epoch : 7 train_loss :1.770 , val_loss :120.401 val_acc : 56.73 lr :0.0000854 epoch_time : 134 sec\n",
            "Train Epoch 8:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 8:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 00:38:51 2024 Epoch 8, lr: 0.0000854, val loss: 113.44820, acc: 59.26000\n",
            "Epoch : 8 train_loss :1.740 , val_loss :113.448 val_acc : 59.26 lr :0.0000794 epoch_time : 134 sec\n",
            "Train Epoch 9:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 9:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 00:41:06 2024 Epoch 9, lr: 0.0000794, val loss: 111.69079, acc: 59.98000\n",
            "Epoch : 9 train_loss :1.705 , val_loss :111.691 val_acc : 59.98 lr :0.0000727 epoch_time : 134 sec\n",
            "Train Epoch 10:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 10:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Current time : Thu May 16 00:43:18 2024 Epoch 10, lr: 0.0000727, val loss: 112.34267, acc: 59.70000\n",
            "Epoch : 10 train_loss :1.669 , val_loss :112.343 val_acc : 59.7 lr :0.0000655 epoch_time : 132 sec\n",
            "Train Epoch 11:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 11:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 00:45:31 2024 Epoch 11, lr: 0.0000655, val loss: 105.55895, acc: 62.25000\n",
            "Epoch : 11 train_loss :1.634 , val_loss :105.559 val_acc : 62.25 lr :0.0000578 epoch_time : 133 sec\n",
            "Train Epoch 12:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 12:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 00:47:45 2024 Epoch 12, lr: 0.0000578, val loss: 101.56443, acc: 63.82000\n",
            "Epoch : 12 train_loss :1.600 , val_loss :101.564 val_acc : 63.82 lr :0.0000500 epoch_time : 134 sec\n",
            "Train Epoch 13:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 13:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 00:49:59 2024 Epoch 13, lr: 0.0000500, val loss: 99.33117, acc: 64.55000\n",
            "Epoch : 13 train_loss :1.574 , val_loss :99.331 val_acc : 64.55 lr :0.0000422 epoch_time : 134 sec\n",
            "Train Epoch 14:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 14:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 00:52:14 2024 Epoch 14, lr: 0.0000422, val loss: 91.72714, acc: 67.87000\n",
            "Epoch : 14 train_loss :1.550 , val_loss :91.727 val_acc : 67.87 lr :0.0000345 epoch_time : 134 sec\n",
            "Train Epoch 15:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 15:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 00:54:30 2024 Epoch 15, lr: 0.0000345, val loss: 89.58627, acc: 68.66000\n",
            "Epoch : 15 train_loss :1.524 , val_loss :89.586 val_acc : 68.66 lr :0.0000273 epoch_time : 136 sec\n",
            "Train Epoch 16:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 16:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Current time : Thu May 16 00:56:43 2024 Epoch 16, lr: 0.0000273, val loss: 89.13460, acc: 68.37000\n",
            "Epoch : 16 train_loss :1.497 , val_loss :89.135 val_acc : 68.37 lr :0.0000206 epoch_time : 132 sec\n",
            "Train Epoch 17:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 17:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 00:58:57 2024 Epoch 17, lr: 0.0000206, val loss: 87.56224, acc: 69.20000\n",
            "Epoch : 17 train_loss :1.472 , val_loss :87.562 val_acc : 69.2 lr :0.0000146 epoch_time : 134 sec\n",
            "Train Epoch 18:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 18:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 01:01:12 2024 Epoch 18, lr: 0.0000146, val loss: 82.78581, acc: 70.58000\n",
            "Epoch : 18 train_loss :1.465 , val_loss :82.786 val_acc : 70.58 lr :0.0000095 epoch_time : 135 sec\n",
            "Train Epoch 19:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 19:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 01:03:26 2024 Epoch 19, lr: 0.0000095, val loss: 81.71563, acc: 71.42000\n",
            "Epoch : 19 train_loss :1.444 , val_loss :81.716 val_acc : 71.42 lr :0.0000054 epoch_time : 134 sec\n",
            "Train Epoch 20:   0%|          | 0/196 [00:00<?, ?it/s]\n",
            "Test_ Epoch 20:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 01:05:42 2024 Epoch 20, lr: 0.0000054, val loss: 80.93419, acc: 71.79000\n",
            "Epoch : 20 train_loss :1.437 , val_loss :80.934 val_acc : 71.79 lr :0.0000024 epoch_time : 135 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! python train.py --net pit --bs 64 --size 224 --n_epochs 20 # A-100 with bs 64 GPU 37.2 GB"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "njWnasyOxBM3",
        "outputId": "b1cc6283-b314-4f8b-f4e9-9c708afec5ee"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Preparing data..\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "==> Building model..\n",
            "cuda\n",
            "/content/train.py:537: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  with tqdm_notebook(total=len(train_ds), desc=f\"Train Epoch {epoch+1}\") as pbar:\n",
            "Train Epoch 1:   0%|          | 0/782 [00:00<?, ?it/s]\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/content/train.py:575: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  with tqdm_notebook(total=len(test_ds), desc=f\"Test_ Epoch {epoch+1}\") as pbar:\n",
            "Test_ Epoch 1:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 00:58:55 2024 Epoch 1, lr: 0.0001000, val loss: 199.40798, acc: 24.54000\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "Epoch : 1 train_loss :2.277 , val_loss :199.408 val_acc : 24.54 lr :0.0000994 epoch_time : 288 sec\n",
            "Train Epoch 2:   0%|          | 0/782 [00:00<?, ?it/s]\n",
            "Test_ Epoch 2:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 01:03:42 2024 Epoch 2, lr: 0.0000994, val loss: 173.64521, acc: 36.38000\n",
            "Epoch : 2 train_loss :2.075 , val_loss :173.645 val_acc : 36.38 lr :0.0001000 epoch_time : 287 sec\n",
            "Train Epoch 3:   0%|          | 0/782 [00:00<?, ?it/s]\n",
            "Test_ Epoch 3:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 01:08:29 2024 Epoch 3, lr: 0.0001000, val loss: 155.84737, acc: 42.52000\n",
            "Epoch : 3 train_loss :1.941 , val_loss :155.847 val_acc : 42.52 lr :0.0000994 epoch_time : 287 sec\n",
            "Train Epoch 4:   0%|          | 0/782 [00:00<?, ?it/s]\n",
            "Test_ Epoch 4:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 01:13:17 2024 Epoch 4, lr: 0.0000994, val loss: 139.89685, acc: 49.86000\n",
            "Epoch : 4 train_loss :1.861 , val_loss :139.897 val_acc : 49.86 lr :0.0000976 epoch_time : 287 sec\n",
            "Train Epoch 5:   0%|          | 0/782 [00:00<?, ?it/s]\n",
            "Test_ Epoch 5:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 01:18:05 2024 Epoch 5, lr: 0.0000976, val loss: 136.24605, acc: 50.83000\n",
            "Epoch : 5 train_loss :1.793 , val_loss :136.246 val_acc : 50.83 lr :0.0000946 epoch_time : 287 sec\n",
            "Train Epoch 6:   0%|          | 0/782 [00:00<?, ?it/s]\n",
            "Test_ Epoch 6:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 01:22:52 2024 Epoch 6, lr: 0.0000946, val loss: 126.93202, acc: 53.54000\n",
            "Epoch : 6 train_loss :1.760 , val_loss :126.932 val_acc : 53.54 lr :0.0000905 epoch_time : 287 sec\n",
            "Train Epoch 7:   0%|          | 0/782 [00:00<?, ?it/s]\n",
            "Test_ Epoch 7:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 01:27:40 2024 Epoch 7, lr: 0.0000905, val loss: 122.00966, acc: 56.28000\n",
            "Epoch : 7 train_loss :1.719 , val_loss :122.010 val_acc : 56.28 lr :0.0000854 epoch_time : 287 sec\n",
            "Train Epoch 8:   0%|          | 0/782 [00:00<?, ?it/s]\n",
            "Test_ Epoch 8:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 01:32:28 2024 Epoch 8, lr: 0.0000854, val loss: 115.58293, acc: 58.68000\n",
            "Epoch : 8 train_loss :1.678 , val_loss :115.583 val_acc : 58.68 lr :0.0000794 epoch_time : 287 sec\n",
            "Train Epoch 9:   0%|          | 0/782 [00:00<?, ?it/s]\n",
            "Test_ Epoch 9:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 01:37:15 2024 Epoch 9, lr: 0.0000794, val loss: 112.22029, acc: 58.69000\n",
            "Epoch : 9 train_loss :1.649 , val_loss :112.220 val_acc : 58.69 lr :0.0000727 epoch_time : 287 sec\n",
            "Train Epoch 10:   0%|          | 0/782 [00:00<?, ?it/s]\n",
            "Test_ Epoch 10:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 01:42:03 2024 Epoch 10, lr: 0.0000727, val loss: 103.39756, acc: 62.58000\n",
            "Epoch : 10 train_loss :1.614 , val_loss :103.398 val_acc : 62.58 lr :0.0000655 epoch_time : 287 sec\n",
            "Train Epoch 11:   0%|          | 0/782 [00:00<?, ?it/s]\n",
            "Test_ Epoch 11:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 01:46:51 2024 Epoch 11, lr: 0.0000655, val loss: 101.69294, acc: 63.43000\n",
            "Epoch : 11 train_loss :1.590 , val_loss :101.693 val_acc : 63.43 lr :0.0000578 epoch_time : 287 sec\n",
            "Train Epoch 12:   0%|          | 0/782 [00:00<?, ?it/s]\n",
            "Test_ Epoch 12:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 01:51:38 2024 Epoch 12, lr: 0.0000578, val loss: 100.25638, acc: 64.09000\n",
            "Epoch : 12 train_loss :1.563 , val_loss :100.256 val_acc : 64.09 lr :0.0000500 epoch_time : 287 sec\n",
            "Train Epoch 13:   0%|          | 0/782 [00:00<?, ?it/s]\n",
            "Test_ Epoch 13:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 01:56:26 2024 Epoch 13, lr: 0.0000500, val loss: 98.64900, acc: 64.48000\n",
            "Epoch : 13 train_loss :1.542 , val_loss :98.649 val_acc : 64.48 lr :0.0000422 epoch_time : 287 sec\n",
            "Train Epoch 14:   0%|          | 0/782 [00:00<?, ?it/s]\n",
            "Test_ Epoch 14:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 02:01:14 2024 Epoch 14, lr: 0.0000422, val loss: 94.65646, acc: 66.25000\n",
            "Epoch : 14 train_loss :1.512 , val_loss :94.656 val_acc : 66.25 lr :0.0000345 epoch_time : 287 sec\n",
            "Train Epoch 15:   0%|          | 0/782 [00:00<?, ?it/s]\n",
            "Test_ Epoch 15:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 02:06:01 2024 Epoch 15, lr: 0.0000345, val loss: 88.98326, acc: 67.95000\n",
            "Epoch : 15 train_loss :1.487 , val_loss :88.983 val_acc : 67.95 lr :0.0000273 epoch_time : 287 sec\n",
            "Train Epoch 16:   0%|          | 0/782 [00:00<?, ?it/s]\n",
            "Test_ Epoch 16:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 02:10:49 2024 Epoch 16, lr: 0.0000273, val loss: 87.01742, acc: 68.82000\n",
            "Epoch : 16 train_loss :1.471 , val_loss :87.017 val_acc : 68.82 lr :0.0000206 epoch_time : 287 sec\n",
            "Train Epoch 17:   0%|          | 0/782 [00:00<?, ?it/s]\n",
            "Test_ Epoch 17:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 02:15:36 2024 Epoch 17, lr: 0.0000206, val loss: 85.17266, acc: 69.39000\n",
            "Epoch : 17 train_loss :1.453 , val_loss :85.173 val_acc : 69.39 lr :0.0000146 epoch_time : 287 sec\n",
            "Train Epoch 18:   0%|          | 0/782 [00:00<?, ?it/s]\n",
            "Test_ Epoch 18:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 02:20:24 2024 Epoch 18, lr: 0.0000146, val loss: 83.99058, acc: 69.73000\n",
            "Epoch : 18 train_loss :1.427 , val_loss :83.991 val_acc : 69.73 lr :0.0000095 epoch_time : 287 sec\n",
            "Train Epoch 19:   0%|          | 0/782 [00:00<?, ?it/s]\n",
            "Test_ Epoch 19:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 02:25:11 2024 Epoch 19, lr: 0.0000095, val loss: 82.78251, acc: 70.17000\n",
            "Epoch : 19 train_loss :1.418 , val_loss :82.783 val_acc : 70.17 lr :0.0000054 epoch_time : 287 sec\n",
            "Train Epoch 20:   0%|          | 0/782 [00:00<?, ?it/s]\n",
            "Test_ Epoch 20:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 02:29:59 2024 Epoch 20, lr: 0.0000054, val loss: 82.28731, acc: 70.36000\n",
            "Epoch : 20 train_loss :1.409 , val_loss :82.287 val_acc : 70.36 lr :0.0000024 epoch_time : 287 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! python train.py --net regionvit --size 224 --n_epochs 20 # A-100 with bs 512 GPU 32.5 GB"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8c0a6c5-cba1-4b05-c686-bf2139c6d483",
        "collapsed": true,
        "id": "M1Fe9MHRCRxX"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Preparing data..\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n",
            "100% 170498071/170498071 [00:01<00:00, 95810818.74it/s] \n",
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "==> Building model..\n",
            "cuda\n",
            "/content/train.py:560: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  with tqdm_notebook(total=len(train_ds), desc=f\"Train Epoch {epoch+1}\") as pbar:\n",
            "Train Epoch 1:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/content/train.py:598: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  with tqdm_notebook(total=len(test_ds), desc=f\"Test_ Epoch {epoch+1}\") as pbar:\n",
            "Test_ Epoch 1:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 01:11:54 2024 Epoch 1, lr: 0.0001000, val loss: 190.50534, acc: 33.42000\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "Epoch : 1 train_loss :2.267 , val_loss :190.505 val_acc : 33.42 lr :0.0000994 epoch_time : 66 sec\n",
            "Train Epoch 2:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 2:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 01:12:57 2024 Epoch 2, lr: 0.0000994, val loss: 180.91230, acc: 36.38000\n",
            "Epoch : 2 train_loss :2.163 , val_loss :180.912 val_acc : 36.38 lr :0.0001000 epoch_time : 63 sec\n",
            "Train Epoch 3:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 3:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 01:14:01 2024 Epoch 3, lr: 0.0001000, val loss: 166.05603, acc: 38.95000\n",
            "Epoch : 3 train_loss :2.054 , val_loss :166.056 val_acc : 38.95 lr :0.0000994 epoch_time : 63 sec\n",
            "Train Epoch 4:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 4:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 01:15:04 2024 Epoch 4, lr: 0.0000994, val loss: 156.98258, acc: 43.40000\n",
            "Epoch : 4 train_loss :1.965 , val_loss :156.983 val_acc : 43.4 lr :0.0000976 epoch_time : 63 sec\n",
            "Train Epoch 5:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 5:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 01:16:07 2024 Epoch 5, lr: 0.0000976, val loss: 144.25848, acc: 48.78000\n",
            "Epoch : 5 train_loss :1.878 , val_loss :144.258 val_acc : 48.78 lr :0.0000946 epoch_time : 63 sec\n",
            "Train Epoch 6:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 6:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 01:17:10 2024 Epoch 6, lr: 0.0000946, val loss: 130.26877, acc: 54.38000\n",
            "Epoch : 6 train_loss :1.799 , val_loss :130.269 val_acc : 54.38 lr :0.0000905 epoch_time : 63 sec\n",
            "Train Epoch 7:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 7:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 01:18:14 2024 Epoch 7, lr: 0.0000905, val loss: 121.04091, acc: 57.05000\n",
            "Epoch : 7 train_loss :1.721 , val_loss :121.041 val_acc : 57.05 lr :0.0000854 epoch_time : 63 sec\n",
            "Train Epoch 8:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 8:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 01:19:17 2024 Epoch 8, lr: 0.0000854, val loss: 117.95756, acc: 57.57000\n",
            "Epoch : 8 train_loss :1.670 , val_loss :117.958 val_acc : 57.57 lr :0.0000794 epoch_time : 63 sec\n",
            "Train Epoch 9:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 9:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 01:20:20 2024 Epoch 9, lr: 0.0000794, val loss: 105.85058, acc: 62.47000\n",
            "Epoch : 9 train_loss :1.632 , val_loss :105.851 val_acc : 62.47 lr :0.0000727 epoch_time : 63 sec\n",
            "Train Epoch 10:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 10:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Current time : Thu May 16 01:21:22 2024 Epoch 10, lr: 0.0000727, val loss: 106.83324, acc: 61.71000\n",
            "Epoch : 10 train_loss :1.584 , val_loss :106.833 val_acc : 61.71 lr :0.0000655 epoch_time : 62 sec\n",
            "Train Epoch 11:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 11:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 01:22:25 2024 Epoch 11, lr: 0.0000655, val loss: 101.92562, acc: 64.02000\n",
            "Epoch : 11 train_loss :1.567 , val_loss :101.926 val_acc : 64.02 lr :0.0000578 epoch_time : 62 sec\n",
            "Train Epoch 12:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 12:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 01:23:28 2024 Epoch 12, lr: 0.0000578, val loss: 97.45728, acc: 65.44000\n",
            "Epoch : 12 train_loss :1.543 , val_loss :97.457 val_acc : 65.44 lr :0.0000500 epoch_time : 63 sec\n",
            "Train Epoch 13:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 13:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 01:24:31 2024 Epoch 13, lr: 0.0000500, val loss: 93.92708, acc: 66.40000\n",
            "Epoch : 13 train_loss :1.505 , val_loss :93.927 val_acc : 66.4 lr :0.0000422 epoch_time : 62 sec\n",
            "Train Epoch 14:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 14:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 01:25:34 2024 Epoch 14, lr: 0.0000422, val loss: 92.36654, acc: 67.06000\n",
            "Epoch : 14 train_loss :1.482 , val_loss :92.367 val_acc : 67.06 lr :0.0000345 epoch_time : 63 sec\n",
            "Train Epoch 15:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 15:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 01:26:38 2024 Epoch 15, lr: 0.0000345, val loss: 91.33155, acc: 67.94000\n",
            "Epoch : 15 train_loss :1.461 , val_loss :91.332 val_acc : 67.94 lr :0.0000273 epoch_time : 63 sec\n",
            "Train Epoch 16:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 16:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 01:27:41 2024 Epoch 16, lr: 0.0000273, val loss: 87.71987, acc: 69.53000\n",
            "Epoch : 16 train_loss :1.447 , val_loss :87.720 val_acc : 69.53 lr :0.0000206 epoch_time : 62 sec\n",
            "Train Epoch 17:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 17:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 01:28:44 2024 Epoch 17, lr: 0.0000206, val loss: 85.65993, acc: 69.76000\n",
            "Epoch : 17 train_loss :1.428 , val_loss :85.660 val_acc : 69.76 lr :0.0000146 epoch_time : 63 sec\n",
            "Train Epoch 18:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 18:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 01:29:47 2024 Epoch 18, lr: 0.0000146, val loss: 84.55405, acc: 70.17000\n",
            "Epoch : 18 train_loss :1.412 , val_loss :84.554 val_acc : 70.17 lr :0.0000095 epoch_time : 63 sec\n",
            "Train Epoch 19:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 19:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 01:30:50 2024 Epoch 19, lr: 0.0000095, val loss: 83.70499, acc: 70.72000\n",
            "Epoch : 19 train_loss :1.407 , val_loss :83.705 val_acc : 70.72 lr :0.0000054 epoch_time : 63 sec\n",
            "Train Epoch 20:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 20:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 01:31:54 2024 Epoch 20, lr: 0.0000054, val loss: 82.92804, acc: 71.03000\n",
            "Epoch : 20 train_loss :1.395 , val_loss :82.928 val_acc : 71.03 lr :0.0000024 epoch_time : 63 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! python train.py --net rvt --size 224 --n_epochs 20 # A-100 with bs 512 GPU 10.2 GB"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acc0477d-5490-4355-87ee-466925dcdf53",
        "collapsed": true,
        "id": "2mEGLQB28XMO"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Preparing data..\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "==> Building model..\n",
            "cuda\n",
            "/content/train.py:602: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  with tqdm_notebook(total=len(train_ds), desc=f\"Train Epoch {epoch+1}\") as pbar:\n",
            "Train Epoch 1:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/content/train.py:640: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  with tqdm_notebook(total=len(test_ds), desc=f\"Test_ Epoch {epoch+1}\") as pbar:\n",
            "Test_ Epoch 1:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 01:33:59 2024 Epoch 1, lr: 0.0001000, val loss: 199.65734, acc: 29.06000\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "Epoch : 1 train_loss :2.341 , val_loss :199.657 val_acc : 29.06 lr :0.0000994 epoch_time : 42 sec\n",
            "Train Epoch 2:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 2:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 01:34:41 2024 Epoch 2, lr: 0.0000994, val loss: 162.97830, acc: 41.56000\n",
            "Epoch : 2 train_loss :2.112 , val_loss :162.978 val_acc : 41.56 lr :0.0001000 epoch_time : 42 sec\n",
            "Train Epoch 3:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 3:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 01:35:23 2024 Epoch 3, lr: 0.0001000, val loss: 148.97775, acc: 46.43000\n",
            "Epoch : 3 train_loss :1.973 , val_loss :148.978 val_acc : 46.43 lr :0.0000994 epoch_time : 41 sec\n",
            "Train Epoch 4:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 4:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 01:36:05 2024 Epoch 4, lr: 0.0000994, val loss: 140.18458, acc: 49.36000\n",
            "Epoch : 4 train_loss :1.889 , val_loss :140.185 val_acc : 49.36 lr :0.0000976 epoch_time : 42 sec\n",
            "Train Epoch 5:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 5:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 01:36:48 2024 Epoch 5, lr: 0.0000976, val loss: 131.01317, acc: 52.23000\n",
            "Epoch : 5 train_loss :1.826 , val_loss :131.013 val_acc : 52.23 lr :0.0000946 epoch_time : 42 sec\n",
            "Train Epoch 6:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 6:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 01:37:30 2024 Epoch 6, lr: 0.0000946, val loss: 122.86530, acc: 55.74000\n",
            "Epoch : 6 train_loss :1.777 , val_loss :122.865 val_acc : 55.74 lr :0.0000905 epoch_time : 42 sec\n",
            "Train Epoch 7:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 7:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 01:38:13 2024 Epoch 7, lr: 0.0000905, val loss: 119.39782, acc: 57.46000\n",
            "Epoch : 7 train_loss :1.729 , val_loss :119.398 val_acc : 57.46 lr :0.0000854 epoch_time : 42 sec\n",
            "Train Epoch 8:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 8:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 01:38:55 2024 Epoch 8, lr: 0.0000854, val loss: 110.70573, acc: 61.36000\n",
            "Epoch : 8 train_loss :1.690 , val_loss :110.706 val_acc : 61.36 lr :0.0000794 epoch_time : 42 sec\n",
            "Train Epoch 9:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 9:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 01:39:38 2024 Epoch 9, lr: 0.0000794, val loss: 108.98406, acc: 62.02000\n",
            "Epoch : 9 train_loss :1.643 , val_loss :108.984 val_acc : 62.02 lr :0.0000727 epoch_time : 42 sec\n",
            "Train Epoch 10:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 10:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 01:40:20 2024 Epoch 10, lr: 0.0000727, val loss: 103.93797, acc: 62.87000\n",
            "Epoch : 10 train_loss :1.608 , val_loss :103.938 val_acc : 62.87 lr :0.0000655 epoch_time : 42 sec\n",
            "Train Epoch 11:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 11:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 01:41:02 2024 Epoch 11, lr: 0.0000655, val loss: 101.01399, acc: 63.47000\n",
            "Epoch : 11 train_loss :1.581 , val_loss :101.014 val_acc : 63.47 lr :0.0000578 epoch_time : 42 sec\n",
            "Train Epoch 12:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 12:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 01:41:45 2024 Epoch 12, lr: 0.0000578, val loss: 94.68278, acc: 66.45000\n",
            "Epoch : 12 train_loss :1.550 , val_loss :94.683 val_acc : 66.45 lr :0.0000500 epoch_time : 42 sec\n",
            "Train Epoch 13:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 13:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 01:42:27 2024 Epoch 13, lr: 0.0000500, val loss: 92.22725, acc: 67.23000\n",
            "Epoch : 13 train_loss :1.527 , val_loss :92.227 val_acc : 67.23 lr :0.0000422 epoch_time : 42 sec\n",
            "Train Epoch 14:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 14:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 01:43:09 2024 Epoch 14, lr: 0.0000422, val loss: 87.98647, acc: 69.23000\n",
            "Epoch : 14 train_loss :1.502 , val_loss :87.986 val_acc : 69.23 lr :0.0000345 epoch_time : 41 sec\n",
            "Train Epoch 15:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 15:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 01:43:51 2024 Epoch 15, lr: 0.0000345, val loss: 88.22101, acc: 69.27000\n",
            "Epoch : 15 train_loss :1.474 , val_loss :88.221 val_acc : 69.27 lr :0.0000273 epoch_time : 42 sec\n",
            "Train Epoch 16:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 16:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 01:44:34 2024 Epoch 16, lr: 0.0000273, val loss: 86.30245, acc: 69.42000\n",
            "Epoch : 16 train_loss :1.447 , val_loss :86.302 val_acc : 69.42 lr :0.0000206 epoch_time : 42 sec\n",
            "Train Epoch 17:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 17:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 01:45:17 2024 Epoch 17, lr: 0.0000206, val loss: 84.27407, acc: 70.27000\n",
            "Epoch : 17 train_loss :1.442 , val_loss :84.274 val_acc : 70.27 lr :0.0000146 epoch_time : 43 sec\n",
            "Train Epoch 18:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 18:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 01:45:59 2024 Epoch 18, lr: 0.0000146, val loss: 81.48481, acc: 71.41000\n",
            "Epoch : 18 train_loss :1.421 , val_loss :81.485 val_acc : 71.41 lr :0.0000095 epoch_time : 42 sec\n",
            "Train Epoch 19:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 19:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 01:46:42 2024 Epoch 19, lr: 0.0000095, val loss: 81.22197, acc: 71.62000\n",
            "Epoch : 19 train_loss :1.416 , val_loss :81.222 val_acc : 71.62 lr :0.0000054 epoch_time : 42 sec\n",
            "Train Epoch 20:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 20:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 01:47:24 2024 Epoch 20, lr: 0.0000054, val loss: 79.96564, acc: 71.99000\n",
            "Epoch : 20 train_loss :1.402 , val_loss :79.966 val_acc : 71.99 lr :0.0000024 epoch_time : 42 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cannot test bs(batch size) 64 at A-100\n",
        "# ! python train.py --net scalable_vit --bs 64 --size 256 --n_epochs 20 # A-100 with bs 128 GPU xxx GB\n",
        "! python train.py --net scalable_vit --bs 32 --size 256 --n_epochs 10 # A-100 with bs 32 GPU 34.9 GB"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-T4f6PPC3ui",
        "outputId": "fb4d5b1d-8714-4ec2-9c71-cbd879565d2c",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Preparing data..\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n",
            "100% 170498071/170498071 [00:04<00:00, 39847927.05it/s]\n",
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "==> Building model..\n",
            "cuda\n",
            "/content/train.py:602: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  with tqdm_notebook(total=len(train_ds), desc=f\"Train Epoch {epoch+1}\") as pbar:\n",
            "Train Epoch 1:   0%|          | 0/1563 [00:00<?, ?it/s]\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/content/train.py:640: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  with tqdm_notebook(total=len(test_ds), desc=f\"Test_ Epoch {epoch+1}\") as pbar:\n",
            "Test_ Epoch 1:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 03:03:50 2024 Epoch 1, lr: 0.0001000, val loss: 232.37332, acc: 10.00000\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "Epoch : 1 train_loss :2.328 , val_loss :232.373 val_acc : 10.0 lr :0.0000976 epoch_time : 649 sec\n",
            "Train Epoch 2:   0%|          | 0/1563 [00:00<?, ?it/s]\n",
            "Test_ Epoch 2:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Current time : Thu May 16 03:14:31 2024 Epoch 2, lr: 0.0000976, val loss: 230.71329, acc: 10.00000\n",
            "Epoch : 2 train_loss :2.313 , val_loss :230.713 val_acc : 10.0 lr :0.0001000 epoch_time : 641 sec\n",
            "Train Epoch 3:   0%|          | 0/1563 [00:00<?, ?it/s]\n",
            "Test_ Epoch 3:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Current time : Thu May 16 03:25:20 2024 Epoch 3, lr: 0.0001000, val loss: 230.78226, acc: 10.00000\n",
            "Epoch : 3 train_loss :2.309 , val_loss :230.782 val_acc : 10.0 lr :0.0000976 epoch_time : 649 sec\n",
            "Train Epoch 4:   0%|          | 0/1563 [00:00<?, ?it/s]\n",
            "Test_ Epoch 4:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 03:35:55 2024 Epoch 4, lr: 0.0000976, val loss: 218.67866, acc: 18.29000\n",
            "Epoch : 4 train_loss :2.292 , val_loss :218.679 val_acc : 18.29 lr :0.0000905 epoch_time : 634 sec\n",
            "Train Epoch 5:   0%|          | 0/1563 [00:00<?, ?it/s]\n",
            "Test_ Epoch 5:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 03:46:22 2024 Epoch 5, lr: 0.0000905, val loss: 211.88492, acc: 21.68000\n",
            "Epoch : 5 train_loss :2.252 , val_loss :211.885 val_acc : 21.68 lr :0.0000794 epoch_time : 626 sec\n",
            "Train Epoch 6:   0%|          | 0/1563 [00:00<?, ?it/s]\n",
            "Test_ Epoch 6:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Current time : Thu May 16 03:56:47 2024 Epoch 6, lr: 0.0000794, val loss: 216.20338, acc: 20.86000\n",
            "Epoch : 6 train_loss :2.238 , val_loss :216.203 val_acc : 20.86 lr :0.0000655 epoch_time : 625 sec\n",
            "Train Epoch 7:   0%|          | 0/1563 [00:00<?, ?it/s]\n",
            "Test_ Epoch 7:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Current time : Thu May 16 04:07:12 2024 Epoch 7, lr: 0.0000655, val loss: 230.98224, acc: 14.55000\n",
            "Epoch : 7 train_loss :2.252 , val_loss :230.982 val_acc : 14.55 lr :0.0000500 epoch_time : 625 sec\n",
            "Train Epoch 8:   0%|          | 0/1563 [00:00<?, ?it/s]\n",
            "Test_ Epoch 8:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Current time : Thu May 16 04:17:38 2024 Epoch 8, lr: 0.0000500, val loss: 256.35568, acc: 14.73000\n",
            "Epoch : 8 train_loss :2.256 , val_loss :256.356 val_acc : 14.73 lr :0.0000345 epoch_time : 626 sec\n",
            "Train Epoch 9:   0%|          | 0/1563 [00:00<?, ?it/s]\n",
            "Test_ Epoch 9:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Current time : Thu May 16 04:28:06 2024 Epoch 9, lr: 0.0000345, val loss: 234.56975, acc: 18.48000\n",
            "Epoch : 9 train_loss :2.224 , val_loss :234.570 val_acc : 18.48 lr :0.0000206 epoch_time : 627 sec\n",
            "Train Epoch 10:   0%|          | 0/1563 [00:00<?, ?it/s]\n",
            "Test_ Epoch 10:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 04:38:54 2024 Epoch 10, lr: 0.0000206, val loss: 208.45773, acc: 24.16000\n",
            "Epoch : 10 train_loss :2.218 , val_loss :208.458 val_acc : 24.16 lr :0.0000095 epoch_time : 647 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! python train.py --net sep_vit --size 224 --n_epochs 20 # A-100 with bs 512 GPU 14.1 GB"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "szKE87VwDCjO",
        "outputId": "178e515a-57c7-4d41-fc01-32b602ea3801"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Preparing data..\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "==> Building model..\n",
            "cuda\n",
            "/content/train.py:602: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  with tqdm_notebook(total=len(train_ds), desc=f\"Train Epoch {epoch+1}\") as pbar:\n",
            "Train Epoch 1:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/content/train.py:640: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  with tqdm_notebook(total=len(test_ds), desc=f\"Test_ Epoch {epoch+1}\") as pbar:\n",
            "Test_ Epoch 1:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 01:51:48 2024 Epoch 1, lr: 0.0001000, val loss: 204.12263, acc: 26.14000\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "Epoch : 1 train_loss :2.282 , val_loss :204.123 val_acc : 26.14 lr :0.0000994 epoch_time : 43 sec\n",
            "Train Epoch 2:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 2:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 01:52:30 2024 Epoch 2, lr: 0.0000994, val loss: 188.71590, acc: 27.52000\n",
            "Epoch : 2 train_loss :2.183 , val_loss :188.716 val_acc : 27.52 lr :0.0001000 epoch_time : 42 sec\n",
            "Train Epoch 3:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 3:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 01:53:12 2024 Epoch 3, lr: 0.0001000, val loss: 174.81660, acc: 35.71000\n",
            "Epoch : 3 train_loss :2.070 , val_loss :174.817 val_acc : 35.71 lr :0.0000994 epoch_time : 41 sec\n",
            "Train Epoch 4:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 4:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 01:53:53 2024 Epoch 4, lr: 0.0000994, val loss: 170.87954, acc: 38.78000\n",
            "Epoch : 4 train_loss :2.003 , val_loss :170.880 val_acc : 38.78 lr :0.0000976 epoch_time : 41 sec\n",
            "Train Epoch 5:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 5:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 01:54:35 2024 Epoch 5, lr: 0.0000976, val loss: 164.20234, acc: 40.69000\n",
            "Epoch : 5 train_loss :1.952 , val_loss :164.202 val_acc : 40.69 lr :0.0000946 epoch_time : 42 sec\n",
            "Train Epoch 6:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 6:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 01:55:17 2024 Epoch 6, lr: 0.0000946, val loss: 158.00238, acc: 42.37000\n",
            "Epoch : 6 train_loss :1.905 , val_loss :158.002 val_acc : 42.37 lr :0.0000905 epoch_time : 41 sec\n",
            "Train Epoch 7:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 7:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 01:55:58 2024 Epoch 7, lr: 0.0000905, val loss: 144.18466, acc: 47.55000\n",
            "Epoch : 7 train_loss :1.861 , val_loss :144.185 val_acc : 47.55 lr :0.0000854 epoch_time : 41 sec\n",
            "Train Epoch 8:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 8:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 01:56:40 2024 Epoch 8, lr: 0.0000854, val loss: 144.84410, acc: 48.54000\n",
            "Epoch : 8 train_loss :1.826 , val_loss :144.844 val_acc : 48.54 lr :0.0000794 epoch_time : 41 sec\n",
            "Train Epoch 9:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 9:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 01:57:22 2024 Epoch 9, lr: 0.0000794, val loss: 142.40299, acc: 49.75000\n",
            "Epoch : 9 train_loss :1.797 , val_loss :142.403 val_acc : 49.75 lr :0.0000727 epoch_time : 41 sec\n",
            "Train Epoch 10:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 10:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 01:58:03 2024 Epoch 10, lr: 0.0000727, val loss: 135.93771, acc: 51.50000\n",
            "Epoch : 10 train_loss :1.764 , val_loss :135.938 val_acc : 51.5 lr :0.0000655 epoch_time : 41 sec\n",
            "Train Epoch 11:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 11:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 01:58:45 2024 Epoch 11, lr: 0.0000655, val loss: 127.44495, acc: 53.82000\n",
            "Epoch : 11 train_loss :1.743 , val_loss :127.445 val_acc : 53.82 lr :0.0000578 epoch_time : 41 sec\n",
            "Train Epoch 12:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 12:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 01:59:26 2024 Epoch 12, lr: 0.0000578, val loss: 127.97393, acc: 53.84000\n",
            "Epoch : 12 train_loss :1.716 , val_loss :127.974 val_acc : 53.84 lr :0.0000500 epoch_time : 41 sec\n",
            "Train Epoch 13:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 13:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 02:00:08 2024 Epoch 13, lr: 0.0000500, val loss: 121.94960, acc: 56.40000\n",
            "Epoch : 13 train_loss :1.696 , val_loss :121.950 val_acc : 56.4 lr :0.0000422 epoch_time : 41 sec\n",
            "Train Epoch 14:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 14:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 02:00:50 2024 Epoch 14, lr: 0.0000422, val loss: 119.05543, acc: 57.31000\n",
            "Epoch : 14 train_loss :1.672 , val_loss :119.055 val_acc : 57.31 lr :0.0000345 epoch_time : 41 sec\n",
            "Train Epoch 15:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 15:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 02:01:32 2024 Epoch 15, lr: 0.0000345, val loss: 115.80191, acc: 58.94000\n",
            "Epoch : 15 train_loss :1.662 , val_loss :115.802 val_acc : 58.94 lr :0.0000273 epoch_time : 41 sec\n",
            "Train Epoch 16:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 16:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 02:02:13 2024 Epoch 16, lr: 0.0000273, val loss: 114.64912, acc: 59.51000\n",
            "Epoch : 16 train_loss :1.641 , val_loss :114.649 val_acc : 59.51 lr :0.0000206 epoch_time : 41 sec\n",
            "Train Epoch 17:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 17:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 02:02:55 2024 Epoch 17, lr: 0.0000206, val loss: 112.08831, acc: 60.23000\n",
            "Epoch : 17 train_loss :1.627 , val_loss :112.088 val_acc : 60.23 lr :0.0000146 epoch_time : 41 sec\n",
            "Train Epoch 18:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 18:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 02:03:37 2024 Epoch 18, lr: 0.0000146, val loss: 111.59813, acc: 60.30000\n",
            "Epoch : 18 train_loss :1.618 , val_loss :111.598 val_acc : 60.3 lr :0.0000095 epoch_time : 41 sec\n",
            "Train Epoch 19:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 19:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 02:04:18 2024 Epoch 19, lr: 0.0000095, val loss: 109.64609, acc: 61.01000\n",
            "Epoch : 19 train_loss :1.612 , val_loss :109.646 val_acc : 61.01 lr :0.0000054 epoch_time : 41 sec\n",
            "Train Epoch 20:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 20:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Current time : Thu May 16 02:05:00 2024 Epoch 20, lr: 0.0000054, val loss: 109.91032, acc: 60.76000\n",
            "Epoch : 20 train_loss :1.607 , val_loss :109.910 val_acc : 60.76 lr :0.0000024 epoch_time : 41 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! python train.py --net twins_svt --bs 128 --size 224 --n_epochs 20 # A-100 with bs 128 GPU 30.8 GB"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43d02afc-35c0-4396-f579-e33c24b6aebf",
        "collapsed": true,
        "id": "ChaN2UGPOwAH"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Preparing data..\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "==> Building model..\n",
            "cuda\n",
            "/content/train.py:627: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  with tqdm_notebook(total=len(train_ds), desc=f\"Train Epoch {epoch+1}\") as pbar:\n",
            "Train Epoch 1:   0%|          | 0/391 [00:00<?, ?it/s]\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/content/train.py:665: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  with tqdm_notebook(total=len(test_ds), desc=f\"Test_ Epoch {epoch+1}\") as pbar:\n",
            "Test_ Epoch 1:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 02:34:50 2024 Epoch 1, lr: 0.0001000, val loss: 220.81420, acc: 16.46000\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "Epoch : 1 train_loss :3.016 , val_loss :220.814 val_acc : 16.46 lr :0.0000994 epoch_time : 178 sec\n",
            "Train Epoch 2:   0%|          | 0/391 [00:00<?, ?it/s]\n",
            "Test_ Epoch 2:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 02:37:50 2024 Epoch 2, lr: 0.0000994, val loss: 215.84831, acc: 19.72000\n",
            "Epoch : 2 train_loss :2.297 , val_loss :215.848 val_acc : 19.72 lr :0.0001000 epoch_time : 180 sec\n",
            "Train Epoch 3:   0%|          | 0/391 [00:00<?, ?it/s]\n",
            "Test_ Epoch 3:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 02:40:48 2024 Epoch 3, lr: 0.0001000, val loss: 214.39271, acc: 20.07000\n",
            "Epoch : 3 train_loss :2.278 , val_loss :214.393 val_acc : 20.07 lr :0.0000994 epoch_time : 177 sec\n",
            "Train Epoch 4:   0%|          | 0/391 [00:00<?, ?it/s]\n",
            "Test_ Epoch 4:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Current time : Thu May 16 02:43:41 2024 Epoch 4, lr: 0.0000994, val loss: 213.45941, acc: 19.95000\n",
            "Epoch : 4 train_loss :2.268 , val_loss :213.459 val_acc : 19.95 lr :0.0000976 epoch_time : 171 sec\n",
            "Train Epoch 5:   0%|          | 0/391 [00:00<?, ?it/s]\n",
            "Test_ Epoch 5:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 02:46:41 2024 Epoch 5, lr: 0.0000976, val loss: 210.54785, acc: 22.17000\n",
            "Epoch : 5 train_loss :2.249 , val_loss :210.548 val_acc : 22.17 lr :0.0000946 epoch_time : 179 sec\n",
            "Train Epoch 6:   0%|          | 0/391 [00:00<?, ?it/s]\n",
            "Test_ Epoch 6:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 02:49:38 2024 Epoch 6, lr: 0.0000946, val loss: 189.25678, acc: 29.50000\n",
            "Epoch : 6 train_loss :2.193 , val_loss :189.257 val_acc : 29.5 lr :0.0000905 epoch_time : 177 sec\n",
            "Train Epoch 7:   0%|          | 0/391 [00:00<?, ?it/s]\n",
            "Test_ Epoch 7:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 02:52:40 2024 Epoch 7, lr: 0.0000905, val loss: 179.29320, acc: 33.52000\n",
            "Epoch : 7 train_loss :2.100 , val_loss :179.293 val_acc : 33.52 lr :0.0000854 epoch_time : 180 sec\n",
            "Train Epoch 8:   0%|          | 0/391 [00:00<?, ?it/s]\n",
            "Test_ Epoch 8:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 02:55:38 2024 Epoch 8, lr: 0.0000854, val loss: 163.91116, acc: 39.17000\n",
            "Epoch : 8 train_loss :2.012 , val_loss :163.911 val_acc : 39.17 lr :0.0000794 epoch_time : 177 sec\n",
            "Train Epoch 9:   0%|          | 0/391 [00:00<?, ?it/s]\n",
            "Test_ Epoch 9:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 02:58:41 2024 Epoch 9, lr: 0.0000794, val loss: 150.65302, acc: 45.90000\n",
            "Epoch : 9 train_loss :1.930 , val_loss :150.653 val_acc : 45.9 lr :0.0000727 epoch_time : 180 sec\n",
            "Train Epoch 10:   0%|          | 0/391 [00:00<?, ?it/s]\n",
            "Test_ Epoch 10:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 03:01:39 2024 Epoch 10, lr: 0.0000727, val loss: 136.38972, acc: 50.18000\n",
            "Epoch : 10 train_loss :1.844 , val_loss :136.390 val_acc : 50.18 lr :0.0000655 epoch_time : 177 sec\n",
            "Train Epoch 11:   0%|          | 0/391 [00:00<?, ?it/s]\n",
            "Test_ Epoch 11:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 03:04:41 2024 Epoch 11, lr: 0.0000655, val loss: 131.45327, acc: 52.00000\n",
            "Epoch : 11 train_loss :1.778 , val_loss :131.453 val_acc : 52.0 lr :0.0000578 epoch_time : 180 sec\n",
            "Train Epoch 12:   0%|          | 0/391 [00:00<?, ?it/s]\n",
            "Test_ Epoch 12:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 03:07:40 2024 Epoch 12, lr: 0.0000578, val loss: 119.22265, acc: 56.48000\n",
            "Epoch : 12 train_loss :1.710 , val_loss :119.223 val_acc : 56.48 lr :0.0000500 epoch_time : 178 sec\n",
            "Train Epoch 13:   0%|          | 0/391 [00:00<?, ?it/s]\n",
            "Test_ Epoch 13:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 03:10:41 2024 Epoch 13, lr: 0.0000500, val loss: 116.54325, acc: 57.97000\n",
            "Epoch : 13 train_loss :1.653 , val_loss :116.543 val_acc : 57.97 lr :0.0000422 epoch_time : 180 sec\n",
            "Train Epoch 14:   0%|          | 0/391 [00:00<?, ?it/s]\n",
            "Test_ Epoch 14:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 03:13:39 2024 Epoch 14, lr: 0.0000422, val loss: 106.48721, acc: 62.59000\n",
            "Epoch : 14 train_loss :1.602 , val_loss :106.487 val_acc : 62.59 lr :0.0000345 epoch_time : 178 sec\n",
            "Train Epoch 15:   0%|          | 0/391 [00:00<?, ?it/s]\n",
            "Test_ Epoch 15:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 03:16:41 2024 Epoch 15, lr: 0.0000345, val loss: 100.16601, acc: 64.28000\n",
            "Epoch : 15 train_loss :1.549 , val_loss :100.166 val_acc : 64.28 lr :0.0000273 epoch_time : 180 sec\n",
            "Train Epoch 16:   0%|          | 0/391 [00:00<?, ?it/s]\n",
            "Test_ Epoch 16:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 03:19:39 2024 Epoch 16, lr: 0.0000273, val loss: 96.55843, acc: 65.64000\n",
            "Epoch : 16 train_loss :1.515 , val_loss :96.558 val_acc : 65.64 lr :0.0000206 epoch_time : 178 sec\n",
            "Train Epoch 17:   0%|          | 0/391 [00:00<?, ?it/s]\n",
            "Test_ Epoch 17:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 03:22:42 2024 Epoch 17, lr: 0.0000206, val loss: 91.46908, acc: 68.20000\n",
            "Epoch : 17 train_loss :1.474 , val_loss :91.469 val_acc : 68.2 lr :0.0000146 epoch_time : 180 sec\n",
            "Train Epoch 18:   0%|          | 0/391 [00:00<?, ?it/s]\n",
            "Test_ Epoch 18:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Current time : Thu May 16 03:25:34 2024 Epoch 18, lr: 0.0000146, val loss: 90.03829, acc: 67.92000\n",
            "Epoch : 18 train_loss :1.447 , val_loss :90.038 val_acc : 67.92 lr :0.0000095 epoch_time : 171 sec\n",
            "Train Epoch 19:   0%|          | 0/391 [00:00<?, ?it/s]\n",
            "Test_ Epoch 19:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 03:28:32 2024 Epoch 19, lr: 0.0000095, val loss: 85.57693, acc: 69.92000\n",
            "Epoch : 19 train_loss :1.412 , val_loss :85.577 val_acc : 69.92 lr :0.0000054 epoch_time : 177 sec\n",
            "Train Epoch 20:   0%|          | 0/391 [00:00<?, ?it/s]\n",
            "Test_ Epoch 20:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 03:31:35 2024 Epoch 20, lr: 0.0000054, val loss: 84.22663, acc: 70.16000\n",
            "Epoch : 20 train_loss :1.399 , val_loss :84.227 val_acc : 70.16 lr :0.0000024 epoch_time : 181 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! python train.py --net vit_with_patch_dropout --size 224 --n_epochs 20 # A-100 with bs 512 GPU 6.2 GB"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "b77d76a6-06bb-4079-8cb7-83b3b20932dd",
        "id": "srGPZARLcmV9"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Preparing data..\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "==> Building model..\n",
            "cuda\n",
            "/content/train.py:646: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  with tqdm_notebook(total=len(train_ds), desc=f\"Train Epoch {epoch+1}\") as pbar:\n",
            "Train Epoch 1:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/content/train.py:684: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  with tqdm_notebook(total=len(test_ds), desc=f\"Test_ Epoch {epoch+1}\") as pbar:\n",
            "Test_ Epoch 1:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 03:32:37 2024 Epoch 1, lr: 0.0001000, val loss: 195.03483, acc: 26.15000\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "Epoch : 1 train_loss :2.335 , val_loss :195.035 val_acc : 26.15 lr :0.0000994 epoch_time : 35 sec\n",
            "Train Epoch 2:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 2:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 03:33:11 2024 Epoch 2, lr: 0.0000994, val loss: 177.38882, acc: 38.39000\n",
            "Epoch : 2 train_loss :2.177 , val_loss :177.389 val_acc : 38.39 lr :0.0001000 epoch_time : 33 sec\n",
            "Train Epoch 3:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 3:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 03:33:46 2024 Epoch 3, lr: 0.0001000, val loss: 161.79452, acc: 43.04000\n",
            "Epoch : 3 train_loss :2.090 , val_loss :161.795 val_acc : 43.04 lr :0.0000994 epoch_time : 34 sec\n",
            "Train Epoch 4:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 4:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 03:34:20 2024 Epoch 4, lr: 0.0000994, val loss: 156.59376, acc: 44.71000\n",
            "Epoch : 4 train_loss :2.024 , val_loss :156.594 val_acc : 44.71 lr :0.0000976 epoch_time : 34 sec\n",
            "Train Epoch 5:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 5:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 03:34:54 2024 Epoch 5, lr: 0.0000976, val loss: 147.60756, acc: 47.72000\n",
            "Epoch : 5 train_loss :1.987 , val_loss :147.608 val_acc : 47.72 lr :0.0000946 epoch_time : 33 sec\n",
            "Train Epoch 6:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 6:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 03:35:28 2024 Epoch 6, lr: 0.0000946, val loss: 144.00047, acc: 49.95000\n",
            "Epoch : 6 train_loss :1.946 , val_loss :144.000 val_acc : 49.95 lr :0.0000905 epoch_time : 33 sec\n",
            "Train Epoch 7:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 7:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Current time : Thu May 16 03:36:00 2024 Epoch 7, lr: 0.0000905, val loss: 142.53729, acc: 48.72000\n",
            "Epoch : 7 train_loss :1.916 , val_loss :142.537 val_acc : 48.72 lr :0.0000854 epoch_time : 32 sec\n",
            "Train Epoch 8:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 8:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 03:36:34 2024 Epoch 8, lr: 0.0000854, val loss: 133.41172, acc: 52.60000\n",
            "Epoch : 8 train_loss :1.896 , val_loss :133.412 val_acc : 52.6 lr :0.0000794 epoch_time : 33 sec\n",
            "Train Epoch 9:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 9:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 03:37:08 2024 Epoch 9, lr: 0.0000794, val loss: 129.30679, acc: 54.18000\n",
            "Epoch : 9 train_loss :1.863 , val_loss :129.307 val_acc : 54.18 lr :0.0000727 epoch_time : 33 sec\n",
            "Train Epoch 10:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 10:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 03:37:41 2024 Epoch 10, lr: 0.0000727, val loss: 129.01759, acc: 54.44000\n",
            "Epoch : 10 train_loss :1.842 , val_loss :129.018 val_acc : 54.44 lr :0.0000655 epoch_time : 33 sec\n",
            "Train Epoch 11:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 11:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 03:38:15 2024 Epoch 11, lr: 0.0000655, val loss: 123.59216, acc: 56.29000\n",
            "Epoch : 11 train_loss :1.821 , val_loss :123.592 val_acc : 56.29 lr :0.0000578 epoch_time : 33 sec\n",
            "Train Epoch 12:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 12:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 03:38:49 2024 Epoch 12, lr: 0.0000578, val loss: 120.50416, acc: 57.41000\n",
            "Epoch : 12 train_loss :1.800 , val_loss :120.504 val_acc : 57.41 lr :0.0000500 epoch_time : 33 sec\n",
            "Train Epoch 13:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 13:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 03:39:22 2024 Epoch 13, lr: 0.0000500, val loss: 120.46963, acc: 57.57000\n",
            "Epoch : 13 train_loss :1.788 , val_loss :120.470 val_acc : 57.57 lr :0.0000422 epoch_time : 33 sec\n",
            "Train Epoch 14:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 14:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 03:39:56 2024 Epoch 14, lr: 0.0000422, val loss: 116.22481, acc: 59.13000\n",
            "Epoch : 14 train_loss :1.763 , val_loss :116.225 val_acc : 59.13 lr :0.0000345 epoch_time : 34 sec\n",
            "Train Epoch 15:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 15:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Current time : Thu May 16 03:40:29 2024 Epoch 15, lr: 0.0000345, val loss: 116.32787, acc: 58.51000\n",
            "Epoch : 15 train_loss :1.741 , val_loss :116.328 val_acc : 58.51 lr :0.0000273 epoch_time : 32 sec\n",
            "Train Epoch 16:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 16:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 03:41:02 2024 Epoch 16, lr: 0.0000273, val loss: 112.94775, acc: 59.94000\n",
            "Epoch : 16 train_loss :1.726 , val_loss :112.948 val_acc : 59.94 lr :0.0000206 epoch_time : 33 sec\n",
            "Train Epoch 17:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 17:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 03:41:36 2024 Epoch 17, lr: 0.0000206, val loss: 110.05511, acc: 61.16000\n",
            "Epoch : 17 train_loss :1.715 , val_loss :110.055 val_acc : 61.16 lr :0.0000146 epoch_time : 33 sec\n",
            "Train Epoch 18:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 18:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Current time : Thu May 16 03:42:08 2024 Epoch 18, lr: 0.0000146, val loss: 110.25134, acc: 60.80000\n",
            "Epoch : 18 train_loss :1.707 , val_loss :110.251 val_acc : 60.8 lr :0.0000095 epoch_time : 32 sec\n",
            "Train Epoch 19:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 19:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 03:42:43 2024 Epoch 19, lr: 0.0000095, val loss: 109.84271, acc: 61.65000\n",
            "Epoch : 19 train_loss :1.699 , val_loss :109.843 val_acc : 61.65 lr :0.0000054 epoch_time : 34 sec\n",
            "Train Epoch 20:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 20:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Current time : Thu May 16 03:43:15 2024 Epoch 20, lr: 0.0000054, val loss: 108.71224, acc: 61.33000\n",
            "Epoch : 20 train_loss :1.684 , val_loss :108.712 val_acc : 61.33 lr :0.0000024 epoch_time : 32 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! python train.py --net vit_with_patch_merger --size 256 --n_epochs 20 # A-100 with bs 512 GPU 39.2 GB"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UyDiX80cwXn",
        "outputId": "07093b9c-d5e5-4577-a53d-673c09e66790"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Preparing data..\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "==> Building model..\n",
            "cuda\n",
            "/content/train.py:651: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  with tqdm_notebook(total=len(train_ds), desc=f\"Train Epoch {epoch+1}\") as pbar:\n",
            "Train Epoch 1:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/content/train.py:689: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  with tqdm_notebook(total=len(test_ds), desc=f\"Test_ Epoch {epoch+1}\") as pbar:\n",
            "Test_ Epoch 1:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 04:12:27 2024 Epoch 1, lr: 0.0001000, val loss: 194.25563, acc: 26.16000\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "Epoch : 1 train_loss :2.295 , val_loss :194.256 val_acc : 26.16 lr :0.0000994 epoch_time : 82 sec\n",
            "Train Epoch 2:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 2:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 04:13:48 2024 Epoch 2, lr: 0.0000994, val loss: 174.97323, acc: 34.13000\n",
            "Epoch : 2 train_loss :2.095 , val_loss :174.973 val_acc : 34.13 lr :0.0001000 epoch_time : 80 sec\n",
            "Train Epoch 3:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 3:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 04:15:08 2024 Epoch 3, lr: 0.0001000, val loss: 163.66658, acc: 39.11000\n",
            "Epoch : 3 train_loss :1.991 , val_loss :163.667 val_acc : 39.11 lr :0.0000994 epoch_time : 80 sec\n",
            "Train Epoch 4:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 4:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 04:16:29 2024 Epoch 4, lr: 0.0000994, val loss: 147.27461, acc: 44.65000\n",
            "Epoch : 4 train_loss :1.918 , val_loss :147.275 val_acc : 44.65 lr :0.0000976 epoch_time : 81 sec\n",
            "Train Epoch 5:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 5:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 04:17:50 2024 Epoch 5, lr: 0.0000976, val loss: 135.00828, acc: 50.90000\n",
            "Epoch : 5 train_loss :1.849 , val_loss :135.008 val_acc : 50.9 lr :0.0000946 epoch_time : 80 sec\n",
            "Train Epoch 6:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 6:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Current time : Thu May 16 04:19:09 2024 Epoch 6, lr: 0.0000946, val loss: 130.68238, acc: 50.52000\n",
            "Epoch : 6 train_loss :1.805 , val_loss :130.682 val_acc : 50.52 lr :0.0000905 epoch_time : 79 sec\n",
            "Train Epoch 7:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 7:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 04:20:31 2024 Epoch 7, lr: 0.0000905, val loss: 120.11409, acc: 56.12000\n",
            "Epoch : 7 train_loss :1.742 , val_loss :120.114 val_acc : 56.12 lr :0.0000854 epoch_time : 81 sec\n",
            "Train Epoch 8:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 8:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 04:21:51 2024 Epoch 8, lr: 0.0000854, val loss: 119.64084, acc: 57.08000\n",
            "Epoch : 8 train_loss :1.710 , val_loss :119.641 val_acc : 57.08 lr :0.0000794 epoch_time : 80 sec\n",
            "Train Epoch 9:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 9:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 04:23:12 2024 Epoch 9, lr: 0.0000794, val loss: 115.90615, acc: 57.49000\n",
            "Epoch : 9 train_loss :1.684 , val_loss :115.906 val_acc : 57.49 lr :0.0000727 epoch_time : 80 sec\n",
            "Train Epoch 10:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 10:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 04:24:33 2024 Epoch 10, lr: 0.0000727, val loss: 109.29803, acc: 60.45000\n",
            "Epoch : 10 train_loss :1.648 , val_loss :109.298 val_acc : 60.45 lr :0.0000655 epoch_time : 80 sec\n",
            "Train Epoch 11:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 11:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Current time : Thu May 16 04:25:51 2024 Epoch 11, lr: 0.0000655, val loss: 109.98945, acc: 60.40000\n",
            "Epoch : 11 train_loss :1.626 , val_loss :109.989 val_acc : 60.4 lr :0.0000578 epoch_time : 78 sec\n",
            "Train Epoch 12:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 12:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 04:27:12 2024 Epoch 12, lr: 0.0000578, val loss: 106.65008, acc: 61.64000\n",
            "Epoch : 12 train_loss :1.600 , val_loss :106.650 val_acc : 61.64 lr :0.0000500 epoch_time : 80 sec\n",
            "Train Epoch 13:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 13:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 04:28:32 2024 Epoch 13, lr: 0.0000500, val loss: 100.27984, acc: 63.76000\n",
            "Epoch : 13 train_loss :1.567 , val_loss :100.280 val_acc : 63.76 lr :0.0000422 epoch_time : 80 sec\n",
            "Train Epoch 14:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 14:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 04:29:52 2024 Epoch 14, lr: 0.0000422, val loss: 98.07589, acc: 65.08000\n",
            "Epoch : 14 train_loss :1.556 , val_loss :98.076 val_acc : 65.08 lr :0.0000345 epoch_time : 80 sec\n",
            "Train Epoch 15:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 15:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 04:31:13 2024 Epoch 15, lr: 0.0000345, val loss: 93.31097, acc: 66.86000\n",
            "Epoch : 15 train_loss :1.537 , val_loss :93.311 val_acc : 66.86 lr :0.0000273 epoch_time : 80 sec\n",
            "Train Epoch 16:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 16:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 04:32:34 2024 Epoch 16, lr: 0.0000273, val loss: 92.04210, acc: 67.54000\n",
            "Epoch : 16 train_loss :1.518 , val_loss :92.042 val_acc : 67.54 lr :0.0000206 epoch_time : 80 sec\n",
            "Train Epoch 17:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 17:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Current time : Thu May 16 04:33:54 2024 Epoch 17, lr: 0.0000206, val loss: 93.22065, acc: 66.77000\n",
            "Epoch : 17 train_loss :1.510 , val_loss :93.221 val_acc : 66.77 lr :0.0000146 epoch_time : 79 sec\n",
            "Train Epoch 18:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 18:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 04:35:14 2024 Epoch 18, lr: 0.0000146, val loss: 90.07031, acc: 68.14000\n",
            "Epoch : 18 train_loss :1.490 , val_loss :90.070 val_acc : 68.14 lr :0.0000095 epoch_time : 80 sec\n",
            "Train Epoch 19:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 19:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 04:36:35 2024 Epoch 19, lr: 0.0000095, val loss: 88.19918, acc: 68.71000\n",
            "Epoch : 19 train_loss :1.480 , val_loss :88.199 val_acc : 68.71 lr :0.0000054 epoch_time : 80 sec\n",
            "Train Epoch 20:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 20:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Current time : Thu May 16 04:37:54 2024 Epoch 20, lr: 0.0000054, val loss: 88.24506, acc: 68.49000\n",
            "Epoch : 20 train_loss :1.469 , val_loss :88.245 val_acc : 68.49 lr :0.0000024 epoch_time : 79 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! python train.py --net xcit --size 256 --n_epochs 20 # A-100 with bs 512 GPU 28.8 GB"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9aYY53FcwgK",
        "outputId": "8d53f12f-d506-4699-cd72-f364ba632c61"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Preparing data..\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "==> Building model..\n",
            "cuda\n",
            "/content/train.py:651: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  with tqdm_notebook(total=len(train_ds), desc=f\"Train Epoch {epoch+1}\") as pbar:\n",
            "Train Epoch 1:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/content/train.py:689: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  with tqdm_notebook(total=len(test_ds), desc=f\"Test_ Epoch {epoch+1}\") as pbar:\n",
            "Test_ Epoch 1:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 03:46:42 2024 Epoch 1, lr: 0.0001000, val loss: 190.00706, acc: 29.58000\n",
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
            "Epoch : 1 train_loss :2.225 , val_loss :190.007 val_acc : 29.58 lr :0.0000994 epoch_time : 77 sec\n",
            "Train Epoch 2:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 2:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 03:47:58 2024 Epoch 2, lr: 0.0000994, val loss: 177.79632, acc: 35.35000\n",
            "Epoch : 2 train_loss :2.094 , val_loss :177.796 val_acc : 35.35 lr :0.0001000 epoch_time : 76 sec\n",
            "Train Epoch 3:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 3:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 03:49:15 2024 Epoch 3, lr: 0.0001000, val loss: 167.58947, acc: 39.18000\n",
            "Epoch : 3 train_loss :2.041 , val_loss :167.589 val_acc : 39.18 lr :0.0000994 epoch_time : 76 sec\n",
            "Train Epoch 4:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 4:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 03:50:33 2024 Epoch 4, lr: 0.0000994, val loss: 160.30336, acc: 42.05000\n",
            "Epoch : 4 train_loss :1.986 , val_loss :160.303 val_acc : 42.05 lr :0.0000976 epoch_time : 77 sec\n",
            "Train Epoch 5:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 5:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 03:51:52 2024 Epoch 5, lr: 0.0000976, val loss: 153.17001, acc: 44.87000\n",
            "Epoch : 5 train_loss :1.959 , val_loss :153.170 val_acc : 44.87 lr :0.0000946 epoch_time : 79 sec\n",
            "Train Epoch 6:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 6:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 03:53:09 2024 Epoch 6, lr: 0.0000946, val loss: 151.41011, acc: 45.77000\n",
            "Epoch : 6 train_loss :1.930 , val_loss :151.410 val_acc : 45.77 lr :0.0000905 epoch_time : 77 sec\n",
            "Train Epoch 7:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 7:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 03:54:26 2024 Epoch 7, lr: 0.0000905, val loss: 145.83054, acc: 46.85000\n",
            "Epoch : 7 train_loss :1.900 , val_loss :145.831 val_acc : 46.85 lr :0.0000854 epoch_time : 76 sec\n",
            "Train Epoch 8:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 8:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 03:55:42 2024 Epoch 8, lr: 0.0000854, val loss: 143.50196, acc: 48.23000\n",
            "Epoch : 8 train_loss :1.864 , val_loss :143.502 val_acc : 48.23 lr :0.0000794 epoch_time : 76 sec\n",
            "Train Epoch 9:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 9:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 03:56:59 2024 Epoch 9, lr: 0.0000794, val loss: 139.21822, acc: 50.43000\n",
            "Epoch : 9 train_loss :1.841 , val_loss :139.218 val_acc : 50.43 lr :0.0000727 epoch_time : 76 sec\n",
            "Train Epoch 10:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 10:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 03:58:14 2024 Epoch 10, lr: 0.0000727, val loss: 136.01771, acc: 51.09000\n",
            "Epoch : 10 train_loss :1.830 , val_loss :136.018 val_acc : 51.09 lr :0.0000655 epoch_time : 75 sec\n",
            "Train Epoch 11:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 11:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 03:59:31 2024 Epoch 11, lr: 0.0000655, val loss: 133.14147, acc: 52.42000\n",
            "Epoch : 11 train_loss :1.813 , val_loss :133.141 val_acc : 52.42 lr :0.0000578 epoch_time : 76 sec\n",
            "Train Epoch 12:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 12:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 04:00:47 2024 Epoch 12, lr: 0.0000578, val loss: 128.21710, acc: 53.91000\n",
            "Epoch : 12 train_loss :1.786 , val_loss :128.217 val_acc : 53.91 lr :0.0000500 epoch_time : 76 sec\n",
            "Train Epoch 13:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 13:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 04:02:03 2024 Epoch 13, lr: 0.0000500, val loss: 127.98107, acc: 53.96000\n",
            "Epoch : 13 train_loss :1.761 , val_loss :127.981 val_acc : 53.96 lr :0.0000422 epoch_time : 76 sec\n",
            "Train Epoch 14:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 14:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 04:03:21 2024 Epoch 14, lr: 0.0000422, val loss: 126.91167, acc: 54.61000\n",
            "Epoch : 14 train_loss :1.762 , val_loss :126.912 val_acc : 54.61 lr :0.0000345 epoch_time : 77 sec\n",
            "Train Epoch 15:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 15:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 04:04:37 2024 Epoch 15, lr: 0.0000345, val loss: 124.86125, acc: 55.35000\n",
            "Epoch : 15 train_loss :1.740 , val_loss :124.861 val_acc : 55.35 lr :0.0000273 epoch_time : 75 sec\n",
            "Train Epoch 16:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 16:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 04:05:53 2024 Epoch 16, lr: 0.0000273, val loss: 121.86537, acc: 55.93000\n",
            "Epoch : 16 train_loss :1.717 , val_loss :121.865 val_acc : 55.93 lr :0.0000206 epoch_time : 76 sec\n",
            "Train Epoch 17:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 17:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 04:07:10 2024 Epoch 17, lr: 0.0000206, val loss: 121.33987, acc: 56.74000\n",
            "Epoch : 17 train_loss :1.721 , val_loss :121.340 val_acc : 56.74 lr :0.0000146 epoch_time : 77 sec\n",
            "Train Epoch 18:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 18:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Current time : Thu May 16 04:08:24 2024 Epoch 18, lr: 0.0000146, val loss: 120.32385, acc: 56.52000\n",
            "Epoch : 18 train_loss :1.700 , val_loss :120.324 val_acc : 56.52 lr :0.0000095 epoch_time : 73 sec\n",
            "Train Epoch 19:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 19:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 04:09:39 2024 Epoch 19, lr: 0.0000095, val loss: 119.71230, acc: 57.20000\n",
            "Epoch : 19 train_loss :1.699 , val_loss :119.712 val_acc : 57.2 lr :0.0000054 epoch_time : 75 sec\n",
            "Train Epoch 20:   0%|          | 0/98 [00:00<?, ?it/s]\n",
            "Test_ Epoch 20:   0%|          | 0/100 [00:00<?, ?it/s]\n",
            "Saving..\n",
            "Current time : Thu May 16 04:10:56 2024 Epoch 20, lr: 0.0000054, val loss: 120.50886, acc: 57.24000\n",
            "Epoch : 20 train_loss :1.693 , val_loss :120.509 val_acc : 57.24 lr :0.0000024 epoch_time : 76 sec\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}